import json
import hashlib
import uuid
from typing import List, Optional
from datetime import datetime
import httpx
import asyncio

from fastapi import APIRouter, Body, HTTPException, Depends, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, field_validator
import pytz
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from sqlalchemy import delete

from ..models import Source, Chunk, WorkflowExecution
from ..database import get_db
from ..fetch_parse import fetch_then_extract, fetch_html
from ..utils.link_extractor import extract_links_from_html
from ..utils.url_grouping import determine_parent_url
from ..utils.task_status import ingest_task_manager, TaskStatus
from ..config import DEFAULT_INGEST_MODEL, SUBDOC_MAX_CONCURRENCY
from ..chunking import chunk_text
from ..embedding_client import embed_texts, DEFAULT_EMBEDDING_MODEL
from ..config import WEBHOOK_TIMEOUT, WEBHOOK_PREFIX, EMBEDDING_MAX_CONCURRENCY, EMBEDDING_BATCH_SIZE, EMBEDDING_DIMENSIONS, SUBDOC_USE_WEBHOOK_FALLBACK
from ..config import SUBDOC_MAX_RETRIES, SUBDOC_RETRY_BACKOFF_BASE, SUBDOC_RETRY_BACKOFF_FACTOR, SUBDOC_RETRY_JITTER
from ..vector_db_client import add_embeddings


router = APIRouter()


class WebhookResponseData(BaseModel):
    """ç”¨äºéªŒè¯webhookè¿”å›æ•°æ®ç»“æ„çš„æ¨¡å‹"""
    document_name: str
    collection_name: str
    url: str
    total_chunks: int
    source_id: Optional[str] = None
    session_id: Optional[str] = None
    task_name: str
    output: Optional[List[dict]] = None  # æ·»åŠ outputå­—æ®µä»¥åŒ…å«sub_docs
    recursive_depth: Optional[int] = 1  # æ·»åŠ é€’å½’æ·±åº¦å­—æ®µï¼Œé»˜è®¤ä¸º1
    request_id: Optional[str] = None  # è¯·æ±‚ID
    webhook_url: Optional[str] = None  # webhook URL
    is_recursive: Optional[bool] = False  # æ·»åŠ é€’å½’æ ‡è®°å­—æ®µï¼Œé»˜è®¤ä¸ºFalse
    collection_id: Optional[str] = None  # æ–°å¢ï¼šç¨³å®šé›†åˆID
    
    @field_validator('total_chunks', mode='before')
    @classmethod
    def validate_total_chunks(cls, v):
        """ç¡®ä¿total_chunksæ˜¯æ•´æ•°ç±»å‹"""
        if isinstance(v, str):
            try:
                return int(v)
            except ValueError:
                raise ValueError(f"æ— æ³•å°†total_chunksè½¬æ¢ä¸ºæ•´æ•°: {v}")
        return v
        
    @field_validator('recursive_depth', mode='before')
    @classmethod
    def validate_recursive_depth(cls, v):
        """ç¡®ä¿recursive_depthæ˜¯æ•´æ•°ç±»å‹"""
        if v is None:
            return 2  # é»˜è®¤å€¼
        if isinstance(v, str):
            try:
                return int(v)
            except ValueError:
                raise ValueError(f"æ— æ³•å°†recursive_depthè½¬æ¢ä¸ºæ•´æ•°: {v}")
        return v


class UnifiedIngestRequest(BaseModel):
    """ç»Ÿä¸€çš„æ‘„å–è¯·æ±‚æ¨¡å‹ï¼Œæ”¯æŒå®¢æˆ·ç«¯è¯·æ±‚å’Œwebhookå›è°ƒ"""
    # å®¢æˆ·ç«¯è¯·æ±‚å¿…éœ€å­—æ®µ
    url: str
    
    # å®¢æˆ·ç«¯è¯·æ±‚å¯é€‰å­—æ®µ
    embedding_model: Optional[str] = None
    embedding_dimensions: Optional[int] = None
    webhook_url: Optional[str] = None
    recursive_depth: Optional[int] = None
    
    # webhookå›è°ƒä¸“æœ‰å­—æ®µ
    document_name: Optional[str] = None
    collection_name: Optional[str] = None
    total_chunks: Optional[int] = None
    chunks: Optional[List[dict]] = None
    source_id: Optional[str] = None
    parent_source_id: Optional[int] = None  # æ·»åŠ çˆ¶çº§Source IDå­—æ®µ
    session_id: Optional[str] = None
    task_name: Optional[str] = None
    is_recursive: Optional[bool] = False  # æ·»åŠ é€’å½’æ ‡è®°å­—æ®µï¼Œé»˜è®¤ä¸ºFalse


async def process_sub_docs_concurrent(
    sub_docs_urls: List[str],
    recursive_depth: int,
    db: AsyncSession,
    parent_doc_name: Optional[str] = None,
    parent_collection_name: Optional[str] = None,
    parent_source_id: Optional[int] = None,
    parent_collection_id: Optional[str] = None
) -> List[dict]:
    """
    å¹¶å‘å¤„ç†å­æ–‡æ¡£çš„é€’å½’æ‘„å–å‡½æ•°

    Args:
        sub_docs_urls: å­æ–‡æ¡£URLåˆ—è¡¨
        recursive_depth: é€’å½’æ·±åº¦é™åˆ¶
        db: æ•°æ®åº“ä¼šè¯
        parent_doc_name: çˆ¶çº§æ–‡æ¡£åç§°
        parent_collection_name: çˆ¶çº§collectionåç§°
        parent_source_id: çˆ¶çº§Source IDï¼Œç”¨äºå°†å­æ–‡æ¡£å†…å®¹æ·»åŠ åˆ°åŒä¸€ä¸ªcollection

    Returns:
        List[dict]: æ¯ä¸ªå­æ–‡æ¡£çš„å¤„ç†ç»“æœ
    """
    results = []
    
    # æ£€æŸ¥é€’å½’æ·±åº¦é™åˆ¶
    if recursive_depth <= 0:
        print(f"è¾¾åˆ°é€’å½’æ·±åº¦é™åˆ¶ï¼Œè·³è¿‡ {len(sub_docs_urls)} ä¸ªå­æ–‡æ¡£çš„å¤„ç†")
        return results
    
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡ - å­æ–‡æ¡£å¤„ç†ä½¿ç”¨æ›´é«˜çš„å¹¶å‘æ•°
    MAX_CONCURRENT_SUB_DOCS = min(SUBDOC_MAX_CONCURRENCY, len(sub_docs_urls))  # ä½¿ç”¨é…ç½®çš„å¹¶å‘æ•°ï¼Œä½†ä¸è¶…è¿‡å­æ–‡æ¡£æ€»æ•°
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_SUB_DOCS)
    
    # å¹¶å‘å¤„ç†æ‰€æœ‰å­æ–‡æ¡£URL
    async def process_single_sub_doc(sub_url: str, parent_doc_name: str = None, parent_collection_name: str = None, parent_source_id: int = None) -> dict:
        async with semaphore:  # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            attempts = int(SUBDOC_MAX_RETRIES) + 1
            last_error: Exception = None
            for attempt in range(1, attempts + 1):
                try:
                    print(f"å¼€å§‹é€’å½’æ‘„å–å­æ–‡æ¡£: {sub_url} (å°è¯• {attempt}/{attempts})")

                    # æ„é€ é€’å½’è°ƒç”¨çš„è¯·æ±‚æ•°æ®
                    sub_request_data = {
                        "url": sub_url,
                        "recursive_depth": recursive_depth - 1,  # å‡å°‘é€’å½’æ·±åº¦
                        "embedding_model": DEFAULT_EMBEDDING_MODEL,
                        "embedding_dimensions": EMBEDDING_DIMENSIONS,
                        "webhook_url": WEBHOOK_PREFIX + "/array2array",
                        "is_recursive": True,  # æ ‡è®°ä¸ºé€’å½’è°ƒç”¨
                        "document_name": parent_doc_name,  # ä¼ é€’çˆ¶çº§æ–‡æ¡£åç§°
                        "collection_name": parent_collection_name,  # ä¼ é€’çˆ¶çº§collectionåç§°
                        "parent_source_id": parent_source_id,  # ä¼ é€’çˆ¶çº§Source ID
                        "collection_id": parent_collection_id  # ä¼ é€’ç¨³å®šé›†åˆID
                    }
                    
                    # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„BackgroundTaskså®ä¾‹ç”¨äºé€’å½’è°ƒç”¨
                    dummy_background_tasks = BackgroundTasks()
                    
                    # ä¸ºæ¯æ¬¡å°è¯•åˆ›å»ºç‹¬ç«‹çš„ä¼šè¯ï¼Œé¿å…äº‹åŠ¡æ±¡æŸ“
                    from ..database import AsyncSessionLocal
                    async with AsyncSessionLocal() as sub_db:
                        result = await auto_ingest(dummy_background_tasks, sub_request_data, sub_db)
                    
                    print(f"å­æ–‡æ¡£æ‘„å–æˆåŠŸ: {sub_url}")
                    return {
                        "url": sub_url,
                        "success": True,
                        "result": result
                    }
                    
                except Exception as e:
                    last_error = e
                    error_msg = f"å­æ–‡æ¡£æ‘„å–å¤±è´¥ {sub_url}ï¼ˆç¬¬ {attempt}/{attempts} æ¬¡ï¼‰: {str(e)}"
                    print(error_msg)
                    
                    if attempt < attempts:
                        # æŒ‡æ•°é€€é¿ + æŠ–åŠ¨
                        delay = SUBDOC_RETRY_BACKOFF_BASE * (SUBDOC_RETRY_BACKOFF_FACTOR ** (attempt - 1))
                        try:
                            import random  # å±€éƒ¨å¯¼å…¥ä»¥é¿å…é¡¶å±‚æœªä½¿ç”¨
                            jitter = random.uniform(0, SUBDOC_RETRY_JITTER)
                        except Exception:
                            jitter = 0.0
                        await asyncio.sleep(delay + jitter)
                        continue
                    
                    # æœ€ç»ˆå¤±è´¥
                    return {
                        "url": sub_url,
                        "success": False,
                        "error": error_msg
                    }
    
    # ä½¿ç”¨asyncio.gatherè¿›è¡Œå¹¶å‘å¤„ç†ï¼Œä½†é€šè¿‡ä¿¡å·é‡æ§åˆ¶æœ€å¤§å¹¶å‘æ•°
    try:
        tasks = [process_single_sub_doc(url, parent_doc_name, parent_collection_name, parent_source_id) for url in sub_docs_urls]
        results = await asyncio.gather(*tasks, return_exceptions=False)
    except Exception as e:
        print(f"å¹¶å‘å¤„ç†å­æ–‡æ¡£æ—¶å‡ºç°å¼‚å¸¸: {str(e)}")
        # é™çº§åˆ°ä¸²è¡Œå¤„ç†
        results = []
        for sub_url in sub_docs_urls:
            result = await process_single_sub_doc(sub_url, parent_doc_name, parent_collection_name, parent_source_id)
            results.append(result)
    
    return results


async def process_sub_docs_concurrent_with_tracking(
    sub_docs_urls: List[str],
    recursive_depth: int,
    db: AsyncSession,
    parent_doc_name: Optional[str] = None,
    parent_collection_name: Optional[str] = None,
    parent_source_id: Optional[int] = None,
    task_id: Optional[str] = None,
    parent_collection_id: Optional[str] = None
) -> List[dict]:
    """
    å¹¶å‘å¤„ç†å­æ–‡æ¡£çš„é€’å½’æ‘„å–å‡½æ•°ï¼ˆå¸¦çŠ¶æ€è¿½è¸ªï¼‰
    """
    results = []
    
    # æ£€æŸ¥é€’å½’æ·±åº¦é™åˆ¶
    if recursive_depth <= 0:
        print(f"è¾¾åˆ°é€’å½’æ·±åº¦é™åˆ¶ï¼Œè·³è¿‡ {len(sub_docs_urls)} ä¸ªå­æ–‡æ¡£çš„å¤„ç†")
        return results
    
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡ - å­æ–‡æ¡£å¤„ç†ä½¿ç”¨æ›´é«˜çš„å¹¶å‘æ•°
    MAX_CONCURRENT_SUB_DOCS = min(SUBDOC_MAX_CONCURRENCY, len(sub_docs_urls))  # ä½¿ç”¨é…ç½®çš„å¹¶å‘æ•°ï¼Œä½†ä¸è¶…è¿‡å­æ–‡æ¡£æ€»æ•°
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_SUB_DOCS)
    
    # å¹¶å‘å¤„ç†æ‰€æœ‰å­æ–‡æ¡£URL
    async def process_single_sub_doc_with_tracking(sub_url: str) -> dict:
        async with semaphore:
            # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
            if task_id:
                await ingest_task_manager.update_sub_doc_status(task_id, sub_url, TaskStatus.RUNNING)

            attempts = int(SUBDOC_MAX_RETRIES) + 1
            last_error: Exception = None

            for attempt in range(1, attempts + 1):
                # ä¸ºæ¯æ¬¡å°è¯•åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼Œé¿å…äº‹åŠ¡å†²çª
                from ..database import AsyncSessionLocal
                async with AsyncSessionLocal() as sub_db:
                    try:
                        print(f"å¼€å§‹é€’å½’æ‘„å–å­æ–‡æ¡£: {sub_url} (å°è¯• {attempt}/{attempts})")

                        # æ„é€ é€’å½’è°ƒç”¨çš„è¯·æ±‚æ•°æ®
                        sub_request_data = {
                            "url": sub_url,
                            "recursive_depth": recursive_depth - 1,  # å‡å°‘é€’å½’æ·±åº¦
                            "embedding_model": DEFAULT_EMBEDDING_MODEL,
                            "embedding_dimensions": EMBEDDING_DIMENSIONS,
                            "webhook_url": WEBHOOK_PREFIX + "/array2array",
                            "is_recursive": True,  # æ ‡è®°ä¸ºé€’å½’è°ƒç”¨
                            "document_name": parent_doc_name,  # ä¼ é€’çˆ¶çº§æ–‡æ¡£åç§°
                            "collection_name": parent_collection_name,  # ä¼ é€’çˆ¶çº§collectionåç§°
                            "parent_source_id": parent_source_id,  # ä¼ é€’çˆ¶çº§Source ID
                            "collection_id": parent_collection_id  # ä¼ é€’ç¨³å®šé›†åˆID
                        }
                        
                        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„BackgroundTaskså®ä¾‹ç”¨äºé€’å½’è°ƒç”¨
                        dummy_background_tasks = BackgroundTasks()
                        
                        # ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯è¿›è¡Œé€’å½’è°ƒç”¨
                        result = await auto_ingest(dummy_background_tasks, sub_request_data, sub_db)
                        
                        print(f"å­æ–‡æ¡£æ‘„å–æˆåŠŸ: {sub_url}")
                        
                        # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
                        if task_id:
                            await ingest_task_manager.update_sub_doc_status(task_id, sub_url, TaskStatus.COMPLETED)
                        
                        return {
                            "url": sub_url,
                            "success": True,
                            "result": result
                        }
                        
                    except Exception as e:
                        last_error = e
                        error_msg = f"å­æ–‡æ¡£æ‘„å–å¤±è´¥ {sub_url}ï¼ˆç¬¬ {attempt}/{attempts} æ¬¡ï¼‰: {str(e)}"
                        print(error_msg)

                        # å¤±è´¥å³æ—¶å†™å…¥çŠ¶æ€ï¼ˆä½†ä¸ç´¯è®¡å¤±è´¥è®¡æ•°ï¼Œç›´åˆ°æœ€ç»ˆå¤±è´¥ï¼‰
                        if task_id and attempt < attempts:
                            await ingest_task_manager.update_sub_doc_status(task_id, sub_url, TaskStatus.RUNNING, error_msg)
                        
                        # å°è¯•å›æ»š
                        try:
                            await sub_db.rollback()
                        except Exception:
                            pass
                        
                        if attempt < attempts:
                            # æŒ‡æ•°é€€é¿ + æŠ–åŠ¨
                            delay = SUBDOC_RETRY_BACKOFF_BASE * (SUBDOC_RETRY_BACKOFF_FACTOR ** (attempt - 1))
                            try:
                                import random
                                jitter = random.uniform(0, SUBDOC_RETRY_JITTER)
                            except Exception:
                                jitter = 0.0
                            await asyncio.sleep(delay + jitter)
                            continue
                        
                        # æœ€ç»ˆå¤±è´¥ï¼Œæ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
                        if task_id:
                            await ingest_task_manager.update_sub_doc_status(task_id, sub_url, TaskStatus.FAILED, error_msg)
                        return {
                            "url": sub_url,
                            "success": False,
                            "error": error_msg
                        }
    
    # ä½¿ç”¨asyncio.gatherè¿›è¡Œå¹¶å‘å¤„ç†ï¼Œä½†é€šè¿‡ä¿¡å·é‡æ§åˆ¶æœ€å¤§å¹¶å‘æ•°
    try:
        tasks = [process_single_sub_doc_with_tracking(url) for url in sub_docs_urls]
        results = await asyncio.gather(*tasks, return_exceptions=False)
    except Exception as e:
        print(f"å¹¶å‘å¤„ç†å­æ–‡æ¡£æ—¶å‡ºç°å¼‚å¸¸: {str(e)}")
        # é™çº§åˆ°ä¸²è¡Œå¤„ç†
        results = []
        for sub_url in sub_docs_urls:
            result = await process_single_sub_doc_with_tracking(sub_url)
            results.append(result)
    
    return results


async def process_sub_docs_background(
    sub_docs_urls: List[str],
    recursive_depth: int,
    request_id: Optional[str] = None,
    parent_doc_name: Optional[str] = None,
    parent_collection_name: Optional[str] = None,
    parent_source_id: Optional[int] = None,
    parent_url: Optional[str] = None,  # æ–°å¢ï¼šçˆ¶æ–‡æ¡£URLï¼Œç”¨äºä»»åŠ¡è¿½è¸ª
    parent_collection_id: Optional[str] = None
):
    """
    åå°å¼‚æ­¥å¤„ç†å­æ–‡æ¡£çš„å‡½æ•° - ä¸é˜»å¡ä¸»å“åº”

    Args:
        sub_docs_urls: å­æ–‡æ¡£URLåˆ—è¡¨
        recursive_depth: é€’å½’æ·±åº¦é™åˆ¶
        request_id: è¯·æ±‚IDï¼Œç”¨äºæ—¥å¿—è¿½è¸ª
        parent_doc_name: çˆ¶çº§æ–‡æ¡£åç§°
        parent_collection_name: çˆ¶çº§collectionåç§°
        parent_source_id: çˆ¶çº§Source IDï¼Œç”¨äºå°†å­æ–‡æ¡£å†…å®¹æ·»åŠ åˆ°åŒä¸€ä¸ªcollection
        parent_url: çˆ¶æ–‡æ¡£URLï¼Œç”¨äºä»»åŠ¡è¿½è¸ª
    """
    task_id = request_id or f"subdoc_{hash(str(sub_docs_urls))}"
    
    try:
        # åˆ›å»ºä»»åŠ¡çŠ¶æ€è¿½è¸ª
        if parent_url and parent_doc_name and parent_collection_name:
            await ingest_task_manager.create_task(
                task_id=task_id,
                parent_url=parent_url,
                document_name=parent_doc_name,
                collection_name=parent_collection_name,
                sub_doc_urls=sub_docs_urls
            )
            await ingest_task_manager.start_task(task_id)
        
        print(f"[åå°ä»»åŠ¡] å¼€å§‹å¤„ç† {len(sub_docs_urls)} ä¸ªå­æ–‡æ¡£ï¼Œtask_id: {task_id}")

        # ğŸ”¥ ä¿®å¤ï¼šä¸å†éœ€è¦å…±äº«æ•°æ®åº“ä¼šè¯ï¼Œæ¯ä¸ªå­æ–‡æ¡£ä½¿ç”¨ç‹¬ç«‹ä¼šè¯
        results = await process_sub_docs_concurrent_with_tracking(
            sub_docs_urls, recursive_depth, None, parent_doc_name, 
            parent_collection_name, parent_source_id, task_id, parent_collection_id
        )

        success_count = len([r for r in results if r.get('success')])
        print(f"[åå°ä»»åŠ¡] å­æ–‡æ¡£å¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{len(results)}, task_id: {task_id}")
        
        # ğŸ”¥ ä¿®å¤ï¼šä¸å†éœ€è¦ç»Ÿä¸€æäº¤ï¼Œæ¯ä¸ªå­æ–‡æ¡£ä¼šè¯å·²ç‹¬ç«‹æäº¤
        print(f"[åå°ä»»åŠ¡] æ‰€æœ‰å­æ–‡æ¡£å·²ç‹¬ç«‹å¤„ç†å’Œæäº¤ï¼Œå¤„ç†äº† {success_count} ä¸ªå­æ–‡æ¡£")
                
    except Exception as e:
        error_msg = f"å­æ–‡æ¡£å¤„ç†å¼‚å¸¸: {str(e)}"
        print(f"[åå°ä»»åŠ¡] {error_msg}, task_id: {task_id}")
        await ingest_task_manager.fail_task(task_id, error_msg)
        
        # ğŸ”¥ ä¿®å¤ï¼šä¸å†éœ€è¦ç»Ÿä¸€å›æ»šï¼Œæ¯ä¸ªå­æ–‡æ¡£ä¼šè¯ä¼šè‡ªåŠ¨ç®¡ç†äº‹åŠ¡


async def process_webhook_response(
    data: WebhookResponseData,
    db: AsyncSession,
    background_tasks: BackgroundTasks = None
):
    """
    å¤„ç†webhookå›è°ƒæ•°æ®çš„ä¸“ç”¨å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå­æ–‡æ¡£å¤„ç†æ”¹ä¸ºåå°ä»»åŠ¡
    """
    print("å¤„ç†webhookå“åº”æ•°æ®...")
    print(f"ä»»åŠ¡åç§°: {data.task_name}")
    print(f"æ–‡æ¡£åç§°: {data.document_name}")
    print(f"Collectionåç§°: {data.collection_name}")
    print(f"æ€»å—æ•°: {data.total_chunks}")
    
    # æ£€æŸ¥ä»»åŠ¡åç§°
    if data.task_name != "auto_ingest":
        return {
            "message": f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {data.task_name}",
            "task_name": data.task_name,
            "success": False
        }
    
    # å®ç°é€’å½’æ‘„å–é€»è¾‘ï¼šæ”¶é›†webhookå“åº”ä¸­çš„å­æ–‡æ¡£URL
    total_sub_docs = 0
    
    if data.output and isinstance(data.output, list):
        print(f"æ£€æµ‹åˆ°å“åº”æ•°æ®ï¼Œå…± {len(data.output)} ä¸ªå“åº”é¡¹")
        
        # æ”¶é›†æ‰€æœ‰å­æ–‡æ¡£URLï¼Œå¹¶è¿›è¡Œå»é‡å¤„ç†
        all_sub_docs = []
        seen_urls = set()  # ç”¨äºè·Ÿè¸ªå·²æ·»åŠ çš„URLï¼Œé¿å…é‡å¤

        for i, output_item in enumerate(data.output):
            if isinstance(output_item, dict) and "response" in output_item:
                response_data = output_item.get("response", {})
                if isinstance(response_data, dict) and "sub_docs" in response_data:
                    sub_docs = response_data.get("sub_docs", [])
                    if isinstance(sub_docs, list):
                        print(f"å“åº”é¡¹ {i} åŒ…å« {len(sub_docs)} ä¸ªå­æ–‡æ¡£")
                        total_sub_docs += len(sub_docs)

                        # é€ä¸ªæ£€æŸ¥å¹¶æ·»åŠ URLï¼Œé¿å…é‡å¤
                        from urllib.parse import urlparse
                        def is_strict_child(child: str, base: str) -> bool:
                            try:
                                cu = urlparse(child)
                                bu = urlparse(base)
                                if cu.netloc != bu.netloc:
                                    return False
                                base_path = bu.path.rstrip('/')
                                url_path = cu.path.rstrip('/')
                                # ä»…å…è®¸ä¸¥æ ¼å­è·¯å¾„ï¼Œå¦‚ /docs/python/*
                                return url_path.startswith(base_path + '/')
                            except Exception:
                                return False

                        for url in sub_docs:
                            if url and is_strict_child(url, data.url) and url not in seen_urls:
                                all_sub_docs.append(url)
                                seen_urls.add(url)
                            elif url and not is_strict_child(url, data.url):
                                print(f"è·³è¿‡å…„å¼Ÿ/éå­è·¯å¾„URL: {url}")
                            elif url in seen_urls:
                                print(f"è·³è¿‡é‡å¤çš„å­æ–‡æ¡£URL: {url}")
                    else:
                        print(f"å“åº”é¡¹ {i} çš„ sub_docs ä¸æ˜¯åˆ—è¡¨æ ¼å¼: {type(sub_docs)}")
                else:
                    print(f"å“åº”é¡¹ {i} çš„ response ä¸åŒ…å« sub_docs å­—æ®µæˆ–æ ¼å¼ä¸æ­£ç¡®")
            else:
                print(f"å“åº”é¡¹ {i} ä¸åŒ…å« response å­—æ®µæˆ–æ ¼å¼ä¸æ­£ç¡®")

        # è®°å½•å»é‡ç»Ÿè®¡ä¿¡æ¯
        if all_sub_docs:
            duplicates_count = total_sub_docs - len(all_sub_docs)
            if duplicates_count > 0:
                print(f"å»é‡å®Œæˆï¼šä» {total_sub_docs} ä¸ªåŸå§‹URLä¸­ç§»é™¤äº† {duplicates_count} ä¸ªé‡å¤é¡¹ï¼Œæœ€ç»ˆå¾—åˆ° {len(all_sub_docs)} ä¸ªå”¯ä¸€URL")
            else:
                print(f"å»é‡å®Œæˆï¼šæ‰€æœ‰ {total_sub_docs} ä¸ªURLéƒ½æ˜¯å”¯ä¸€çš„")
        
        if all_sub_docs:
            print(f"æ€»å…±å‘ç° {len(all_sub_docs)} ä¸ªå­æ–‡æ¡£URL")
            
            # ä»åŸå§‹æ•°æ®ä¸­è·å–é€’å½’æ·±åº¦å‚æ•°ï¼Œé»˜è®¤ä¸º1
            recursive_depth = 1
            if hasattr(data, 'recursive_depth') and isinstance(data.recursive_depth, int):
                recursive_depth = data.recursive_depth
            
            # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šå°†å­æ–‡æ¡£å¤„ç†ä½œä¸ºåå°ä»»åŠ¡å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡å“åº”
            # è·å–Source IDï¼ˆè¿™é‡Œçš„dataæ˜¯webhookå“åº”ï¼Œéœ€è¦é€šè¿‡document_nameå’Œcollectionæ‰¾åˆ°å¯¹åº”çš„source_idï¼‰
            source_id = None
            if data.document_name:
                try:
                    from sqlalchemy.future import select
                    FIXED_SESSION_ID = "fixed_session_id_for_auto_ingest"
                    stmt = select(Source).where(
                        Source.title == data.document_name,
                        Source.session_id == FIXED_SESSION_ID
                    ).order_by(Source.created_at.desc())  # è·å–æœ€æ–°åˆ›å»ºçš„source
                    result = await db.execute(stmt)
                    source = result.scalar_one_or_none()
                    if source:
                        source_id = source.id
                        print(f"æ‰¾åˆ°çˆ¶çº§Source ID: {source_id}")
                    else:
                        print(f"æœªæ‰¾åˆ°åŒ¹é…çš„Sourceï¼Œæ–‡æ¡£åç§°: {data.document_name}")
                except Exception as e:
                    print(f"æŸ¥æ‰¾çˆ¶çº§Sourceå¤±è´¥: {e}")
                    
            if background_tasks:
                print("å°†å­æ–‡æ¡£å¤„ç†æ·»åŠ åˆ°åå°ä»»åŠ¡é˜Ÿåˆ—...")
                background_tasks.add_task(
                    process_sub_docs_background,
                    all_sub_docs,
                    recursive_depth,
                    data.request_id,
                    data.document_name,
                    data.collection_name,
                    source_id
                )
            else:
                # å¦‚æœæ²¡æœ‰background_tasksï¼Œç›´æ¥å¯åŠ¨åç¨‹ä»»åŠ¡ï¼ˆä¸ç­‰å¾…ï¼‰
                print("å¯åŠ¨å­æ–‡æ¡£åå°å¤„ç†åç¨‹...")
                asyncio.create_task(
                    process_sub_docs_background(all_sub_docs, recursive_depth, data.request_id, data.document_name, data.collection_name, source_id)
                )
                
        else:
            print("æœªå‘ç°ä»»ä½•å­æ–‡æ¡£URL")
    else:
        print("å“åº”æ•°æ®ä¸­æœªåŒ…å«æœ‰æ•ˆçš„responseå­—æ®µ")
    
    # æ›´æ–°å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€
    try:
        if data.request_id:
            from sqlalchemy import update
            stmt = update(WorkflowExecution).where(
                WorkflowExecution.execution_id == data.request_id
            ).values(
                status="success",
                stopped_at=datetime.now(pytz.timezone('Asia/Shanghai'))
            )
            await db.execute(stmt)
            await db.commit()
            print(f"å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€å·²æ›´æ–°ä¸ºæˆåŠŸ: {data.request_id}")
    except Exception as e:
        print(f"æ›´æ–°å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€å¤±è´¥: {e}")
    
    # ğŸš€ ç«‹å³è¿”å›å“åº”ï¼Œä¸ç­‰å¾…å­æ–‡æ¡£å¤„ç†å®Œæˆ
    return {
        "message": "Webhookå“åº”å¤„ç†æˆåŠŸï¼Œå­æ–‡æ¡£å¤„ç†å·²å¯åŠ¨åå°ä»»åŠ¡",
        "task_name": data.task_name,
        "document_name": data.document_name,
        "total_sub_docs": total_sub_docs,
        "sub_docs_processing": "åå°å¤„ç†ä¸­" if total_sub_docs > 0 else "æ— éœ€å¤„ç†",
        "success": True
    }


async def generate_document_names(url: str, model: str = None) -> dict:
    """
    ä½¿ç”¨å¤§æ¨¡å‹ä¸ºæ–‡æ¡£å’Œcollectionç”Ÿæˆåç§°
    """
    # åˆ›å»ºä¸€ä¸ªä¸“é—¨ç”¨äºæ–‡æ¡£åç§°ç”Ÿæˆçš„æç¤ºæ¨¡æ¿ï¼Œé¿å…ä½¿ç”¨queryæ¥å£ä¸­çš„generate_answerå‡½æ•°
    # é€šè¿‡è°ƒç”¨llm_clientçš„åº•å±‚APIå®ç°ï¼Œè€Œä¸æ˜¯å¤ç”¨generate_answerå‡½æ•°
    from ..llm_client import chat_complete
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
    if model is None:
        model = DEFAULT_INGEST_MODEL
    
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åç§°ç”ŸæˆåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£ä¸ºç½‘é¡µå†…å®¹ç”Ÿæˆåˆé€‚çš„ä¸­æ–‡æ ‡é¢˜å’Œè‹±æ–‡collectionåç§°ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "- åªè¾“å‡ºä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«å¤šä½™è§£é‡Šæˆ–æ ‡ç‚¹ã€‚\n"
        "- document_name ä½¿ç”¨ç®€æ´å‡†ç¡®çš„ä¸­æ–‡æ ‡é¢˜ã€‚\n"
        "- collection_name å…¨å°å†™ã€ä½¿ç”¨ä¸‹åˆ’çº¿è¿æ¥ã€åªå«è‹±æ–‡å­—æ¯æ•°å­—ä¸‹åˆ’çº¿ã€‚"
    )
    user_prompt = (
        f"è¯·ä¸ºä»¥ä¸‹URLç”Ÿæˆåç§°ï¼š\nURL: {url}\n\n"
        "è¿”å›JSONï¼ŒåŒ…å«ï¼š\n"
        "1. document_name: æ–‡æ¡£çš„ä¸­æ–‡åç§°\n"
        "2. collection_name: è‹±æ–‡collectionåç§°ï¼ˆå°å†™+ä¸‹åˆ’çº¿ï¼‰\n\n"
        "ç¤ºä¾‹ï¼š{\\\"document_name\\\": \\\"æœºå™¨å­¦ä¹ å…¥é—¨æŒ‡å—\\\", \\\"collection_name\\\": \\\"machine_learning_guide\\\"}"
    )
    
    try:
        # ç»Ÿä¸€é€šè¿‡ llm_client è°ƒç”¨
        response_content = await chat_complete(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            model=model,
            timeout=300,
        )

        # å°è¯•è§£æJSON
        import re
        json_match = re.search(r'\{[\s\S]*\}', response_content.strip())
        if json_match:
            try:
                result = json.loads(json_match.group())
                # ç»“æœåŸºæœ¬æ ¡éªŒä¸æ¸…ç†
                doc_name = str(result.get("document_name") or "").strip() or (url.split('/')[-1] or "æœªå‘½åæ–‡æ¡£")
                coll_name = str(result.get("collection_name") or "").strip()
                # è§„èŒƒåŒ– collection_nameï¼šå°å†™ã€ä¸‹åˆ’çº¿ã€å»é™¤éæ³•å­—ç¬¦
                import re as _re
                coll_name = _re.sub(r"[^a-z0-9_]", "_", coll_name.lower()) or f"doc_{hashlib.md5(url.encode()).hexdigest()[:8]}"
                return {"document_name": doc_name, "collection_name": coll_name}
            except Exception:
                pass
        # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨é»˜è®¤åç§°
        return {
            "document_name": url.split('/')[-1] or "æœªå‘½åæ–‡æ¡£",
            "collection_name": f"doc_{hashlib.md5(url.encode()).hexdigest()[:8]}"
        }
    except Exception as e:
        print(f"ç”Ÿæˆæ–‡æ¡£åç§°å¤±è´¥: {e}")
        return {
            "document_name": url.split('/')[-1] or "æœªå‘½åæ–‡æ¡£",
            "collection_name": f"doc_{hashlib.md5(url.encode()).hexdigest()[:8]}"
        }


@router.post("/auto-ingest", summary="æ™ºèƒ½æ–‡æ¡£æ‘„å–æ¥å£ï¼ˆç»Ÿä¸€å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚å’Œwebhookå›è°ƒï¼‰")
async def auto_ingest(
    background_tasks: BackgroundTasks,
    data: dict = Body(...),
    db: AsyncSession = Depends(get_db)
):
    """
    ç»Ÿä¸€çš„æ™ºèƒ½æ–‡æ¡£æ‘„å–æ¥å£ï¼š
    - å®¢æˆ·ç«¯è¯·æ±‚ï¼šè·å–URLå¹¶ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ–‡æ¡£åç§°å’Œcollectionåç§°ï¼Œæ‹‰å–å¹¶å¤„ç†å†…å®¹ï¼Œåˆ›å»ºæ–°çš„å‘é‡åº“collectionå­˜å‚¨ï¼Œå‘é€webhooké€šçŸ¥
    - webhookå›è°ƒï¼šå¤„ç†å·¥ä½œæµå“åº”æ•°æ®å¹¶æ‰§è¡Œé€’å½’æ‘„å–é€»è¾‘
    
    é€šè¿‡æ£€æµ‹æ•°æ®ä¸­æ˜¯å¦åŒ…å« 'task_name' å­—æ®µæ¥åˆ¤æ–­è¯·æ±‚ç±»å‹
    """
    
    # æ£€æµ‹è¯·æ±‚ç±»å‹ï¼šå¦‚æœåŒ…å« task_name å­—æ®µï¼Œè¯´æ˜æ˜¯ webhook å›è°ƒ
    # éœ€è¦è€ƒè™‘æ•°æ®å¯èƒ½åµŒå¥—åœ¨ body å­—æ®µä¸­çš„æƒ…å†µ
    is_webhook_callback = 'task_name' in data or ('body' in data and isinstance(data.get('body'), dict) and 'task_name' in data['body'])
    
    if is_webhook_callback:
        # å¤„ç† webhook å›è°ƒ
        print("æ£€æµ‹åˆ°webhookå›è°ƒè¯·æ±‚")
        # ä¿ç•™æ¥å£ï¼Œä½†ä¸æ‰§è¡Œä»»ä½•é€’å½’/å­˜å‚¨é€»è¾‘ï¼ˆé¢„ç•™æœªæ¥ç”¨é€”ï¼‰
        print("æ£€æµ‹åˆ°webhookå›è°ƒè¯·æ±‚ï¼ˆå ä½å®ç°ï¼Œä¸æ‰§è¡Œä¸šåŠ¡é€»è¾‘ï¼‰")
        return {"success": True, "message": "webhook endpoint reserved"}
    
    # å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
    print("æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ‘„å–è¯·æ±‚")
    url = data.get("url")
    if not url:
        raise HTTPException(status_code=400, detail="URLå¿…é¡»æä¾›")

    model = data.get("model", DEFAULT_INGEST_MODEL)  # è·å–æ¨¡å‹å‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨ DEFAULT_INGEST_MODEL
    embedding_model = data.get("embedding_model", DEFAULT_EMBEDDING_MODEL)
    embedding_dimensions = data.get("embedding_dimensions", EMBEDDING_DIMENSIONS)
    webhook_url = data.get("webhook_url", WEBHOOK_PREFIX + "/array2array")
    recursive_depth = data.get("recursive_depth", 2)  # é»˜è®¤é€’å½’æ·±åº¦ä¸º2
    is_recursive = data.get("is_recursive", False)  # æ£€æµ‹æ˜¯å¦ä¸ºé€’å½’è°ƒç”¨
    # æ˜¯å¦å¯ç”¨ webhook å…œåº•ï¼Œå¯è¢«è¯·æ±‚å‚æ•°è¦†ç›–
    use_webhook_fallback = data.get("webhook_fallback", SUBDOC_USE_WEBHOOK_FALLBACK)
    try:
        # 1. ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ–‡æ¡£åç§°å’Œcollectionåç§°ï¼ˆä»…åœ¨éé€’å½’è°ƒç”¨æ—¶ï¼‰
        print("æ­£åœ¨ç”Ÿæˆæ–‡æ¡£åç§°...")
        if is_recursive:
            # é€’å½’è°ƒç”¨æ—¶ï¼Œä»æ•°æ®ä¸­è·å–å·²æœ‰çš„æ–‡æ¡£åç§°å’Œcollectionåç§°
            document_name = data.get("document_name")
            collection_name = data.get("collection_name")
            collection_id = data.get("collection_id")
            if not document_name or not collection_name:
                # å¦‚æœé€’å½’è°ƒç”¨æ—¶ç¼ºå°‘æ–‡æ¡£åç§°æˆ–collectionåç§°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                document_name = document_name or f"å­æ–‡æ¡£_{url.split('/')[-1] or 'æœªå‘½å'}"
                url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
                collection_name = collection_name or f"subdoc_{url_hash}"
            # å¦‚æœé€’å½’æœªä¼ é€’ collection_idï¼Œåˆ™åŸºäºçˆ¶URLè§„åˆ™å›é€€è®¡ç®—
            if not collection_id:
                parent_url = determine_parent_url(url)
                parent_hash = hashlib.md5(parent_url.encode()).hexdigest()[:8]
                collection_id = f"collection_{parent_hash}"
            print(f"æ£€æµ‹åˆ°é€’å½’è°ƒç”¨ï¼Œä½¿ç”¨å·²æœ‰çš„æ–‡æ¡£åç§°: {document_name}, collectionåç§°: {collection_name}")
        else:
            # éé€’å½’è°ƒç”¨æ—¶ï¼Œæ­£å¸¸ç”Ÿæˆæ–‡æ¡£åç§°å’Œcollectionåç§°
            names = await generate_document_names(url, model)
            document_name = names["document_name"]
            # ä½¿ç”¨URLçš„hashç”Ÿæˆç¨³å®šçš„collectionåç§°ï¼Œç¡®ä¿åŒä¸€URLæ€»æ˜¯å¾—åˆ°ç›¸åŒçš„collection_name
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            collection_name = f"collection_{url_hash}"
            # ç»Ÿä¸€ï¼šåŸºäº determine_parent_url(entry_url) ç”Ÿæˆç¨³å®šé›†åˆID
            parent_url = determine_parent_url(url)
            parent_hash = hashlib.md5(parent_url.encode()).hexdigest()[:8]
            collection_id = f"collection_{parent_hash}"
        
        print(f"æ–‡æ¡£åç§°: {document_name}")
        print(f"Collectionåç§°: {collection_name}")

        # 2. æ‹‰å–å¹¶è§£æå†…å®¹
        print("æ­£åœ¨æ‹‰å–ç½‘é¡µå†…å®¹...")
        text = await fetch_then_extract(url)
        # ä»…è·å–HTMLç”¨äºå­æ–‡æ¡£é“¾æ¥æå–ï¼Œä¸ç”¨äºåˆ†å—å­˜å‚¨
        raw_html = await fetch_html(url)

        # 3. åˆ†å—å¤„ç†æ–‡æœ¬ï¼ˆä»…å¤„ç†plaintextï¼‰
        print("æ­£åœ¨åˆ†å—å¤„ç†æ–‡æœ¬...")
        chunks = chunk_text(text)
        if not chunks:
            raise ValueError("æ— æ³•ä»URLä¸­æå–ä»»ä½•å†…å®¹")

        total_chunks = len(chunks)
        print(f"æ€»å…±ç”Ÿæˆäº† {total_chunks} ä¸ªæ–‡æœ¬å—")

        # 4. åˆ›å»ºæˆ–è·å–Sourceå¯¹è±¡ (æ”¯æŒUPSERTæ“ä½œ)ï¼Œå¹¶æŒä¹…åŒ– collection_id
        FIXED_SESSION_ID = "fixed_session_id_for_auto_ingest"
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒURLçš„Sourceè®°å½•ï¼ˆå…¨å±€å”¯ä¸€çº¦æŸï¼Œä¸è€ƒè™‘session_idï¼‰
        existing_source_stmt = select(Source).where(Source.url == url)
        existing_source_result = await db.execute(existing_source_stmt)
        existing_source = existing_source_result.scalars().first()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé€’å½’è°ƒç”¨ä¸”æä¾›äº†parent_source_id
        parent_source_id = data.get("parent_source_id")
        from datetime import datetime
        if is_recursive and parent_source_id:
            # é€’å½’è°ƒç”¨æ—¶ï¼Œä¸ºæ¯ä¸ªå­æ–‡æ¡£åˆ›å»ºç‹¬ç«‹çš„Sourceè®°å½•
            print(f"é€’å½’è°ƒç”¨ï¼šä¸ºå­æ–‡æ¡£åˆ›å»ºç‹¬ç«‹çš„Sourceè®°å½•ï¼Œçˆ¶çº§ID: {parent_source_id}")
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒURLçš„Sourceè®°å½•
            if existing_source:
                print(f"å­æ–‡æ¡£å·²å­˜åœ¨ï¼Œæ›´æ–°ç°æœ‰çš„Source: {existing_source.title} (ID: {existing_source.id})")
                # æ›´æ–°ç°æœ‰è®°å½•çš„ä¿¡æ¯
                existing_source.title = document_name
                existing_source.session_id = FIXED_SESSION_ID
                existing_source.collection_id = collection_id
                existing_source.created_at = datetime.now(pytz.timezone('Asia/Shanghai'))
                source = existing_source
                
                # åˆ é™¤ä¸ç°æœ‰Sourceç›¸å…³çš„æ—§chunksï¼Œå‡†å¤‡é‡æ–°å¤„ç†æœ€æ–°å†…å®¹
                print("åˆ é™¤æ—§çš„chunks...")
                await db.execute(delete(Chunk).where(Chunk.source_id == source.id))
                await db.flush()
            else:
                print(f"ä¸ºå­æ–‡æ¡£åˆ›å»ºæ–°çš„Sourceè®°å½•: {document_name}")
                source = Source(url=url, title=document_name, session_id=FIXED_SESSION_ID, collection_id=collection_id)
        else:
            # éé€’å½’è°ƒç”¨æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸åŒURLçš„Source (UPSERTé€»è¾‘)
            if existing_source:
                print(f"éé€’å½’è°ƒç”¨ï¼šæ›´æ–°ç°æœ‰Source: {existing_source.title} (ID: {existing_source.id})")
                # æ›´æ–°ç°æœ‰è®°å½•çš„æ‰€æœ‰ä¿¡æ¯
                existing_source.title = document_name
                existing_source.session_id = FIXED_SESSION_ID  # æ›´æ–° session_id
                existing_source.collection_id = collection_id
                existing_source.created_at = datetime.now(pytz.timezone('Asia/Shanghai'))
                source = existing_source
                
                # åˆ é™¤ä¸ç°æœ‰Sourceç›¸å…³çš„æ—§chunksï¼Œå‡†å¤‡é‡æ–°å¤„ç†æœ€æ–°å†…å®¹
                print("åˆ é™¤æ—§çš„chunks...")
                await db.execute(delete(Chunk).where(Chunk.source_id == source.id))
                await db.flush()
            else:
                print("éé€’å½’è°ƒç”¨ï¼šåˆ›å»ºæ–°çš„Sourceå¯¹è±¡")
                source = Source(url=url, title=document_name, session_id=FIXED_SESSION_ID, collection_id=collection_id)
        
        # åˆ›å»ºChunkå¯¹è±¡åˆ—è¡¨
        chunk_objects = []
        for index, text in enumerate(chunks):
            # ç”Ÿæˆå”¯ä¸€çš„chunk_id
            raw = f"{FIXED_SESSION_ID}|{url}|{index}".encode("utf-8", errors="ignore")
            generated_chunk_id = hashlib.md5(raw).hexdigest()
            chunk_obj = Chunk(
                chunk_id=generated_chunk_id,
                content=text,
                source_id=None,  # åœ¨æ•°æ®åº“ä¸­æš‚æ—¶ä¸è®¾ç½®source_id
                session_id=FIXED_SESSION_ID,
            )
            chunk_objects.append(chunk_obj)
        
        
        
        # 5. å°†chunkå¯¹è±¡ä¿å­˜åˆ°æ•°æ®åº“
        print(f"æ­£åœ¨ä¿å­˜chunkåˆ°æ•°æ®åº“... (æ–‡æœ¬å—: {len(chunk_objects)})")
        
        # åªæœ‰åœ¨åˆ›å»ºæ–°Sourceæ—¶æ‰éœ€è¦æ·»åŠ åˆ°æ•°æ®åº“
        if not (is_recursive and parent_source_id and source.id):
            db.add(source)
            await db.flush()
        
        # ä¸ºæ¯ä¸ªchunkè®¾ç½®source_id
        for chunk in chunk_objects:
            chunk.source_id = source.id
        
        db.add_all(chunk_objects)
        await db.flush()
        # æå‰æäº¤ï¼Œç¼©çŸ­äº‹åŠ¡å ç”¨æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´å†™é”
        await db.commit()
        
        # 6. ä¸ºæ‰€æœ‰chunkç”ŸæˆåµŒå…¥å‘é‡å¹¶å­˜å‚¨åˆ°Qdrant
        print("æ­£åœ¨ç”ŸæˆåµŒå…¥...")
        MAX_PARALLEL = int(EMBEDDING_MAX_CONCURRENCY)
        BATCH_SIZE = int(EMBEDDING_BATCH_SIZE)

        # åˆ†æ‰¹æ„é€ ä»»åŠ¡ï¼šæ¯ä¸ªä»»åŠ¡åªå‘èµ·ä¸€æ¬¡ /embeddings è¯·æ±‚ï¼ˆå°† batch_size ä¼ ä¸ºè¯¥æ‰¹å¤§å°ï¼‰
        chunk_batches = [
            chunk_objects[i: i + BATCH_SIZE]
            for i in range(0, len(chunk_objects), BATCH_SIZE)
        ]

        sem = asyncio.Semaphore(MAX_PARALLEL)

        async def embed_batch_worker(batch_index: int, batch_chunks: List[Chunk]):
            async with sem:
                texts = [c.content for c in batch_chunks]
                # è®©æ¯ä¸ªä»»åŠ¡åªå‘ä¸€æ¬¡è¯·æ±‚
                embeddings = await embed_texts(
                    texts,
                    model=embedding_model,
                    batch_size=len(texts),
                    dimensions=embedding_dimensions,
                )
                return batch_index, embeddings

        tasks = [
            asyncio.create_task(embed_batch_worker(idx, batch))
            for idx, batch in enumerate(chunk_batches)
        ]

        # ç­‰å¾…æ‰€æœ‰åµŒå…¥ä»»åŠ¡å®Œæˆ
        for coro in asyncio.as_completed(tasks):
            try:
                batch_index, embeddings = await coro
                batch_chunks = chunk_batches[batch_index]
                if not embeddings or len(embeddings) != len(batch_chunks):
                    # æœ¬æ‰¹å¤±è´¥æˆ–æ•°é‡ä¸ä¸€è‡´ï¼šè·³è¿‡å¹¶è®°å½•
                    print(
                        f"Embedding batch {batch_index} failed or size mismatch: got {len(embeddings) if embeddings else 0}, expected {len(batch_chunks)}"
                    )
                    continue

                # å°†è¯¥æ‰¹ç»“æœå†™å…¥å‘é‡åº“
                await add_embeddings(source.id, batch_chunks, embeddings)
            except Exception as e:
                print(f"Embedding task failed: {e}")
                # ä¸ä¸­æ–­æ•´ä½“æµç¨‹ï¼Œç»§ç»­å…¶ä»–æ‰¹æ¬¡

        # 7. æå–å­URLï¼ˆæœ¬åœ°è§£æä¼˜å…ˆï¼‰
        print("æ­£åœ¨æœ¬åœ°è§£æå­æ–‡æ¡£URL...")
        try:
            extracted_sub_docs = extract_links_from_html(raw_html, url)
            # ä¾èµ–ç»Ÿä¸€çš„ is_potential_sub_doc è§„åˆ™ï¼ˆåœ¨ link_extractor ä¸­ï¼‰
            # extract_links_from_html å·²ç»æŒ‰è§„åˆ™ç­›è¿‡ï¼›æ­¤å¤„ä»…å»é‡ä¸æ—¥å¿—
            extracted_sub_docs = list(dict.fromkeys(extracted_sub_docs))
            print(f"æœ¬åœ°è§£æåˆ° {len(extracted_sub_docs)} ä¸ªæ½œåœ¨å­æ–‡æ¡£URL")
        except Exception as e:
            print(f"æœ¬åœ°è§£æå­æ–‡æ¡£URLå¤±è´¥: {e}")
            extracted_sub_docs = []

        # 8. æœ¬åœ°å­æ–‡æ¡£å¤„ç†ï¼ˆå¼‚æ­¥åå°ï¼Œä¸é˜»å¡å“åº”ï¼‰
        if recursive_depth > 0 and extracted_sub_docs:
            print("å°†æœ¬åœ°è§£æçš„å­æ–‡æ¡£åŠ å…¥åå°å¤„ç†ä»»åŠ¡...")
            
            # ç”Ÿæˆä»»åŠ¡ID
            task_id = f"ingest_{hashlib.md5(url.encode()).hexdigest()[:8]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            if background_tasks:
                background_tasks.add_task(
                    process_sub_docs_background,
                    extracted_sub_docs,
                    recursive_depth,
                    task_id,  # ä½¿ç”¨ç”Ÿæˆçš„task_id
                    document_name,
                    collection_name,
                    source.id,
                    url,  # æ·»åŠ parent_urlå‚æ•°
                    collection_id
                )
            else:
                asyncio.create_task(
                    process_sub_docs_background(
                        extracted_sub_docs,
                        recursive_depth,
                        task_id,  # ä½¿ç”¨ç”Ÿæˆçš„task_id
                        document_name,
                        collection_name,
                        source.id,
                        url,  # æ·»åŠ parent_urlå‚æ•°
                        collection_id
                    )
                )

        # 9. ç§»é™¤ webhook å›é€€é€»è¾‘ï¼šä¸å†å‘é€/ä¾èµ– webhook è¯†åˆ«

        # å‡†å¤‡è¿”å›ç»“æœï¼ˆè¿”å›ç¨³å®š collection_id ä¾›å‰ç«¯/è°ƒç”¨æ–¹ä½¿ç”¨ï¼‰
        result = {
            "success": True,
            "message": f"æˆåŠŸæ‘„å–æ–‡æ¡£ï¼Œå…±å¤„ç†äº† {total_chunks} ä¸ªæ–‡æœ¬å—",
            "document_name": document_name,
            "collection_name": collection_name,
            "collection_id": collection_id,
            "total_chunks": total_chunks,
            "source_id": source.id  # è¿”å›Source IDç”¨äºé€’å½’è°ƒç”¨
        }
        
        # å¦‚æœå¯åŠ¨äº†å­æ–‡æ¡£åå°ä»»åŠ¡ï¼Œè¿”å›ä»»åŠ¡IDä¾›å‰ç«¯ç›‘æ§
        if recursive_depth > 0 and extracted_sub_docs:
            result["sub_docs_task_id"] = task_id
            result["sub_docs_count"] = len(extracted_sub_docs)
            result["sub_docs_processing"] = True
        
        return result

    except Exception as e:
        error_message = f"æ‘„å–å¤±è´¥: {e.__class__.__name__}: {str(e)}"
        print(error_message)
        raise HTTPException(status_code=500, detail=error_message)


@router.get("/documents", summary="è·å–é€šè¿‡auto ingestå¤„ç†çš„æ–‡æ¡£åˆ—è¡¨")
async def get_auto_ingest_documents(
    db: AsyncSession = Depends(get_db)
):
    """
    è·å–é€šè¿‡auto ingestå¤„ç†çš„æ–‡æ¡£åˆ—è¡¨
    è¿”å›æ‰€æœ‰ä½¿ç”¨å›ºå®šsession_idå­˜å‚¨çš„æ–‡æ¡£
    """
    try:
        FIXED_SESSION_ID = "fixed_session_id_for_auto_ingest"
        
        # æŸ¥è¯¢æ•°æ®åº“ä¸­çš„sourceè®°å½•
        stmt = select(Source).where(Source.session_id == FIXED_SESSION_ID)
        result = await db.execute(stmt)
        sources = result.scalars().all()
        
        documents = []
        for source in sources:
            documents.append({
                "id": source.id,
                "title": source.title,
                "url": source.url,
                "created_at": source.created_at.isoformat() if source.created_at else None
            })
        
        return {
            "success": True,
            "documents": documents,
            "total": len(documents)
        }
    
    except Exception as e:
        error_message = f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e.__class__.__name__}: {str(e)}"
        print(error_message)
        raise HTTPException(status_code=500, detail=error_message)


@router.post("/workflow_response", summary="å·¥ä½œæµå“åº”å¤„ç†æ¥å£ï¼ˆå…¼å®¹æ€§ç«¯ç‚¹ï¼‰")
async def workflow_response(
    background_tasks: BackgroundTasks,
    data: dict = Body(...),
    db: AsyncSession = Depends(get_db)
):
    """
    å·¥ä½œæµå“åº”å¤„ç†æ¥å£ - å…¼å®¹æ€§ç«¯ç‚¹
    æ­¤ç«¯ç‚¹å°†è¯·æ±‚é‡å®šå‘åˆ°ç»Ÿä¸€çš„ auto_ingest æ¥å£è¿›è¡Œå¤„ç†
    """
    print("æ”¶åˆ°workflow_responseè¯·æ±‚ï¼Œé‡å®šå‘åˆ°ç»Ÿä¸€å¤„ç†æ¥å£")
    return await auto_ingest(background_tasks, data, db)


# ========== ä»»åŠ¡ç›‘æ§APIç«¯ç‚¹ ==========

async def stream_ingest_progress(task_id: str):
    """æµå¼è¿”å›æ‘„å–ä»»åŠ¡è¿›åº¦"""
    import json
    
    while True:
        status = await ingest_task_manager.get_task_status(task_id)
        if not status:
            # ä»»åŠ¡ä¸å­˜åœ¨ï¼Œå¯èƒ½å·²å®Œæˆå¹¶è¢«æ¸…ç†
            yield f"data: {json.dumps({'error': 'Task not found', 'task_id': task_id})}\n\n"
            break
            
        # å‘é€å½“å‰çŠ¶æ€
        yield f"data: {json.dumps(status.to_dict())}\n\n"
        
        # å¦‚æœä»»åŠ¡å®Œæˆæˆ–å¤±è´¥ï¼Œç»“æŸæµ
        if status.status in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.PARTIALLY_COMPLETED]:
            break
            
        await asyncio.sleep(2)  # æ¯2ç§’æ›´æ–°ä¸€æ¬¡


@router.get("/api/ingest-progress/{task_id}", summary="è·å–æ‘„å–ä»»åŠ¡è¿›åº¦ï¼ˆæµå¼ï¼‰")
async def get_ingest_progress(task_id: str):
    """è·å–æ‘„å–ä»»åŠ¡è¿›åº¦çš„æµå¼å“åº”"""
    return StreamingResponse(
        stream_ingest_progress(task_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )


@router.get("/api/ingest-status/{task_id}", summary="è·å–æ‘„å–ä»»åŠ¡çŠ¶æ€")
async def get_ingest_status(task_id: str):
    """è·å–æ‘„å–ä»»åŠ¡çš„å½“å‰çŠ¶æ€"""
    status = await ingest_task_manager.get_task_status(task_id)
    if not status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return {
        "success": True,
        "task_id": task_id,
        "status": status.to_dict()
    }


@router.get("/api/ingest-tasks", summary="è·å–æ‰€æœ‰æ´»è·ƒçš„æ‘„å–ä»»åŠ¡")
async def list_ingest_tasks():
    """è·å–æ‰€æœ‰æ´»è·ƒçš„æ‘„å–ä»»åŠ¡åˆ—è¡¨"""
    tasks = await ingest_task_manager.list_active_tasks()
    return {
        "success": True,
        "tasks": [task.to_dict() for task in tasks],
        "total": len(tasks)
    }


@router.delete("/api/ingest-task/{task_id}", summary="åˆ é™¤æ‘„å–ä»»åŠ¡")
async def delete_ingest_task(task_id: str):
    """åˆ é™¤æŒ‡å®šçš„æ‘„å–ä»»åŠ¡ï¼ˆé€šå¸¸åœ¨ä»»åŠ¡å®Œæˆåæ¸…ç†ï¼‰"""
    success = await ingest_task_manager.remove_task(task_id)
    if not success:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return {
        "success": True,
        "message": f"ä»»åŠ¡ {task_id} å·²åˆ é™¤"
    }
