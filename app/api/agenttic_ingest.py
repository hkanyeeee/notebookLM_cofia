import json
import hashlib
import uuid
from typing import List, Optional
from datetime import datetime
import httpx
import asyncio

from fastapi import APIRouter, Body, HTTPException, Depends, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, field_validator
import pytz
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from sqlalchemy import delete

from ..models import Source, Chunk, WorkflowExecution
from ..database import get_db
from ..fetch_parse import fetch_then_extract, fetch_html
from ..utils.link_extractor import extract_links_from_html
from ..utils.task_status import ingest_task_manager, TaskStatus
from app.config import LLM_SERVICE_URL, DEFAULT_INGEST_MODEL, SUBDOC_MAX_CONCURRENCY
from ..chunking import chunk_text
from ..embedding_client import embed_texts, DEFAULT_EMBEDDING_MODEL
from ..config import WEBHOOK_TIMEOUT, WEBHOOK_PREFIX, EMBEDDING_MAX_CONCURRENCY, EMBEDDING_BATCH_SIZE, EMBEDDING_DIMENSIONS, SUBDOC_USE_WEBHOOK_FALLBACK
from ..vector_db_client import add_embeddings


router = APIRouter()


class WebhookResponseData(BaseModel):
    """ç”¨äºéªŒè¯webhookè¿”å›æ•°æ®ç»“æ„çš„æ¨¡å‹"""
    document_name: str
    collection_name: str
    url: str
    total_chunks: int
    source_id: Optional[str] = None
    session_id: Optional[str] = None
    task_name: str
    output: Optional[List[dict]] = None  # æ·»åŠ outputå­—æ®µä»¥åŒ…å«sub_docs
    recursive_depth: Optional[int] = 1  # æ·»åŠ é€’å½’æ·±åº¦å­—æ®µï¼Œé»˜è®¤ä¸º1
    request_id: Optional[str] = None  # è¯·æ±‚ID
    webhook_url: Optional[str] = None  # webhook URL
    is_recursive: Optional[bool] = False  # æ·»åŠ é€’å½’æ ‡è®°å­—æ®µï¼Œé»˜è®¤ä¸ºFalse
    
    @field_validator('total_chunks', mode='before')
    @classmethod
    def validate_total_chunks(cls, v):
        """ç¡®ä¿total_chunksæ˜¯æ•´æ•°ç±»å‹"""
        if isinstance(v, str):
            try:
                return int(v)
            except ValueError:
                raise ValueError(f"æ— æ³•å°†total_chunksè½¬æ¢ä¸ºæ•´æ•°: {v}")
        return v
        
    @field_validator('recursive_depth', mode='before')
    @classmethod
    def validate_recursive_depth(cls, v):
        """ç¡®ä¿recursive_depthæ˜¯æ•´æ•°ç±»å‹"""
        if v is None:
            return 2  # é»˜è®¤å€¼
        if isinstance(v, str):
            try:
                return int(v)
            except ValueError:
                raise ValueError(f"æ— æ³•å°†recursive_depthè½¬æ¢ä¸ºæ•´æ•°: {v}")
        return v


class UnifiedIngestRequest(BaseModel):
    """ç»Ÿä¸€çš„æ‘„å–è¯·æ±‚æ¨¡å‹ï¼Œæ”¯æŒå®¢æˆ·ç«¯è¯·æ±‚å’Œwebhookå›è°ƒ"""
    # å®¢æˆ·ç«¯è¯·æ±‚å¿…éœ€å­—æ®µ
    url: str
    
    # å®¢æˆ·ç«¯è¯·æ±‚å¯é€‰å­—æ®µ
    embedding_model: Optional[str] = None
    embedding_dimensions: Optional[int] = None
    webhook_url: Optional[str] = None
    recursive_depth: Optional[int] = None
    
    # webhookå›è°ƒä¸“æœ‰å­—æ®µ
    document_name: Optional[str] = None
    collection_name: Optional[str] = None
    total_chunks: Optional[int] = None
    chunks: Optional[List[dict]] = None
    source_id: Optional[str] = None
    parent_source_id: Optional[int] = None  # æ·»åŠ çˆ¶çº§Source IDå­—æ®µ
    session_id: Optional[str] = None
    task_name: Optional[str] = None
    is_recursive: Optional[bool] = False  # æ·»åŠ é€’å½’æ ‡è®°å­—æ®µï¼Œé»˜è®¤ä¸ºFalse


async def process_sub_docs_concurrent(
    sub_docs_urls: List[str],
    recursive_depth: int,
    db: AsyncSession,
    parent_doc_name: Optional[str] = None,
    parent_collection_name: Optional[str] = None,
    parent_source_id: Optional[int] = None
) -> List[dict]:
    """
    å¹¶å‘å¤„ç†å­æ–‡æ¡£çš„é€’å½’æ‘„å–å‡½æ•°

    Args:
        sub_docs_urls: å­æ–‡æ¡£URLåˆ—è¡¨
        recursive_depth: é€’å½’æ·±åº¦é™åˆ¶
        db: æ•°æ®åº“ä¼šè¯
        parent_doc_name: çˆ¶çº§æ–‡æ¡£åç§°
        parent_collection_name: çˆ¶çº§collectionåç§°
        parent_source_id: çˆ¶çº§Source IDï¼Œç”¨äºå°†å­æ–‡æ¡£å†…å®¹æ·»åŠ åˆ°åŒä¸€ä¸ªcollection

    Returns:
        List[dict]: æ¯ä¸ªå­æ–‡æ¡£çš„å¤„ç†ç»“æœ
    """
    results = []
    
    # æ£€æŸ¥é€’å½’æ·±åº¦é™åˆ¶
    if recursive_depth <= 0:
        print(f"è¾¾åˆ°é€’å½’æ·±åº¦é™åˆ¶ï¼Œè·³è¿‡ {len(sub_docs_urls)} ä¸ªå­æ–‡æ¡£çš„å¤„ç†")
        return results
    
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡ - å­æ–‡æ¡£å¤„ç†ä½¿ç”¨æ›´é«˜çš„å¹¶å‘æ•°
    MAX_CONCURRENT_SUB_DOCS = min(SUBDOC_MAX_CONCURRENCY, len(sub_docs_urls))  # ä½¿ç”¨é…ç½®çš„å¹¶å‘æ•°ï¼Œä½†ä¸è¶…è¿‡å­æ–‡æ¡£æ€»æ•°
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_SUB_DOCS)
    
    # å¹¶å‘å¤„ç†æ‰€æœ‰å­æ–‡æ¡£URL
    async def process_single_sub_doc(sub_url: str, parent_doc_name: str = None, parent_collection_name: str = None, parent_source_id: int = None) -> dict:
        async with semaphore:  # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            try:
                print(f"å¼€å§‹é€’å½’æ‘„å–å­æ–‡æ¡£: {sub_url}")

                # æ„é€ é€’å½’è°ƒç”¨çš„è¯·æ±‚æ•°æ®
                sub_request_data = {
                    "url": sub_url,
                    "recursive_depth": recursive_depth - 1,  # å‡å°‘é€’å½’æ·±åº¦
                    "embedding_model": DEFAULT_EMBEDDING_MODEL,
                    "embedding_dimensions": EMBEDDING_DIMENSIONS,
                    "webhook_url": WEBHOOK_PREFIX + "/array2array",
                    "is_recursive": True,  # æ ‡è®°ä¸ºé€’å½’è°ƒç”¨
                    "document_name": parent_doc_name,  # ä¼ é€’çˆ¶çº§æ–‡æ¡£åç§°
                    "collection_name": parent_collection_name,  # ä¼ é€’çˆ¶çº§collectionåç§°
                    "parent_source_id": parent_source_id  # ä¼ é€’çˆ¶çº§Source ID
                }
                
                # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„BackgroundTaskså®ä¾‹ç”¨äºé€’å½’è°ƒç”¨
                dummy_background_tasks = BackgroundTasks()
                
                # è°ƒç”¨æœ¬æ¨¡å—çš„agenttic_ingestå‡½æ•°è¿›è¡Œé€’å½’å¤„ç†ï¼Œå‚æ•°é¡ºåºè¦æ­£ç¡®
                result = await agenttic_ingest(dummy_background_tasks, sub_request_data, db)
                
                print(f"å­æ–‡æ¡£æ‘„å–æˆåŠŸ: {sub_url}")
                return {
                    "url": sub_url,
                    "success": True,
                    "result": result
                }
                
            except Exception as e:
                error_msg = f"å­æ–‡æ¡£æ‘„å–å¤±è´¥ {sub_url}: {str(e)}"
                print(error_msg)
                return {
                    "url": sub_url,
                    "success": False,
                    "error": error_msg
                }
    
    # ä½¿ç”¨asyncio.gatherè¿›è¡Œå¹¶å‘å¤„ç†ï¼Œä½†é€šè¿‡ä¿¡å·é‡æ§åˆ¶æœ€å¤§å¹¶å‘æ•°
    try:
        tasks = [process_single_sub_doc(url, parent_doc_name, parent_collection_name, parent_source_id) for url in sub_docs_urls]
        results = await asyncio.gather(*tasks, return_exceptions=False)
    except Exception as e:
        print(f"å¹¶å‘å¤„ç†å­æ–‡æ¡£æ—¶å‡ºç°å¼‚å¸¸: {str(e)}")
        # é™çº§åˆ°ä¸²è¡Œå¤„ç†
        results = []
        for sub_url in sub_docs_urls:
            result = await process_single_sub_doc(sub_url, parent_doc_name, parent_collection_name, parent_source_id)
            results.append(result)
    
    return results


async def process_sub_docs_concurrent_with_tracking(
    sub_docs_urls: List[str],
    recursive_depth: int,
    db: AsyncSession,
    parent_doc_name: Optional[str] = None,
    parent_collection_name: Optional[str] = None,
    parent_source_id: Optional[int] = None,
    task_id: Optional[str] = None
) -> List[dict]:
    """
    å¹¶å‘å¤„ç†å­æ–‡æ¡£çš„é€’å½’æ‘„å–å‡½æ•°ï¼ˆå¸¦çŠ¶æ€è¿½è¸ªï¼‰
    """
    results = []
    
    # æ£€æŸ¥é€’å½’æ·±åº¦é™åˆ¶
    if recursive_depth <= 0:
        print(f"è¾¾åˆ°é€’å½’æ·±åº¦é™åˆ¶ï¼Œè·³è¿‡ {len(sub_docs_urls)} ä¸ªå­æ–‡æ¡£çš„å¤„ç†")
        return results
    
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡ - å­æ–‡æ¡£å¤„ç†ä½¿ç”¨æ›´é«˜çš„å¹¶å‘æ•°
    MAX_CONCURRENT_SUB_DOCS = min(SUBDOC_MAX_CONCURRENCY, len(sub_docs_urls))  # ä½¿ç”¨é…ç½®çš„å¹¶å‘æ•°ï¼Œä½†ä¸è¶…è¿‡å­æ–‡æ¡£æ€»æ•°
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_SUB_DOCS)
    
    # å¹¶å‘å¤„ç†æ‰€æœ‰å­æ–‡æ¡£URL
    async def process_single_sub_doc_with_tracking(sub_url: str) -> dict:
        async with semaphore:
            # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
            if task_id:
                await ingest_task_manager.update_sub_doc_status(task_id, sub_url, TaskStatus.RUNNING)
            
            # ğŸ”¥ ä¿®å¤ï¼šä¸ºæ¯ä¸ªå­æ–‡æ¡£åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼Œé¿å…äº‹åŠ¡å†²çª
            from ..database import AsyncSessionLocal
            async with AsyncSessionLocal() as sub_db:
                try:
                    print(f"å¼€å§‹é€’å½’æ‘„å–å­æ–‡æ¡£: {sub_url}")

                    # æ„é€ é€’å½’è°ƒç”¨çš„è¯·æ±‚æ•°æ®
                    sub_request_data = {
                        "url": sub_url,
                        "recursive_depth": recursive_depth - 1,  # å‡å°‘é€’å½’æ·±åº¦
                        "embedding_model": DEFAULT_EMBEDDING_MODEL,
                        "embedding_dimensions": EMBEDDING_DIMENSIONS,
                        "webhook_url": WEBHOOK_PREFIX + "/array2array",
                        "is_recursive": True,  # æ ‡è®°ä¸ºé€’å½’è°ƒç”¨
                        "document_name": parent_doc_name,  # ä¼ é€’çˆ¶çº§æ–‡æ¡£åç§°
                        "collection_name": parent_collection_name,  # ä¼ é€’çˆ¶çº§collectionåç§°
                        "parent_source_id": parent_source_id  # ä¼ é€’çˆ¶çº§Source ID
                    }
                    
                    # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„BackgroundTaskså®ä¾‹ç”¨äºé€’å½’è°ƒç”¨
                    dummy_background_tasks = BackgroundTasks()
                    
                    # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯è¿›è¡Œé€’å½’è°ƒç”¨
                    result = await agenttic_ingest(dummy_background_tasks, sub_request_data, sub_db)
                    
                    print(f"å­æ–‡æ¡£æ‘„å–æˆåŠŸ: {sub_url}")
                    
                    # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
                    if task_id:
                        await ingest_task_manager.update_sub_doc_status(task_id, sub_url, TaskStatus.COMPLETED)
                    
                    return {
                        "url": sub_url,
                        "success": True,
                        "result": result
                    }
                    
                except Exception as e:
                    error_msg = f"å­æ–‡æ¡£æ‘„å–å¤±è´¥ {sub_url}: {str(e)}"
                    print(error_msg)
                    
                    # ç¡®ä¿å‡ºé”™æ—¶ä¹Ÿå›æ»šå­æ–‡æ¡£çš„æ•°æ®åº“ä¼šè¯
                    try:
                        await sub_db.rollback()
                    except:
                        pass
                    
                    # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
                    if task_id:
                        await ingest_task_manager.update_sub_doc_status(task_id, sub_url, TaskStatus.FAILED, error_msg)
                    
                    return {
                        "url": sub_url,
                        "success": False,
                        "error": error_msg
                    }
    
    # ä½¿ç”¨asyncio.gatherè¿›è¡Œå¹¶å‘å¤„ç†ï¼Œä½†é€šè¿‡ä¿¡å·é‡æ§åˆ¶æœ€å¤§å¹¶å‘æ•°
    try:
        tasks = [process_single_sub_doc_with_tracking(url) for url in sub_docs_urls]
        results = await asyncio.gather(*tasks, return_exceptions=False)
    except Exception as e:
        print(f"å¹¶å‘å¤„ç†å­æ–‡æ¡£æ—¶å‡ºç°å¼‚å¸¸: {str(e)}")
        # é™çº§åˆ°ä¸²è¡Œå¤„ç†
        results = []
        for sub_url in sub_docs_urls:
            result = await process_single_sub_doc_with_tracking(sub_url)
            results.append(result)
    
    return results


async def process_sub_docs_background(
    sub_docs_urls: List[str],
    recursive_depth: int,
    request_id: Optional[str] = None,
    parent_doc_name: Optional[str] = None,
    parent_collection_name: Optional[str] = None,
    parent_source_id: Optional[int] = None,
    parent_url: Optional[str] = None  # æ–°å¢ï¼šçˆ¶æ–‡æ¡£URLï¼Œç”¨äºä»»åŠ¡è¿½è¸ª
):
    """
    åå°å¼‚æ­¥å¤„ç†å­æ–‡æ¡£çš„å‡½æ•° - ä¸é˜»å¡ä¸»å“åº”

    Args:
        sub_docs_urls: å­æ–‡æ¡£URLåˆ—è¡¨
        recursive_depth: é€’å½’æ·±åº¦é™åˆ¶
        request_id: è¯·æ±‚IDï¼Œç”¨äºæ—¥å¿—è¿½è¸ª
        parent_doc_name: çˆ¶çº§æ–‡æ¡£åç§°
        parent_collection_name: çˆ¶çº§collectionåç§°
        parent_source_id: çˆ¶çº§Source IDï¼Œç”¨äºå°†å­æ–‡æ¡£å†…å®¹æ·»åŠ åˆ°åŒä¸€ä¸ªcollection
        parent_url: çˆ¶æ–‡æ¡£URLï¼Œç”¨äºä»»åŠ¡è¿½è¸ª
    """
    task_id = request_id or f"subdoc_{hash(str(sub_docs_urls))}"
    
    try:
        # åˆ›å»ºä»»åŠ¡çŠ¶æ€è¿½è¸ª
        if parent_url and parent_doc_name and parent_collection_name:
            await ingest_task_manager.create_task(
                task_id=task_id,
                parent_url=parent_url,
                document_name=parent_doc_name,
                collection_name=parent_collection_name,
                sub_doc_urls=sub_docs_urls
            )
            await ingest_task_manager.start_task(task_id)
        
        print(f"[åå°ä»»åŠ¡] å¼€å§‹å¤„ç† {len(sub_docs_urls)} ä¸ªå­æ–‡æ¡£ï¼Œtask_id: {task_id}")

        # ğŸ”¥ ä¿®å¤ï¼šä¸å†éœ€è¦å…±äº«æ•°æ®åº“ä¼šè¯ï¼Œæ¯ä¸ªå­æ–‡æ¡£ä½¿ç”¨ç‹¬ç«‹ä¼šè¯
        results = await process_sub_docs_concurrent_with_tracking(
            sub_docs_urls, recursive_depth, None, parent_doc_name, 
            parent_collection_name, parent_source_id, task_id
        )

        success_count = len([r for r in results if r.get('success')])
        print(f"[åå°ä»»åŠ¡] å­æ–‡æ¡£å¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{len(results)}, task_id: {task_id}")
        
        # ğŸ”¥ ä¿®å¤ï¼šä¸å†éœ€è¦ç»Ÿä¸€æäº¤ï¼Œæ¯ä¸ªå­æ–‡æ¡£ä¼šè¯å·²ç‹¬ç«‹æäº¤
        print(f"[åå°ä»»åŠ¡] æ‰€æœ‰å­æ–‡æ¡£å·²ç‹¬ç«‹å¤„ç†å’Œæäº¤ï¼Œå¤„ç†äº† {success_count} ä¸ªå­æ–‡æ¡£")
                
    except Exception as e:
        error_msg = f"å­æ–‡æ¡£å¤„ç†å¼‚å¸¸: {str(e)}"
        print(f"[åå°ä»»åŠ¡] {error_msg}, task_id: {task_id}")
        await ingest_task_manager.fail_task(task_id, error_msg)
        
        # ğŸ”¥ ä¿®å¤ï¼šä¸å†éœ€è¦ç»Ÿä¸€å›æ»šï¼Œæ¯ä¸ªå­æ–‡æ¡£ä¼šè¯ä¼šè‡ªåŠ¨ç®¡ç†äº‹åŠ¡


async def process_webhook_response(
    data: WebhookResponseData,
    db: AsyncSession,
    background_tasks: BackgroundTasks = None
):
    """
    å¤„ç†webhookå›è°ƒæ•°æ®çš„ä¸“ç”¨å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå­æ–‡æ¡£å¤„ç†æ”¹ä¸ºåå°ä»»åŠ¡
    """
    print("å¤„ç†webhookå“åº”æ•°æ®...")
    print(f"ä»»åŠ¡åç§°: {data.task_name}")
    print(f"æ–‡æ¡£åç§°: {data.document_name}")
    print(f"Collectionåç§°: {data.collection_name}")
    print(f"æ€»å—æ•°: {data.total_chunks}")
    
    # æ£€æŸ¥ä»»åŠ¡åç§°
    if data.task_name != "agenttic_ingest":
        return {
            "message": f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {data.task_name}",
            "task_name": data.task_name,
            "success": False
        }
    
    # å®ç°é€’å½’æ‘„å–é€»è¾‘ï¼šæ”¶é›†webhookå“åº”ä¸­çš„å­æ–‡æ¡£URL
    total_sub_docs = 0
    
    if data.output and isinstance(data.output, list):
        print(f"æ£€æµ‹åˆ°å“åº”æ•°æ®ï¼Œå…± {len(data.output)} ä¸ªå“åº”é¡¹")
        
        # æ”¶é›†æ‰€æœ‰å­æ–‡æ¡£URLï¼Œå¹¶è¿›è¡Œå»é‡å¤„ç†
        all_sub_docs = []
        seen_urls = set()  # ç”¨äºè·Ÿè¸ªå·²æ·»åŠ çš„URLï¼Œé¿å…é‡å¤

        for i, output_item in enumerate(data.output):
            if isinstance(output_item, dict) and "response" in output_item:
                response_data = output_item.get("response", {})
                if isinstance(response_data, dict) and "sub_docs" in response_data:
                    sub_docs = response_data.get("sub_docs", [])
                    if isinstance(sub_docs, list):
                        print(f"å“åº”é¡¹ {i} åŒ…å« {len(sub_docs)} ä¸ªå­æ–‡æ¡£")
                        total_sub_docs += len(sub_docs)

                        # é€ä¸ªæ£€æŸ¥å¹¶æ·»åŠ URLï¼Œé¿å…é‡å¤
                        from urllib.parse import urlparse
                        def is_strict_child(child: str, base: str) -> bool:
                            try:
                                cu = urlparse(child)
                                bu = urlparse(base)
                                if cu.netloc != bu.netloc:
                                    return False
                                base_path = bu.path.rstrip('/')
                                url_path = cu.path.rstrip('/')
                                # ä»…å…è®¸ä¸¥æ ¼å­è·¯å¾„ï¼Œå¦‚ /docs/python/*
                                return url_path.startswith(base_path + '/')
                            except Exception:
                                return False

                        for url in sub_docs:
                            if url and is_strict_child(url, data.url) and url not in seen_urls:
                                all_sub_docs.append(url)
                                seen_urls.add(url)
                            elif url and not is_strict_child(url, data.url):
                                print(f"è·³è¿‡å…„å¼Ÿ/éå­è·¯å¾„URL: {url}")
                            elif url in seen_urls:
                                print(f"è·³è¿‡é‡å¤çš„å­æ–‡æ¡£URL: {url}")
                    else:
                        print(f"å“åº”é¡¹ {i} çš„ sub_docs ä¸æ˜¯åˆ—è¡¨æ ¼å¼: {type(sub_docs)}")
                else:
                    print(f"å“åº”é¡¹ {i} çš„ response ä¸åŒ…å« sub_docs å­—æ®µæˆ–æ ¼å¼ä¸æ­£ç¡®")
            else:
                print(f"å“åº”é¡¹ {i} ä¸åŒ…å« response å­—æ®µæˆ–æ ¼å¼ä¸æ­£ç¡®")

        # è®°å½•å»é‡ç»Ÿè®¡ä¿¡æ¯
        if all_sub_docs:
            duplicates_count = total_sub_docs - len(all_sub_docs)
            if duplicates_count > 0:
                print(f"å»é‡å®Œæˆï¼šä» {total_sub_docs} ä¸ªåŸå§‹URLä¸­ç§»é™¤äº† {duplicates_count} ä¸ªé‡å¤é¡¹ï¼Œæœ€ç»ˆå¾—åˆ° {len(all_sub_docs)} ä¸ªå”¯ä¸€URL")
            else:
                print(f"å»é‡å®Œæˆï¼šæ‰€æœ‰ {total_sub_docs} ä¸ªURLéƒ½æ˜¯å”¯ä¸€çš„")
        
        if all_sub_docs:
            print(f"æ€»å…±å‘ç° {len(all_sub_docs)} ä¸ªå­æ–‡æ¡£URL")
            
            # ä»åŸå§‹æ•°æ®ä¸­è·å–é€’å½’æ·±åº¦å‚æ•°ï¼Œé»˜è®¤ä¸º1
            recursive_depth = 1
            if hasattr(data, 'recursive_depth') and isinstance(data.recursive_depth, int):
                recursive_depth = data.recursive_depth
            
            # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šå°†å­æ–‡æ¡£å¤„ç†ä½œä¸ºåå°ä»»åŠ¡å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡å“åº”
            # è·å–Source IDï¼ˆè¿™é‡Œçš„dataæ˜¯webhookå“åº”ï¼Œéœ€è¦é€šè¿‡document_nameå’Œcollectionæ‰¾åˆ°å¯¹åº”çš„source_idï¼‰
            source_id = None
            if data.document_name:
                try:
                    from sqlalchemy.future import select
                    FIXED_SESSION_ID = "fixed_session_id_for_agenttic_ingest"
                    stmt = select(Source).where(
                        Source.title == data.document_name,
                        Source.session_id == FIXED_SESSION_ID
                    ).order_by(Source.created_at.desc())  # è·å–æœ€æ–°åˆ›å»ºçš„source
                    result = await db.execute(stmt)
                    source = result.scalar_one_or_none()
                    if source:
                        source_id = source.id
                        print(f"æ‰¾åˆ°çˆ¶çº§Source ID: {source_id}")
                    else:
                        print(f"æœªæ‰¾åˆ°åŒ¹é…çš„Sourceï¼Œæ–‡æ¡£åç§°: {data.document_name}")
                except Exception as e:
                    print(f"æŸ¥æ‰¾çˆ¶çº§Sourceå¤±è´¥: {e}")
                    
            if background_tasks:
                print("å°†å­æ–‡æ¡£å¤„ç†æ·»åŠ åˆ°åå°ä»»åŠ¡é˜Ÿåˆ—...")
                background_tasks.add_task(
                    process_sub_docs_background,
                    all_sub_docs,
                    recursive_depth,
                    data.request_id,
                    data.document_name,
                    data.collection_name,
                    source_id
                )
            else:
                # å¦‚æœæ²¡æœ‰background_tasksï¼Œç›´æ¥å¯åŠ¨åç¨‹ä»»åŠ¡ï¼ˆä¸ç­‰å¾…ï¼‰
                print("å¯åŠ¨å­æ–‡æ¡£åå°å¤„ç†åç¨‹...")
                asyncio.create_task(
                    process_sub_docs_background(all_sub_docs, recursive_depth, data.request_id, data.document_name, data.collection_name, source_id)
                )
                
        else:
            print("æœªå‘ç°ä»»ä½•å­æ–‡æ¡£URL")
    else:
        print("å“åº”æ•°æ®ä¸­æœªåŒ…å«æœ‰æ•ˆçš„responseå­—æ®µ")
    
    # æ›´æ–°å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€
    try:
        if data.request_id:
            from sqlalchemy import update
            stmt = update(WorkflowExecution).where(
                WorkflowExecution.execution_id == data.request_id
            ).values(
                status="success",
                stopped_at=datetime.now(pytz.timezone('Asia/Shanghai'))
            )
            await db.execute(stmt)
            await db.commit()
            print(f"å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€å·²æ›´æ–°ä¸ºæˆåŠŸ: {data.request_id}")
    except Exception as e:
        print(f"æ›´æ–°å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€å¤±è´¥: {e}")
    
    # ğŸš€ ç«‹å³è¿”å›å“åº”ï¼Œä¸ç­‰å¾…å­æ–‡æ¡£å¤„ç†å®Œæˆ
    return {
        "message": "Webhookå“åº”å¤„ç†æˆåŠŸï¼Œå­æ–‡æ¡£å¤„ç†å·²å¯åŠ¨åå°ä»»åŠ¡",
        "task_name": data.task_name,
        "document_name": data.document_name,
        "total_sub_docs": total_sub_docs,
        "sub_docs_processing": "åå°å¤„ç†ä¸­" if total_sub_docs > 0 else "æ— éœ€å¤„ç†",
        "success": True
    }


async def generate_document_names(url: str, model: str = None) -> dict:
    """
    ä½¿ç”¨å¤§æ¨¡å‹ä¸ºæ–‡æ¡£å’Œcollectionç”Ÿæˆåç§°
    """
    # åˆ›å»ºä¸€ä¸ªä¸“é—¨ç”¨äºæ–‡æ¡£åç§°ç”Ÿæˆçš„æç¤ºæ¨¡æ¿ï¼Œé¿å…ä½¿ç”¨queryæ¥å£ä¸­çš„generate_answerå‡½æ•°
    # é€šè¿‡è°ƒç”¨llm_clientçš„åº•å±‚APIå®ç°ï¼Œè€Œä¸æ˜¯å¤ç”¨generate_answerå‡½æ•°
    from ..config import DEFAULT_INGEST_MODEL
    from ..llm_client import chat_complete
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
    if model is None:
        model = DEFAULT_INGEST_MODEL
    
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åç§°ç”ŸæˆåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£ä¸ºç½‘é¡µå†…å®¹ç”Ÿæˆåˆé€‚çš„ä¸­æ–‡æ ‡é¢˜å’Œè‹±æ–‡collectionåç§°ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "- åªè¾“å‡ºä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«å¤šä½™è§£é‡Šæˆ–æ ‡ç‚¹ã€‚\n"
        "- document_name ä½¿ç”¨ç®€æ´å‡†ç¡®çš„ä¸­æ–‡æ ‡é¢˜ã€‚\n"
        "- collection_name å…¨å°å†™ã€ä½¿ç”¨ä¸‹åˆ’çº¿è¿æ¥ã€åªå«è‹±æ–‡å­—æ¯æ•°å­—ä¸‹åˆ’çº¿ã€‚"
    )
    user_prompt = (
        f"è¯·ä¸ºä»¥ä¸‹URLç”Ÿæˆåç§°ï¼š\nURL: {url}\n\n"
        "è¿”å›JSONï¼ŒåŒ…å«ï¼š\n"
        "1. document_name: æ–‡æ¡£çš„ä¸­æ–‡åç§°\n"
        "2. collection_name: è‹±æ–‡collectionåç§°ï¼ˆå°å†™+ä¸‹åˆ’çº¿ï¼‰\n\n"
        "ç¤ºä¾‹ï¼š{\\\"document_name\\\": \\\"æœºå™¨å­¦ä¹ å…¥é—¨æŒ‡å—\\\", \\\"collection_name\\\": \\\"machine_learning_guide\\\"}"
    )
    
    try:
        # ç»Ÿä¸€é€šè¿‡ llm_client è°ƒç”¨
        response_content = await chat_complete(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            model=model,
            timeout=300,
        )

        # å°è¯•è§£æJSON
        import re
        json_match = re.search(r'\{[\s\S]*\}', response_content.strip())
        if json_match:
            try:
                result = json.loads(json_match.group())
                # ç»“æœåŸºæœ¬æ ¡éªŒä¸æ¸…ç†
                doc_name = str(result.get("document_name") or "").strip() or (url.split('/')[-1] or "æœªå‘½åæ–‡æ¡£")
                coll_name = str(result.get("collection_name") or "").strip()
                # è§„èŒƒåŒ– collection_nameï¼šå°å†™ã€ä¸‹åˆ’çº¿ã€å»é™¤éæ³•å­—ç¬¦
                import re as _re
                coll_name = _re.sub(r"[^a-z0-9_]", "_", coll_name.lower()) or f"doc_{hashlib.md5(url.encode()).hexdigest()[:8]}"
                return {"document_name": doc_name, "collection_name": coll_name}
            except Exception:
                pass
        # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨é»˜è®¤åç§°
        return {
            "document_name": url.split('/')[-1] or "æœªå‘½åæ–‡æ¡£",
            "collection_name": f"doc_{hashlib.md5(url.encode()).hexdigest()[:8]}"
        }
    except Exception as e:
        print(f"ç”Ÿæˆæ–‡æ¡£åç§°å¤±è´¥: {e}")
        return {
            "document_name": url.split('/')[-1] or "æœªå‘½åæ–‡æ¡£",
            "collection_name": f"doc_{hashlib.md5(url.encode()).hexdigest()[:8]}"
        }


@router.post("/agenttic-ingest", summary="æ™ºèƒ½æ–‡æ¡£æ‘„å–æ¥å£ï¼ˆç»Ÿä¸€å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚å’Œwebhookå›è°ƒï¼‰")
async def agenttic_ingest(
    background_tasks: BackgroundTasks,
    data: dict = Body(...),
    db: AsyncSession = Depends(get_db)
):
    """
    ç»Ÿä¸€çš„æ™ºèƒ½æ–‡æ¡£æ‘„å–æ¥å£ï¼š
    - å®¢æˆ·ç«¯è¯·æ±‚ï¼šè·å–URLå¹¶ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ–‡æ¡£åç§°å’Œcollectionåç§°ï¼Œæ‹‰å–å¹¶å¤„ç†å†…å®¹ï¼Œåˆ›å»ºæ–°çš„å‘é‡åº“collectionå­˜å‚¨ï¼Œå‘é€webhooké€šçŸ¥
    - webhookå›è°ƒï¼šå¤„ç†å·¥ä½œæµå“åº”æ•°æ®å¹¶æ‰§è¡Œé€’å½’æ‘„å–é€»è¾‘
    
    é€šè¿‡æ£€æµ‹æ•°æ®ä¸­æ˜¯å¦åŒ…å« 'task_name' å­—æ®µæ¥åˆ¤æ–­è¯·æ±‚ç±»å‹
    """
    
    # æ£€æµ‹è¯·æ±‚ç±»å‹ï¼šå¦‚æœåŒ…å« task_name å­—æ®µï¼Œè¯´æ˜æ˜¯ webhook å›è°ƒ
    # éœ€è¦è€ƒè™‘æ•°æ®å¯èƒ½åµŒå¥—åœ¨ body å­—æ®µä¸­çš„æƒ…å†µ
    is_webhook_callback = 'task_name' in data or ('body' in data and isinstance(data.get('body'), dict) and 'task_name' in data['body'])
    
    if is_webhook_callback:
        # å¤„ç† webhook å›è°ƒ
        print("æ£€æµ‹åˆ°webhookå›è°ƒè¯·æ±‚")
        try:
            # å¦‚æœæ•°æ®åµŒå¥—åœ¨bodyä¸­ï¼Œæå–bodyå†…å®¹
            webhook_request_data = data
            if 'body' in data and isinstance(data.get('body'), dict) and 'task_name' in data['body']:
                print("æ£€æµ‹åˆ°åµŒå¥—åœ¨bodyä¸­çš„webhookæ•°æ®ï¼Œæ­£åœ¨æå–...")
                webhook_request_data = data['body']
            
            webhook_data = WebhookResponseData(**webhook_request_data)
            return await process_webhook_response(webhook_data, db, background_tasks)
        except Exception as e:
            error_message = f"Webhookå›è°ƒå¤„ç†å¤±è´¥: {e.__class__.__name__}: {str(e)}"
            print(error_message)
            print(f"åŸå§‹æ•°æ®: {data}")  # æ·»åŠ åŸå§‹æ•°æ®æ—¥å¿—ä»¥ä¾¿è°ƒè¯•
            raise HTTPException(status_code=500, detail=error_message)
    
    # å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
    print("æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ‘„å–è¯·æ±‚")
    url = data.get("url")
    if not url:
        raise HTTPException(status_code=400, detail="URLå¿…é¡»æä¾›")

    model = data.get("model", DEFAULT_INGEST_MODEL)  # è·å–æ¨¡å‹å‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨ DEFAULT_INGEST_MODEL
    embedding_model = data.get("embedding_model", DEFAULT_EMBEDDING_MODEL)
    embedding_dimensions = data.get("embedding_dimensions", EMBEDDING_DIMENSIONS)
    webhook_url = data.get("webhook_url", WEBHOOK_PREFIX + "/array2array")
    recursive_depth = data.get("recursive_depth", 2)  # é»˜è®¤é€’å½’æ·±åº¦ä¸º2
    is_recursive = data.get("is_recursive", False)  # æ£€æµ‹æ˜¯å¦ä¸ºé€’å½’è°ƒç”¨
    # æ˜¯å¦å¯ç”¨ webhook å…œåº•ï¼Œå¯è¢«è¯·æ±‚å‚æ•°è¦†ç›–
    use_webhook_fallback = data.get("webhook_fallback", SUBDOC_USE_WEBHOOK_FALLBACK)
    print(f"æ¨¡å‹: {model}")
    try:
        # 1. ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ–‡æ¡£åç§°å’Œcollectionåç§°ï¼ˆä»…åœ¨éé€’å½’è°ƒç”¨æ—¶ï¼‰
        print("æ­£åœ¨ç”Ÿæˆæ–‡æ¡£åç§°...")
        if is_recursive:
            # é€’å½’è°ƒç”¨æ—¶ï¼Œä»æ•°æ®ä¸­è·å–å·²æœ‰çš„æ–‡æ¡£åç§°å’Œcollectionåç§°
            document_name = data.get("document_name")
            collection_name = data.get("collection_name")
            if not document_name or not collection_name:
                # å¦‚æœé€’å½’è°ƒç”¨æ—¶ç¼ºå°‘æ–‡æ¡£åç§°æˆ–collectionåç§°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                document_name = document_name or f"å­æ–‡æ¡£_{url.split('/')[-1] or 'æœªå‘½å'}"
                url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
                collection_name = collection_name or f"subdoc_{url_hash}"
            print(f"æ£€æµ‹åˆ°é€’å½’è°ƒç”¨ï¼Œä½¿ç”¨å·²æœ‰çš„æ–‡æ¡£åç§°: {document_name}, collectionåç§°: {collection_name}")
        else:
            # éé€’å½’è°ƒç”¨æ—¶ï¼Œæ­£å¸¸ç”Ÿæˆæ–‡æ¡£åç§°å’Œcollectionåç§°
            names = await generate_document_names(url, model)
            document_name = names["document_name"]
            # ä½¿ç”¨URLçš„hashç”Ÿæˆç¨³å®šçš„collectionåç§°ï¼Œç¡®ä¿åŒä¸€URLæ€»æ˜¯å¾—åˆ°ç›¸åŒçš„collection_name
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            collection_name = f"collection_{url_hash}"
        
        print(f"æ–‡æ¡£åç§°: {document_name}")
        print(f"Collectionåç§°: {collection_name}")

        # 2. æ‹‰å–å¹¶è§£æå†…å®¹
        print("æ­£åœ¨æ‹‰å–ç½‘é¡µå†…å®¹...")
        text = await fetch_then_extract(url)
        # ä»…è·å–HTMLç”¨äºå­æ–‡æ¡£é“¾æ¥æå–ï¼Œä¸ç”¨äºåˆ†å—å­˜å‚¨
        raw_html = await fetch_html(url)

        # 3. åˆ†å—å¤„ç†æ–‡æœ¬ï¼ˆä»…å¤„ç†plaintextï¼‰
        print("æ­£åœ¨åˆ†å—å¤„ç†æ–‡æœ¬...")
        chunks = chunk_text(text)
        if not chunks:
            raise ValueError("æ— æ³•ä»URLä¸­æå–ä»»ä½•å†…å®¹")

        total_chunks = len(chunks)
        print(f"æ€»å…±ç”Ÿæˆäº† {total_chunks} ä¸ªæ–‡æœ¬å—")

        # 4. åˆ›å»ºæˆ–è·å–Sourceå¯¹è±¡ (æ”¯æŒUPSERTæ“ä½œ)
        FIXED_SESSION_ID = "fixed_session_id_for_agenttic_ingest"
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒURLçš„Sourceè®°å½•ï¼ˆå…¨å±€å”¯ä¸€çº¦æŸï¼Œä¸è€ƒè™‘session_idï¼‰
        existing_source_stmt = select(Source).where(Source.url == url)
        existing_source_result = await db.execute(existing_source_stmt)
        existing_source = existing_source_result.scalars().first()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé€’å½’è°ƒç”¨ä¸”æä¾›äº†parent_source_id
        parent_source_id = data.get("parent_source_id")
        from datetime import datetime
        if is_recursive and parent_source_id:
            # é€’å½’è°ƒç”¨æ—¶ï¼Œä¸ºæ¯ä¸ªå­æ–‡æ¡£åˆ›å»ºç‹¬ç«‹çš„Sourceè®°å½•
            print(f"é€’å½’è°ƒç”¨ï¼šä¸ºå­æ–‡æ¡£åˆ›å»ºç‹¬ç«‹çš„Sourceè®°å½•ï¼Œçˆ¶çº§ID: {parent_source_id}")
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒURLçš„Sourceè®°å½•
            if existing_source:
                print(f"å­æ–‡æ¡£å·²å­˜åœ¨ï¼Œæ›´æ–°ç°æœ‰çš„Source: {existing_source.title} (ID: {existing_source.id})")
                # æ›´æ–°ç°æœ‰è®°å½•çš„ä¿¡æ¯
                existing_source.title = document_name
                existing_source.session_id = FIXED_SESSION_ID
                existing_source.created_at = datetime.now(pytz.timezone('Asia/Shanghai'))
                source = existing_source
                
                # åˆ é™¤ä¸ç°æœ‰Sourceç›¸å…³çš„æ—§chunksï¼Œå‡†å¤‡é‡æ–°å¤„ç†æœ€æ–°å†…å®¹
                print("åˆ é™¤æ—§çš„chunks...")
                await db.execute(delete(Chunk).where(Chunk.source_id == source.id))
                await db.flush()
            else:
                print(f"ä¸ºå­æ–‡æ¡£åˆ›å»ºæ–°çš„Sourceè®°å½•: {document_name}")
                source = Source(url=url, title=document_name, session_id=FIXED_SESSION_ID)
        else:
            # éé€’å½’è°ƒç”¨æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸åŒURLçš„Source (UPSERTé€»è¾‘)
            if existing_source:
                print(f"éé€’å½’è°ƒç”¨ï¼šæ›´æ–°ç°æœ‰Source: {existing_source.title} (ID: {existing_source.id})")
                # æ›´æ–°ç°æœ‰è®°å½•çš„æ‰€æœ‰ä¿¡æ¯
                existing_source.title = document_name
                existing_source.session_id = FIXED_SESSION_ID  # æ›´æ–° session_id
                existing_source.created_at = datetime.now(pytz.timezone('Asia/Shanghai'))
                source = existing_source
                
                # åˆ é™¤ä¸ç°æœ‰Sourceç›¸å…³çš„æ—§chunksï¼Œå‡†å¤‡é‡æ–°å¤„ç†æœ€æ–°å†…å®¹
                print("åˆ é™¤æ—§çš„chunks...")
                await db.execute(delete(Chunk).where(Chunk.source_id == source.id))
                await db.flush()
            else:
                print("éé€’å½’è°ƒç”¨ï¼šåˆ›å»ºæ–°çš„Sourceå¯¹è±¡")
                source = Source(url=url, title=document_name, session_id=FIXED_SESSION_ID)
        
        # åˆ›å»ºChunkå¯¹è±¡åˆ—è¡¨
        chunk_objects = []
        for index, text in enumerate(chunks):
            # ç”Ÿæˆå”¯ä¸€çš„chunk_id
            raw = f"{FIXED_SESSION_ID}|{url}|{index}".encode("utf-8", errors="ignore")
            generated_chunk_id = hashlib.md5(raw).hexdigest()
            chunk_obj = Chunk(
                chunk_id=generated_chunk_id,
                content=text,
                source_id=None,  # åœ¨æ•°æ®åº“ä¸­æš‚æ—¶ä¸è®¾ç½®source_id
                session_id=FIXED_SESSION_ID,
            )
            chunk_objects.append(chunk_obj)
        
        
        
        # 5. å°†chunkå¯¹è±¡ä¿å­˜åˆ°æ•°æ®åº“
        print(f"æ­£åœ¨ä¿å­˜chunkåˆ°æ•°æ®åº“... (æ–‡æœ¬å—: {len(chunk_objects)})")
        
        # åªæœ‰åœ¨åˆ›å»ºæ–°Sourceæ—¶æ‰éœ€è¦æ·»åŠ åˆ°æ•°æ®åº“
        if not (is_recursive and parent_source_id and source.id):
            db.add(source)
            await db.flush()
        
        # ä¸ºæ¯ä¸ªchunkè®¾ç½®source_id
        for chunk in chunk_objects:
            chunk.source_id = source.id
        
        db.add_all(chunk_objects)
        await db.flush()
        # æå‰æäº¤ï¼Œç¼©çŸ­äº‹åŠ¡å ç”¨æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´å†™é”
        await db.commit()
        
        # 6. ä¸ºæ‰€æœ‰chunkç”ŸæˆåµŒå…¥å‘é‡å¹¶å­˜å‚¨åˆ°Qdrant
        print("æ­£åœ¨ç”ŸæˆåµŒå…¥...")
        MAX_PARALLEL = int(EMBEDDING_MAX_CONCURRENCY)
        BATCH_SIZE = int(EMBEDDING_BATCH_SIZE)

        # åˆ†æ‰¹æ„é€ ä»»åŠ¡ï¼šæ¯ä¸ªä»»åŠ¡åªå‘èµ·ä¸€æ¬¡ /embeddings è¯·æ±‚ï¼ˆå°† batch_size ä¼ ä¸ºè¯¥æ‰¹å¤§å°ï¼‰
        chunk_batches = [
            chunk_objects[i: i + BATCH_SIZE]
            for i in range(0, len(chunk_objects), BATCH_SIZE)
        ]

        sem = asyncio.Semaphore(MAX_PARALLEL)

        async def embed_batch_worker(batch_index: int, batch_chunks: List[Chunk]):
            async with sem:
                texts = [c.content for c in batch_chunks]
                # è®©æ¯ä¸ªä»»åŠ¡åªå‘ä¸€æ¬¡è¯·æ±‚
                embeddings = await embed_texts(
                    texts,
                    model=embedding_model,
                    batch_size=len(texts),
                    dimensions=embedding_dimensions,
                )
                return batch_index, embeddings

        tasks = [
            asyncio.create_task(embed_batch_worker(idx, batch))
            for idx, batch in enumerate(chunk_batches)
        ]

        # ç­‰å¾…æ‰€æœ‰åµŒå…¥ä»»åŠ¡å®Œæˆ
        for coro in asyncio.as_completed(tasks):
            try:
                batch_index, embeddings = await coro
                batch_chunks = chunk_batches[batch_index]
                if not embeddings or len(embeddings) != len(batch_chunks):
                    # æœ¬æ‰¹å¤±è´¥æˆ–æ•°é‡ä¸ä¸€è‡´ï¼šè·³è¿‡å¹¶è®°å½•
                    print(
                        f"Embedding batch {batch_index} failed or size mismatch: got {len(embeddings) if embeddings else 0}, expected {len(batch_chunks)}"
                    )
                    continue

                # å°†è¯¥æ‰¹ç»“æœå†™å…¥å‘é‡åº“
                await add_embeddings(source.id, batch_chunks, embeddings)
            except Exception as e:
                print(f"Embedding task failed: {e}")
                # ä¸ä¸­æ–­æ•´ä½“æµç¨‹ï¼Œç»§ç»­å…¶ä»–æ‰¹æ¬¡

        # 7. æå–å­URLï¼ˆæœ¬åœ°è§£æä¼˜å…ˆï¼‰
        print("æ­£åœ¨æœ¬åœ°è§£æå­æ–‡æ¡£URL...")
        try:
            extracted_sub_docs = extract_links_from_html(raw_html, url)
            # ä¸¥æ ¼è¿‡æ»¤ä»…ä¿ç•™å­è·¯å¾„ï¼Œæ’é™¤å…„å¼Ÿæ–‡æ¡£
            from urllib.parse import urlparse
            def is_strict_child(child: str, base: str) -> bool:
                try:
                    cu = urlparse(child)
                    bu = urlparse(base)
                    if cu.netloc != bu.netloc:
                        return False
                    base_path = bu.path.rstrip('/')
                    url_path = cu.path.rstrip('/')
                    return url_path.startswith(base_path + '/')
                except Exception:
                    return False
            extracted_sub_docs = [u for u in extracted_sub_docs if is_strict_child(u, url)]
            # å»é‡
            extracted_sub_docs = list(dict.fromkeys(extracted_sub_docs))
            print(f"æœ¬åœ°è§£æåˆ° {len(extracted_sub_docs)} ä¸ªæ½œåœ¨å­æ–‡æ¡£URL")
        except Exception as e:
            print(f"æœ¬åœ°è§£æå­æ–‡æ¡£URLå¤±è´¥: {e}")
            extracted_sub_docs = []

        # 8. æœ¬åœ°å­æ–‡æ¡£å¤„ç†ï¼ˆå¼‚æ­¥åå°ï¼Œä¸é˜»å¡å“åº”ï¼‰
        if recursive_depth > 0 and extracted_sub_docs:
            print("å°†æœ¬åœ°è§£æçš„å­æ–‡æ¡£åŠ å…¥åå°å¤„ç†ä»»åŠ¡...")
            
            # ç”Ÿæˆä»»åŠ¡ID
            task_id = f"ingest_{hashlib.md5(url.encode()).hexdigest()[:8]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            if background_tasks:
                background_tasks.add_task(
                    process_sub_docs_background,
                    extracted_sub_docs,
                    recursive_depth,
                    task_id,  # ä½¿ç”¨ç”Ÿæˆçš„task_id
                    document_name,
                    collection_name,
                    source.id,
                    url  # æ·»åŠ parent_urlå‚æ•°
                )
            else:
                asyncio.create_task(
                    process_sub_docs_background(
                        extracted_sub_docs,
                        recursive_depth,
                        task_id,  # ä½¿ç”¨ç”Ÿæˆçš„task_id
                        document_name,
                        collection_name,
                        source.id,
                        url  # æ·»åŠ parent_urlå‚æ•°
                    )
                )

        # 9. å‡†å¤‡webhookæ•°æ®ï¼ˆä½œä¸ºå›é€€/è¡¥å……æ¸ é“ï¼‰
        import uuid
        from datetime import datetime
        
        # ç”Ÿæˆrequest_id: url + å½“å‰æ—¥æœŸ + uuid
        # å¯¹URLè¿›è¡Œç¼–ç ä»¥å¤„ç†ç‰¹æ®Šå­—ç¬¦
        import urllib.parse
        encoded_url = urllib.parse.quote(url, safe=':/')
        request_id = f"{encoded_url}_{datetime.now().strftime('%Y%m%d')}_{str(uuid.uuid4())}"
        
        # å¦‚æœéœ€è¦webhookè¯†åˆ«å­æ–‡æ¡£ï¼Œä¸´æ—¶åˆ†å—HTMLï¼ˆä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼‰
        temp_html_chunks = chunk_text(raw_html) if raw_html else []
        
        webhook_data = {
            "document_name": document_name,
            "collection_name": collection_name,
            "url": url,
            "total_chunks": total_chunks,
            "task_name": "agenttic_ingest",
            "prompt": 
            f"ä½ æ­£åœ¨é˜…è¯»ä¸€ä¸ªç½‘é¡µçš„éƒ¨åˆ†htmlï¼Œè¿™ä¸ªç½‘é¡µçš„urlæ˜¯{url}ï¼Œå†…å®¹æ˜¯æŸä¸ªå¼€æºæ¡†æ¶æ–‡æ¡£ã€‚ç°åœ¨æˆ‘éœ€è¦ä½ è¯†åˆ«è¿™ä¸ªæ–‡æ¡£ä¸‹é¢çš„çš„å­æ–‡æ¡£ã€‚æ¯”å¦‚ï¼šhttps://lmstudio.ai/docs/python/getting-started/project-setupæ˜¯https://lmstudio.ai/docs/pythonçš„å­æ–‡æ¡£ã€‚å­æ–‡æ¡£çš„URLæœ‰å¯èƒ½åœ¨HTMLä¸­ä»¥aæ ‡ç­¾çš„hrefï¼Œbuttonçš„è·³è½¬linkç­‰ç­‰å½¢å¼å­˜åœ¨ï¼Œä½ éœ€è¦è°ƒç”¨ä½ çš„ç¼–ç¨‹çŸ¥è¯†è¿›è¡Œè¯†åˆ«ï¼Œä½¿ç”¨{url}è¿›è¡Œæ‹¼æ¥ã€‚æœ€ç»ˆå°†è¯†åˆ«å‡ºæ¥çš„å­æ–‡æ¡£URLä»¥æ•°ç»„çš„å½¢å¼æ”¾åœ¨sub_docså±æ€§è”åˆchunk_idã€indexè¿”å›ï¼Œæ³¨æ„ï¼šå¦‚æœæ²¡æœ‰å‘ç°ä»»ä½•å­æ–‡æ¡£ï¼Œé‚£ä¹ˆè¿”å›ç©ºæ•°ç»„",
            "data_list": [
                {
                    "chunk_id": f"temp_html_{idx}",
                    "content": html_chunk,
                    "index": idx
                }
                for idx, html_chunk in enumerate(temp_html_chunks)
            ],
            "request_id": request_id,
            "recursive_depth": recursive_depth,  # æ·»åŠ é€’å½’æ·±åº¦å‚æ•°
        }

        # 10. å‘é€webhookï¼ˆä»…åœ¨é€’å½’æ·±åº¦å¤§äº0ã€ä¸”éœ€è¦è¡¥å……è¯†åˆ«ã€ä¸”å¯ç”¨å…œåº•æ—¶ï¼‰
        if recursive_depth > 0 and not extracted_sub_docs and use_webhook_fallback:
            print("æ­£åœ¨å‘é€webhookè¿›è¡Œå­æ–‡æ¡£è¯†åˆ«...")
            
            # åˆ›å»ºå·¥ä½œæµæ‰§è¡Œè®°å½•
            try:
                workflow_execution = WorkflowExecution(
                    execution_id=request_id,  # ä½¿ç”¨request_idä½œä¸ºä¸´æ—¶execution_id
                    document_name=document_name,
                    status="running",
                    session_id=FIXED_SESSION_ID
                )
                db.add(workflow_execution)
                await db.commit()
                print(f"å·¥ä½œæµæ‰§è¡Œè®°å½•å·²åˆ›å»º: {request_id}")
            except Exception as e:
                print(f"åˆ›å»ºå·¥ä½œæµæ‰§è¡Œè®°å½•å¤±è´¥: {e}")
                # ä¸é˜»å¡webhookå‘é€ï¼Œç»§ç»­æ‰§è¡Œ
            
            # ç›´æ¥å‘æŒ‡å®šçš„webhook URLå‘é€POSTè¯·æ±‚
            try:
                async with httpx.AsyncClient(timeout=WEBHOOK_TIMEOUT) as client:
                    response = await client.post(webhook_url, json=webhook_data)
                    response.raise_for_status()
                    print("Webhookå‘é€æˆåŠŸ")
            except Exception as e:
                print(f"Webhookå‘é€å¤±è´¥: {e}")
                # å¦‚æœwebhookå‘é€å¤±è´¥ï¼Œæ›´æ–°æ‰§è¡Œè®°å½•çŠ¶æ€
                try:
                    from sqlalchemy import update
                    stmt = update(WorkflowExecution).where(
                        WorkflowExecution.execution_id == request_id
                    ).values(
                        status="error",
                        stopped_at=datetime.now(pytz.timezone('Asia/Shanghai'))
                    )
                    await db.execute(stmt)
                    await db.commit()
                except Exception as update_error:
                    print(f"æ›´æ–°å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€å¤±è´¥: {update_error}")
        else:
            if recursive_depth <= 0:
                print(f"é€’å½’æ·±åº¦ä¸º0ï¼Œè·³è¿‡å­æ–‡æ¡£è¯†åˆ«webhook")
            elif not use_webhook_fallback and not extracted_sub_docs:
                print("é…ç½®ç¦æ­¢ webhook å…œåº•ï¼Œä¸”æœ¬åœ°æœªè§£æåˆ°å­æ–‡æ¡£URL")
            else:
                print("å·²é€šè¿‡æœ¬åœ°è§£æè·å¾—å­æ–‡æ¡£URLï¼Œè·³è¿‡webhookè¯†åˆ«")

        # å‡†å¤‡è¿”å›ç»“æœ
        result = {
            "success": True,
            "message": f"æˆåŠŸæ‘„å–æ–‡æ¡£ï¼Œå…±å¤„ç†äº† {total_chunks} ä¸ªæ–‡æœ¬å—",
            "document_name": document_name,
            "collection_name": collection_name,
            "total_chunks": total_chunks,
            "source_id": source.id  # è¿”å›Source IDç”¨äºé€’å½’è°ƒç”¨
        }
        
        # å¦‚æœå¯åŠ¨äº†å­æ–‡æ¡£åå°ä»»åŠ¡ï¼Œè¿”å›ä»»åŠ¡IDä¾›å‰ç«¯ç›‘æ§
        if recursive_depth > 0 and extracted_sub_docs:
            result["sub_docs_task_id"] = task_id
            result["sub_docs_count"] = len(extracted_sub_docs)
            result["sub_docs_processing"] = True
        
        return result

    except Exception as e:
        error_message = f"æ‘„å–å¤±è´¥: {e.__class__.__name__}: {str(e)}"
        print(error_message)
        raise HTTPException(status_code=500, detail=error_message)


@router.get("/documents", summary="è·å–é€šè¿‡agentic ingestå¤„ç†çš„æ–‡æ¡£åˆ—è¡¨")
async def get_agentic_ingest_documents(
    db: AsyncSession = Depends(get_db)
):
    """
    è·å–é€šè¿‡agentic ingestå¤„ç†çš„æ–‡æ¡£åˆ—è¡¨
    è¿”å›æ‰€æœ‰ä½¿ç”¨å›ºå®šsession_idå­˜å‚¨çš„æ–‡æ¡£
    """
    try:
        FIXED_SESSION_ID = "fixed_session_id_for_agenttic_ingest"
        
        # æŸ¥è¯¢æ•°æ®åº“ä¸­çš„sourceè®°å½•
        stmt = select(Source).where(Source.session_id == FIXED_SESSION_ID)
        result = await db.execute(stmt)
        sources = result.scalars().all()
        
        documents = []
        for source in sources:
            documents.append({
                "id": source.id,
                "title": source.title,
                "url": source.url,
                "created_at": source.created_at.isoformat() if source.created_at else None
            })
        
        return {
            "success": True,
            "documents": documents,
            "total": len(documents)
        }
    
    except Exception as e:
        error_message = f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e.__class__.__name__}: {str(e)}"
        print(error_message)
        raise HTTPException(status_code=500, detail=error_message)


@router.post("/workflow_response", summary="å·¥ä½œæµå“åº”å¤„ç†æ¥å£ï¼ˆå…¼å®¹æ€§ç«¯ç‚¹ï¼‰")
async def workflow_response(
    background_tasks: BackgroundTasks,
    data: dict = Body(...),
    db: AsyncSession = Depends(get_db)
):
    """
    å·¥ä½œæµå“åº”å¤„ç†æ¥å£ - å…¼å®¹æ€§ç«¯ç‚¹
    æ­¤ç«¯ç‚¹å°†è¯·æ±‚é‡å®šå‘åˆ°ç»Ÿä¸€çš„ agenttic_ingest æ¥å£è¿›è¡Œå¤„ç†
    """
    print("æ”¶åˆ°workflow_responseè¯·æ±‚ï¼Œé‡å®šå‘åˆ°ç»Ÿä¸€å¤„ç†æ¥å£")
    return await agenttic_ingest(background_tasks, data, db)


# ========== ä»»åŠ¡ç›‘æ§APIç«¯ç‚¹ ==========

async def stream_ingest_progress(task_id: str):
    """æµå¼è¿”å›æ‘„å–ä»»åŠ¡è¿›åº¦"""
    import json
    
    while True:
        status = await ingest_task_manager.get_task_status(task_id)
        if not status:
            # ä»»åŠ¡ä¸å­˜åœ¨ï¼Œå¯èƒ½å·²å®Œæˆå¹¶è¢«æ¸…ç†
            yield f"data: {json.dumps({'error': 'Task not found', 'task_id': task_id})}\n\n"
            break
            
        # å‘é€å½“å‰çŠ¶æ€
        yield f"data: {json.dumps(status.to_dict())}\n\n"
        
        # å¦‚æœä»»åŠ¡å®Œæˆæˆ–å¤±è´¥ï¼Œç»“æŸæµ
        if status.status in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.PARTIALLY_COMPLETED]:
            break
            
        await asyncio.sleep(2)  # æ¯2ç§’æ›´æ–°ä¸€æ¬¡


@router.get("/api/ingest-progress/{task_id}", summary="è·å–æ‘„å–ä»»åŠ¡è¿›åº¦ï¼ˆæµå¼ï¼‰")
async def get_ingest_progress(task_id: str):
    """è·å–æ‘„å–ä»»åŠ¡è¿›åº¦çš„æµå¼å“åº”"""
    return StreamingResponse(
        stream_ingest_progress(task_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )


@router.get("/api/ingest-status/{task_id}", summary="è·å–æ‘„å–ä»»åŠ¡çŠ¶æ€")
async def get_ingest_status(task_id: str):
    """è·å–æ‘„å–ä»»åŠ¡çš„å½“å‰çŠ¶æ€"""
    status = await ingest_task_manager.get_task_status(task_id)
    if not status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return {
        "success": True,
        "task_id": task_id,
        "status": status.to_dict()
    }


@router.get("/api/ingest-tasks", summary="è·å–æ‰€æœ‰æ´»è·ƒçš„æ‘„å–ä»»åŠ¡")
async def list_ingest_tasks():
    """è·å–æ‰€æœ‰æ´»è·ƒçš„æ‘„å–ä»»åŠ¡åˆ—è¡¨"""
    tasks = await ingest_task_manager.list_active_tasks()
    return {
        "success": True,
        "tasks": [task.to_dict() for task in tasks],
        "total": len(tasks)
    }


@router.delete("/api/ingest-task/{task_id}", summary="åˆ é™¤æ‘„å–ä»»åŠ¡")
async def delete_ingest_task(task_id: str):
    """åˆ é™¤æŒ‡å®šçš„æ‘„å–ä»»åŠ¡ï¼ˆé€šå¸¸åœ¨ä»»åŠ¡å®Œæˆåæ¸…ç†ï¼‰"""
    success = await ingest_task_manager.remove_task(task_id)
    if not success:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return {
        "success": True,
        "message": f"ä»»åŠ¡ {task_id} å·²åˆ é™¤"
    }
