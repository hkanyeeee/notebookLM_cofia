import json
import hashlib
import uuid
from typing import List, Optional
from datetime import datetime
import httpx
import asyncio

from fastapi import APIRouter, Body, HTTPException, Depends, BackgroundTasks
from pydantic import BaseModel, field_validator
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select

from ..models import Source, Chunk, WorkflowExecution
from ..database import get_db
from ..fetch_parse import fetch_then_extract, fetch_html
from ..chunking import chunk_text
from ..embedding_client import embed_texts, DEFAULT_EMBEDDING_MODEL
from ..config_manager import get_config_value
from ..vector_db_client import add_embeddings


router = APIRouter()


class WebhookResponseData(BaseModel):
    """ç”¨äºéªŒè¯webhookè¿”å›æ•°æ®ç»“æ„çš„æ¨¡å‹"""
    document_name: str
    collection_name: str
    url: str
    total_chunks: int
    source_id: Optional[str] = None
    session_id: Optional[str] = None
    task_name: str
    output: Optional[List[dict]] = None  # æ·»åŠ outputå­—æ®µä»¥åŒ…å«sub_docs
    recursive_depth: Optional[int] = 1  # æ·»åŠ é€’å½’æ·±åº¦å­—æ®µï¼Œé»˜è®¤ä¸º1
    request_id: Optional[str] = None  # è¯·æ±‚ID
    webhook_url: Optional[str] = None  # webhook URL
    is_recursive: Optional[bool] = False  # æ·»åŠ é€’å½’æ ‡è®°å­—æ®µï¼Œé»˜è®¤ä¸ºFalse
    
    @field_validator('total_chunks', mode='before')
    @classmethod
    def validate_total_chunks(cls, v):
        """ç¡®ä¿total_chunksæ˜¯æ•´æ•°ç±»å‹"""
        if isinstance(v, str):
            try:
                return int(v)
            except ValueError:
                raise ValueError(f"æ— æ³•å°†total_chunksè½¬æ¢ä¸ºæ•´æ•°: {v}")
        return v
        
    @field_validator('recursive_depth', mode='before')
    @classmethod
    def validate_recursive_depth(cls, v):
        """ç¡®ä¿recursive_depthæ˜¯æ•´æ•°ç±»å‹"""
        if v is None:
            return 2  # é»˜è®¤å€¼
        if isinstance(v, str):
            try:
                return int(v)
            except ValueError:
                raise ValueError(f"æ— æ³•å°†recursive_depthè½¬æ¢ä¸ºæ•´æ•°: {v}")
        return v


class UnifiedIngestRequest(BaseModel):
    """ç»Ÿä¸€çš„æ‘„å–è¯·æ±‚æ¨¡å‹ï¼Œæ”¯æŒå®¢æˆ·ç«¯è¯·æ±‚å’Œwebhookå›è°ƒ"""
    # å®¢æˆ·ç«¯è¯·æ±‚å¿…éœ€å­—æ®µ
    url: str
    
    # å®¢æˆ·ç«¯è¯·æ±‚å¯é€‰å­—æ®µ
    embedding_model: Optional[str] = None
    embedding_dimensions: Optional[int] = None
    webhook_url: Optional[str] = None
    recursive_depth: Optional[int] = None
    
    # webhookå›è°ƒä¸“æœ‰å­—æ®µ
    document_name: Optional[str] = None
    collection_name: Optional[str] = None
    total_chunks: Optional[int] = None
    chunks: Optional[List[dict]] = None
    source_id: Optional[str] = None
    parent_source_id: Optional[int] = None  # æ·»åŠ çˆ¶çº§Source IDå­—æ®µ
    session_id: Optional[str] = None
    task_name: Optional[str] = None
    is_recursive: Optional[bool] = False  # æ·»åŠ é€’å½’æ ‡è®°å­—æ®µï¼Œé»˜è®¤ä¸ºFalse


async def process_sub_docs_concurrent(
    sub_docs_urls: List[str],
    recursive_depth: int,
    db: AsyncSession,
    parent_doc_name: Optional[str] = None,
    parent_collection_name: Optional[str] = None,
    parent_source_id: Optional[int] = None
) -> List[dict]:
    """
    å¹¶å‘å¤„ç†å­æ–‡æ¡£çš„é€’å½’æ‘„å–å‡½æ•°

    Args:
        sub_docs_urls: å­æ–‡æ¡£URLåˆ—è¡¨
        recursive_depth: é€’å½’æ·±åº¦é™åˆ¶
        db: æ•°æ®åº“ä¼šè¯
        parent_doc_name: çˆ¶çº§æ–‡æ¡£åç§°
        parent_collection_name: çˆ¶çº§collectionåç§°
        parent_source_id: çˆ¶çº§Source IDï¼Œç”¨äºå°†å­æ–‡æ¡£å†…å®¹æ·»åŠ åˆ°åŒä¸€ä¸ªcollection

    Returns:
        List[dict]: æ¯ä¸ªå­æ–‡æ¡£çš„å¤„ç†ç»“æœ
    """
    results = []
    
    # æ£€æŸ¥é€’å½’æ·±åº¦é™åˆ¶
    if recursive_depth <= 0:
        print(f"è¾¾åˆ°é€’å½’æ·±åº¦é™åˆ¶ï¼Œè·³è¿‡ {len(sub_docs_urls)} ä¸ªå­æ–‡æ¡£çš„å¤„ç†")
        return results
    
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡
    MAX_CONCURRENT_SUB_DOCS = int(get_config_value("embedding_max_concurrency", "4"))
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_SUB_DOCS)
    
    # å¹¶å‘å¤„ç†æ‰€æœ‰å­æ–‡æ¡£URL
    async def process_single_sub_doc(sub_url: str, parent_doc_name: str = None, parent_collection_name: str = None, parent_source_id: int = None) -> dict:
        async with semaphore:  # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            try:
                print(f"å¼€å§‹é€’å½’æ‘„å–å­æ–‡æ¡£: {sub_url}")

                # æ„é€ é€’å½’è°ƒç”¨çš„è¯·æ±‚æ•°æ®
                sub_request_data = {
                    "url": sub_url,
                    "recursive_depth": recursive_depth - 1,  # å‡å°‘é€’å½’æ·±åº¦
                    "embedding_model": DEFAULT_EMBEDDING_MODEL,
                    "embedding_dimensions": int(get_config_value("embedding_dimensions", "1024")),
                    "webhook_url": get_config_value("webhook_prefix", "http://192.168.31.125:5678/webhook") + "/array2array",
                    "is_recursive": True,  # æ ‡è®°ä¸ºé€’å½’è°ƒç”¨
                    "document_name": parent_doc_name,  # ä¼ é€’çˆ¶çº§æ–‡æ¡£åç§°
                    "collection_name": parent_collection_name,  # ä¼ é€’çˆ¶çº§collectionåç§°
                    "parent_source_id": parent_source_id  # ä¼ é€’çˆ¶çº§Source ID
                }
                
                # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„BackgroundTaskså®ä¾‹ç”¨äºé€’å½’è°ƒç”¨
                dummy_background_tasks = BackgroundTasks()
                
                # è°ƒç”¨æœ¬æ¨¡å—çš„agenttic_ingestå‡½æ•°è¿›è¡Œé€’å½’å¤„ç†ï¼Œå‚æ•°é¡ºåºè¦æ­£ç¡®
                result = await agenttic_ingest(dummy_background_tasks, sub_request_data, db)
                
                print(f"å­æ–‡æ¡£æ‘„å–æˆåŠŸ: {sub_url}")
                return {
                    "url": sub_url,
                    "success": True,
                    "result": result
                }
                
            except Exception as e:
                error_msg = f"å­æ–‡æ¡£æ‘„å–å¤±è´¥ {sub_url}: {str(e)}"
                print(error_msg)
                return {
                    "url": sub_url,
                    "success": False,
                    "error": error_msg
                }
    
    # ä½¿ç”¨asyncio.gatherè¿›è¡Œå¹¶å‘å¤„ç†ï¼Œä½†é€šè¿‡ä¿¡å·é‡æ§åˆ¶æœ€å¤§å¹¶å‘æ•°
    try:
        tasks = [process_single_sub_doc(url, parent_doc_name, parent_collection_name, parent_source_id) for url in sub_docs_urls]
        results = await asyncio.gather(*tasks, return_exceptions=False)
    except Exception as e:
        print(f"å¹¶å‘å¤„ç†å­æ–‡æ¡£æ—¶å‡ºç°å¼‚å¸¸: {str(e)}")
        # é™çº§åˆ°ä¸²è¡Œå¤„ç†
        results = []
        for sub_url in sub_docs_urls:
            result = await process_single_sub_doc(sub_url, parent_doc_name, parent_collection_name, parent_source_id)
            results.append(result)
    
    return results


async def process_sub_docs_background(
    sub_docs_urls: List[str],
    recursive_depth: int,
    request_id: Optional[str] = None,
    parent_doc_name: Optional[str] = None,
    parent_collection_name: Optional[str] = None,
    parent_source_id: Optional[int] = None
):
    """
    åå°å¼‚æ­¥å¤„ç†å­æ–‡æ¡£çš„å‡½æ•° - ä¸é˜»å¡ä¸»å“åº”

    Args:
        sub_docs_urls: å­æ–‡æ¡£URLåˆ—è¡¨
        recursive_depth: é€’å½’æ·±åº¦é™åˆ¶
        request_id: è¯·æ±‚IDï¼Œç”¨äºæ—¥å¿—è¿½è¸ª
        parent_doc_name: çˆ¶çº§æ–‡æ¡£åç§°
        parent_collection_name: çˆ¶çº§collectionåç§°
        parent_source_id: çˆ¶çº§Source IDï¼Œç”¨äºå°†å­æ–‡æ¡£å†…å®¹æ·»åŠ åˆ°åŒä¸€ä¸ªcollection
    """
    try:
        # åˆ›å»ºæ–°çš„æ•°æ®åº“ä¼šè¯
        from ..database import AsyncSessionLocal
        async with AsyncSessionLocal() as db:
            print(f"[åå°ä»»åŠ¡] å¼€å§‹å¤„ç† {len(sub_docs_urls)} ä¸ªå­æ–‡æ¡£ï¼Œrequest_id: {request_id}")

            results = await process_sub_docs_concurrent(sub_docs_urls, recursive_depth, db, parent_doc_name, parent_collection_name, parent_source_id)

            success_count = len([r for r in results if r.get('success')])
            print(f"[åå°ä»»åŠ¡] å­æ–‡æ¡£å¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{len(results)}, request_id: {request_id}")
            
    except Exception as e:
        print(f"[åå°ä»»åŠ¡] å­æ–‡æ¡£å¤„ç†å¼‚å¸¸: {str(e)}, request_id: {request_id}")


async def process_webhook_response(
    data: WebhookResponseData,
    db: AsyncSession,
    background_tasks: BackgroundTasks = None
):
    """
    å¤„ç†webhookå›è°ƒæ•°æ®çš„ä¸“ç”¨å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå­æ–‡æ¡£å¤„ç†æ”¹ä¸ºåå°ä»»åŠ¡
    """
    print("å¤„ç†webhookå“åº”æ•°æ®...")
    print(f"ä»»åŠ¡åç§°: {data.task_name}")
    print(f"æ–‡æ¡£åç§°: {data.document_name}")
    print(f"Collectionåç§°: {data.collection_name}")
    print(f"æ€»å—æ•°: {data.total_chunks}")
    
    # æ£€æŸ¥ä»»åŠ¡åç§°
    if data.task_name != "agenttic_ingest":
        return {
            "message": f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {data.task_name}",
            "task_name": data.task_name,
            "success": False
        }
    
    # å®ç°é€’å½’æ‘„å–é€»è¾‘ï¼šæ”¶é›†webhookå“åº”ä¸­çš„å­æ–‡æ¡£URL
    total_sub_docs = 0
    
    if data.output and isinstance(data.output, list):
        print(f"æ£€æµ‹åˆ°å“åº”æ•°æ®ï¼Œå…± {len(data.output)} ä¸ªå“åº”é¡¹")
        
        # æ”¶é›†æ‰€æœ‰å­æ–‡æ¡£URLï¼Œå¹¶è¿›è¡Œå»é‡å¤„ç†
        all_sub_docs = []
        seen_urls = set()  # ç”¨äºè·Ÿè¸ªå·²æ·»åŠ çš„URLï¼Œé¿å…é‡å¤

        for i, output_item in enumerate(data.output):
            if isinstance(output_item, dict) and "response" in output_item:
                response_data = output_item.get("response", {})
                if isinstance(response_data, dict) and "sub_docs" in response_data:
                    sub_docs = response_data.get("sub_docs", [])
                    if isinstance(sub_docs, list):
                        print(f"å“åº”é¡¹ {i} åŒ…å« {len(sub_docs)} ä¸ªå­æ–‡æ¡£")
                        total_sub_docs += len(sub_docs)

                        # é€ä¸ªæ£€æŸ¥å¹¶æ·»åŠ URLï¼Œé¿å…é‡å¤
                        for url in sub_docs:
                            if url and url not in seen_urls:
                                all_sub_docs.append(url)
                                seen_urls.add(url)
                            elif url in seen_urls:
                                print(f"è·³è¿‡é‡å¤çš„å­æ–‡æ¡£URL: {url}")
                    else:
                        print(f"å“åº”é¡¹ {i} çš„ sub_docs ä¸æ˜¯åˆ—è¡¨æ ¼å¼: {type(sub_docs)}")
                else:
                    print(f"å“åº”é¡¹ {i} çš„ response ä¸åŒ…å« sub_docs å­—æ®µæˆ–æ ¼å¼ä¸æ­£ç¡®")
            else:
                print(f"å“åº”é¡¹ {i} ä¸åŒ…å« response å­—æ®µæˆ–æ ¼å¼ä¸æ­£ç¡®")

        # è®°å½•å»é‡ç»Ÿè®¡ä¿¡æ¯
        if all_sub_docs:
            duplicates_count = total_sub_docs - len(all_sub_docs)
            if duplicates_count > 0:
                print(f"å»é‡å®Œæˆï¼šä» {total_sub_docs} ä¸ªåŸå§‹URLä¸­ç§»é™¤äº† {duplicates_count} ä¸ªé‡å¤é¡¹ï¼Œæœ€ç»ˆå¾—åˆ° {len(all_sub_docs)} ä¸ªå”¯ä¸€URL")
            else:
                print(f"å»é‡å®Œæˆï¼šæ‰€æœ‰ {total_sub_docs} ä¸ªURLéƒ½æ˜¯å”¯ä¸€çš„")
        
        if all_sub_docs:
            print(f"æ€»å…±å‘ç° {len(all_sub_docs)} ä¸ªå­æ–‡æ¡£URL")
            
            # ä»åŸå§‹æ•°æ®ä¸­è·å–é€’å½’æ·±åº¦å‚æ•°ï¼Œé»˜è®¤ä¸º1
            recursive_depth = 1
            if hasattr(data, 'recursive_depth') and isinstance(data.recursive_depth, int):
                recursive_depth = data.recursive_depth
            
            # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šå°†å­æ–‡æ¡£å¤„ç†ä½œä¸ºåå°ä»»åŠ¡å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡å“åº”
            # è·å–Source IDï¼ˆè¿™é‡Œçš„dataæ˜¯webhookå“åº”ï¼Œéœ€è¦é€šè¿‡document_nameå’Œcollectionæ‰¾åˆ°å¯¹åº”çš„source_idï¼‰
            source_id = None
            if data.document_name:
                try:
                    from sqlalchemy.future import select
                    FIXED_SESSION_ID = "fixed_session_id_for_agenttic_ingest"
                    stmt = select(Source).where(
                        Source.title == data.document_name,
                        Source.session_id == FIXED_SESSION_ID
                    ).order_by(Source.created_at.desc())  # è·å–æœ€æ–°åˆ›å»ºçš„source
                    result = await db.execute(stmt)
                    source = result.scalar_one_or_none()
                    if source:
                        source_id = source.id
                        print(f"æ‰¾åˆ°çˆ¶çº§Source ID: {source_id}")
                    else:
                        print(f"æœªæ‰¾åˆ°åŒ¹é…çš„Sourceï¼Œæ–‡æ¡£åç§°: {data.document_name}")
                except Exception as e:
                    print(f"æŸ¥æ‰¾çˆ¶çº§Sourceå¤±è´¥: {e}")
                    
            if background_tasks:
                print("å°†å­æ–‡æ¡£å¤„ç†æ·»åŠ åˆ°åå°ä»»åŠ¡é˜Ÿåˆ—...")
                background_tasks.add_task(
                    process_sub_docs_background,
                    all_sub_docs,
                    recursive_depth,
                    data.request_id,
                    data.document_name,
                    data.collection_name,
                    source_id
                )
            else:
                # å¦‚æœæ²¡æœ‰background_tasksï¼Œç›´æ¥å¯åŠ¨åç¨‹ä»»åŠ¡ï¼ˆä¸ç­‰å¾…ï¼‰
                print("å¯åŠ¨å­æ–‡æ¡£åå°å¤„ç†åç¨‹...")
                asyncio.create_task(
                    process_sub_docs_background(all_sub_docs, recursive_depth, data.request_id, data.document_name, data.collection_name, source_id)
                )
                
        else:
            print("æœªå‘ç°ä»»ä½•å­æ–‡æ¡£URL")
    else:
        print("å“åº”æ•°æ®ä¸­æœªåŒ…å«æœ‰æ•ˆçš„responseå­—æ®µ")
    
    # æ›´æ–°å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€
    try:
        if data.request_id:
            from sqlalchemy import update
            stmt = update(WorkflowExecution).where(
                WorkflowExecution.execution_id == data.request_id
            ).values(
                status="success",
                stopped_at=datetime.utcnow()
            )
            await db.execute(stmt)
            await db.commit()
            print(f"å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€å·²æ›´æ–°ä¸ºæˆåŠŸ: {data.request_id}")
    except Exception as e:
        print(f"æ›´æ–°å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€å¤±è´¥: {e}")
    
    # ğŸš€ ç«‹å³è¿”å›å“åº”ï¼Œä¸ç­‰å¾…å­æ–‡æ¡£å¤„ç†å®Œæˆ
    return {
        "message": "Webhookå“åº”å¤„ç†æˆåŠŸï¼Œå­æ–‡æ¡£å¤„ç†å·²å¯åŠ¨åå°ä»»åŠ¡",
        "task_name": data.task_name,
        "document_name": data.document_name,
        "total_sub_docs": total_sub_docs,
        "sub_docs_processing": "åå°å¤„ç†ä¸­" if total_sub_docs > 0 else "æ— éœ€å¤„ç†",
        "success": True
    }


async def generate_document_names(url: str) -> dict:
    """
    ä½¿ç”¨å¤§æ¨¡å‹ä¸ºæ–‡æ¡£å’Œcollectionç”Ÿæˆåç§°
    """
    # åˆ›å»ºä¸€ä¸ªä¸“é—¨ç”¨äºæ–‡æ¡£åç§°ç”Ÿæˆçš„æç¤ºæ¨¡æ¿ï¼Œé¿å…ä½¿ç”¨queryæ¥å£ä¸­çš„generate_answerå‡½æ•°
    # é€šè¿‡è°ƒç”¨llm_clientçš„åº•å±‚APIå®ç°ï¼Œè€Œä¸æ˜¯å¤ç”¨generate_answerå‡½æ•°
    import httpx
    
    prompt = f"""
è¯·ä¸ºä»¥ä¸‹URLçš„æ–‡æ¡£ç”Ÿæˆåˆé€‚çš„ä¸­æ–‡åç§°å’Œè‹±æ–‡collectionåç§°ï¼š

URL: {url}

è¯·è¿”å›JSONæ ¼å¼ï¼ŒåŒ…å«ï¼š
1. document_name: æ–‡æ¡£çš„ä¸­æ–‡åç§°ï¼ˆç®€æ´æ˜äº†ï¼Œé€‚åˆæ˜¾ç¤ºç»™ç”¨æˆ·ï¼‰
2. collection_name: å‘é‡åº“collectionçš„è‹±æ–‡åç§°ï¼ˆå°å†™ï¼Œç”¨ä¸‹åˆ’çº¿è¿æ¥ï¼Œé€‚åˆä½œä¸ºæ•°æ®åº“åç§°ï¼‰

ç¤ºä¾‹æ ¼å¼ï¼š
{{"document_name": "æœºå™¨å­¦ä¹ å…¥é—¨æŒ‡å—", "collection_name": "machine_learning_guide"}}
"""
    
    try:
        # ä½¿ç”¨LLMæœåŠ¡çš„åº•å±‚APIç›´æ¥è°ƒç”¨ï¼Œç»•è¿‡generate_answerå‡½æ•°
        llm_service_url = get_config_value("llm_service_url", "http://localhost:11434/v1")
        url = f"{llm_service_url}/chat/completions"
        
        payload = {
            "model": "qwen3-30b-a3b-thinking-2507-mlx",
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åç§°ç”ŸæˆåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£ä¸ºç½‘é¡µå†…å®¹ç”Ÿæˆåˆé€‚çš„ä¸­æ–‡æ ‡é¢˜å’Œè‹±æ–‡collectionåç§°ã€‚"},
                {"role": "user", "content": prompt},
            ],
        }

        async with httpx.AsyncClient(timeout=300) as client:
            response = await client.post(url, json=payload)
            response.raise_for_status()
            data = response.json()
            # è·å–æ¨¡å‹è¿”å›çš„å†…å®¹
            message = (data.get("choices") or [{}])[0].get("message", {})
            response_content = message.get("content") or ""
            
            # å°è¯•è§£æJSON
            import re
            json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return result
            else:
                # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨é»˜è®¤åç§°
                return {
                    "document_name": url.split('/')[-1] or "æœªå‘½åæ–‡æ¡£",
                    "collection_name": f"doc_{hashlib.md5(url.encode()).hexdigest()[:8]}"
                }
    except Exception as e:
        print(f"ç”Ÿæˆæ–‡æ¡£åç§°å¤±è´¥: {e}")
        return {
            "document_name": url.split('/')[-1] or "æœªå‘½åæ–‡æ¡£",
            "collection_name": f"doc_{hashlib.md5(url.encode()).hexdigest()[:8]}"
        }


@router.post("/agenttic-ingest", summary="æ™ºèƒ½æ–‡æ¡£æ‘„å–æ¥å£ï¼ˆç»Ÿä¸€å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚å’Œwebhookå›è°ƒï¼‰")
async def agenttic_ingest(
    background_tasks: BackgroundTasks,
    data: dict = Body(...),
    db: AsyncSession = Depends(get_db)
):
    """
    ç»Ÿä¸€çš„æ™ºèƒ½æ–‡æ¡£æ‘„å–æ¥å£ï¼š
    - å®¢æˆ·ç«¯è¯·æ±‚ï¼šè·å–URLå¹¶ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ–‡æ¡£åç§°å’Œcollectionåç§°ï¼Œæ‹‰å–å¹¶å¤„ç†å†…å®¹ï¼Œåˆ›å»ºæ–°çš„å‘é‡åº“collectionå­˜å‚¨ï¼Œå‘é€webhooké€šçŸ¥
    - webhookå›è°ƒï¼šå¤„ç†å·¥ä½œæµå“åº”æ•°æ®å¹¶æ‰§è¡Œé€’å½’æ‘„å–é€»è¾‘
    
    é€šè¿‡æ£€æµ‹æ•°æ®ä¸­æ˜¯å¦åŒ…å« 'task_name' å­—æ®µæ¥åˆ¤æ–­è¯·æ±‚ç±»å‹
    """
    
    # æ£€æµ‹è¯·æ±‚ç±»å‹ï¼šå¦‚æœåŒ…å« task_name å­—æ®µï¼Œè¯´æ˜æ˜¯ webhook å›è°ƒ
    # éœ€è¦è€ƒè™‘æ•°æ®å¯èƒ½åµŒå¥—åœ¨ body å­—æ®µä¸­çš„æƒ…å†µ
    is_webhook_callback = 'task_name' in data or ('body' in data and isinstance(data.get('body'), dict) and 'task_name' in data['body'])
    
    if is_webhook_callback:
        # å¤„ç† webhook å›è°ƒ
        print("æ£€æµ‹åˆ°webhookå›è°ƒè¯·æ±‚")
        try:
            # å¦‚æœæ•°æ®åµŒå¥—åœ¨bodyä¸­ï¼Œæå–bodyå†…å®¹
            webhook_request_data = data
            if 'body' in data and isinstance(data.get('body'), dict) and 'task_name' in data['body']:
                print("æ£€æµ‹åˆ°åµŒå¥—åœ¨bodyä¸­çš„webhookæ•°æ®ï¼Œæ­£åœ¨æå–...")
                webhook_request_data = data['body']
            
            webhook_data = WebhookResponseData(**webhook_request_data)
            return await process_webhook_response(webhook_data, db, background_tasks)
        except Exception as e:
            error_message = f"Webhookå›è°ƒå¤„ç†å¤±è´¥: {e.__class__.__name__}: {str(e)}"
            print(error_message)
            print(f"åŸå§‹æ•°æ®: {data}")  # æ·»åŠ åŸå§‹æ•°æ®æ—¥å¿—ä»¥ä¾¿è°ƒè¯•
            raise HTTPException(status_code=500, detail=error_message)
    
    # å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
    print("æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ‘„å–è¯·æ±‚")
    url = data.get("url")
    if not url:
        raise HTTPException(status_code=400, detail="URLå¿…é¡»æä¾›")

    embedding_model = data.get("embedding_model", DEFAULT_EMBEDDING_MODEL)
    embedding_dimensions = data.get("embedding_dimensions", int(get_config_value("embedding_dimensions", "1024")))
    webhook_url = data.get("webhook_url", get_config_value("webhook_prefix", "http://192.168.31.125:5678/webhook") + "/array2array")
    recursive_depth = data.get("recursive_depth", 2)  # é»˜è®¤é€’å½’æ·±åº¦ä¸º2
    is_recursive = data.get("is_recursive", False)  # æ£€æµ‹æ˜¯å¦ä¸ºé€’å½’è°ƒç”¨

    try:
        # 1. ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ–‡æ¡£åç§°å’Œcollectionåç§°ï¼ˆä»…åœ¨éé€’å½’è°ƒç”¨æ—¶ï¼‰
        print("æ­£åœ¨ç”Ÿæˆæ–‡æ¡£åç§°...")
        if is_recursive:
            # é€’å½’è°ƒç”¨æ—¶ï¼Œä»æ•°æ®ä¸­è·å–å·²æœ‰çš„æ–‡æ¡£åç§°å’Œcollectionåç§°
            document_name = data.get("document_name")
            collection_name = data.get("collection_name")
            if not document_name or not collection_name:
                # å¦‚æœé€’å½’è°ƒç”¨æ—¶ç¼ºå°‘æ–‡æ¡£åç§°æˆ–collectionåç§°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                document_name = document_name or f"å­æ–‡æ¡£_{url.split('/')[-1] or 'æœªå‘½å'}"
                url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
                collection_name = collection_name or f"subdoc_{url_hash}"
            print(f"æ£€æµ‹åˆ°é€’å½’è°ƒç”¨ï¼Œä½¿ç”¨å·²æœ‰çš„æ–‡æ¡£åç§°: {document_name}, collectionåç§°: {collection_name}")
        else:
            # éé€’å½’è°ƒç”¨æ—¶ï¼Œæ­£å¸¸ç”Ÿæˆæ–‡æ¡£åç§°å’Œcollectionåç§°
            names = await generate_document_names(url)
            document_name = names["document_name"]
            # ä½¿ç”¨URLçš„hashç”Ÿæˆç¨³å®šçš„collectionåç§°ï¼Œç¡®ä¿åŒä¸€URLæ€»æ˜¯å¾—åˆ°ç›¸åŒçš„collection_name
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            collection_name = f"collection_{url_hash}"
        
        print(f"æ–‡æ¡£åç§°: {document_name}")
        print(f"Collectionåç§°: {collection_name}")

        # 2. æ‹‰å–å¹¶è§£æå†…å®¹
        print("æ­£åœ¨æ‹‰å–ç½‘é¡µå†…å®¹...")
        text = await fetch_then_extract(url)
        raw_html = await fetch_html(url)

        # 3. åˆ†å—å¤„ç†æ–‡æœ¬
        print("æ­£åœ¨åˆ†å—å¤„ç†æ–‡æœ¬...")
        chunk_size = int(get_config_value("chunk_size", "1000"))
        chunk_overlap = int(get_config_value("chunk_overlap", "100"))
        chunks = chunk_text(text, tokens_per_chunk=chunk_size, overlap_tokens=chunk_overlap)
        raw_html_chunks = chunk_text(raw_html, 4000, 200)
        if not chunks:
            raise ValueError("æ— æ³•ä»URLä¸­æå–ä»»ä½•å†…å®¹")

        total_chunks = len(chunks)
        print(f"æ€»å…±ç”Ÿæˆäº† {total_chunks} ä¸ªæ–‡æœ¬å—")

        # 4. åˆ›å»ºæˆ–è·å–Sourceå¯¹è±¡
        FIXED_SESSION_ID = "fixed_session_id_for_agenttic_ingest"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé€’å½’è°ƒç”¨ä¸”æä¾›äº†parent_source_id
        parent_source_id = data.get("parent_source_id")
        if is_recursive and parent_source_id:
            # é€’å½’è°ƒç”¨æ—¶ï¼Œè·å–çˆ¶çº§Sourceå¯¹è±¡è€Œä¸æ˜¯åˆ›å»ºæ–°çš„
            print(f"é€’å½’è°ƒç”¨ï¼šå°è¯•è·å–çˆ¶çº§Source ID: {parent_source_id}")
            stmt = select(Source).where(Source.id == parent_source_id)
            result = await db.execute(stmt)
            source = result.scalar_one_or_none()
            
            if not source:
                print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°çˆ¶çº§Source ID {parent_source_id}ï¼Œåˆ›å»ºæ–°çš„Source")
                source = Source(url=url, title=document_name, session_id=FIXED_SESSION_ID)
            else:
                print(f"æˆåŠŸè·å–çˆ¶çº§Source: {source.title} (ID: {source.id})")
        else:
            # éé€’å½’è°ƒç”¨æ—¶ï¼Œåˆ›å»ºæ–°çš„Sourceå¯¹è±¡
            print("éé€’å½’è°ƒç”¨ï¼šåˆ›å»ºæ–°çš„Sourceå¯¹è±¡")
            source = Source(url=url, title=document_name, session_id=FIXED_SESSION_ID)
        
        # åˆ›å»ºChunkå¯¹è±¡åˆ—è¡¨
        chunk_objects = []
        for index, text in enumerate(chunks):
            # ç”Ÿæˆå”¯ä¸€çš„chunk_id
            raw = f"{FIXED_SESSION_ID}|{url}|{index}".encode("utf-8", errors="ignore")
            generated_chunk_id = hashlib.md5(raw).hexdigest()
            chunk_obj = Chunk(
                chunk_id=generated_chunk_id,
                content=text,
                source_id=None,  # åœ¨æ•°æ®åº“ä¸­æš‚æ—¶ä¸è®¾ç½®source_id
                session_id=FIXED_SESSION_ID,
            )
            chunk_objects.append(chunk_obj)
        
        # åˆ›å»ºraw_html_chunkå¯¹è±¡åˆ—è¡¨
        raw_html_chunk_objects = []
        for index, html in enumerate(raw_html_chunks):
            # ç”Ÿæˆå”¯ä¸€çš„chunk_id (æ·»åŠ 'html'å‰ç¼€ä»¥åŒºåˆ†æ™®é€šæ–‡æœ¬chunk)
            raw = f"{FIXED_SESSION_ID}|{url}|html|{index}".encode("utf-8", errors="ignore")
            generated_chunk_id = hashlib.md5(raw).hexdigest()
            raw_html_chunk_obj = Chunk(
                chunk_id=generated_chunk_id,
                content=html,
                source_id=None,  # åœ¨æ•°æ®åº“ä¸­æš‚æ—¶ä¸è®¾ç½®source_id
                session_id=FIXED_SESSION_ID,
            )
            raw_html_chunk_objects.append(raw_html_chunk_obj)
        
        
        # 5. å°†chunkå¯¹è±¡ä¿å­˜åˆ°æ•°æ®åº“
        print("æ­£åœ¨ä¿å­˜chunkåˆ°æ•°æ®åº“...")
        
        # åªæœ‰åœ¨åˆ›å»ºæ–°Sourceæ—¶æ‰éœ€è¦æ·»åŠ åˆ°æ•°æ®åº“
        if not (is_recursive and parent_source_id and source.id):
            db.add(source)
            await db.flush()
        
        # ä¸ºæ¯ä¸ªchunkè®¾ç½®source_id
        for chunk in chunk_objects:
            chunk.source_id = source.id
        
        db.add_all(chunk_objects)
        await db.flush()
        # æå‰æäº¤ï¼Œç¼©çŸ­äº‹åŠ¡å ç”¨æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´å†™é”
        await db.commit()
        
        # 6. ä¸ºæ‰€æœ‰chunkç”ŸæˆåµŒå…¥å‘é‡å¹¶å­˜å‚¨åˆ°Qdrant
        print("æ­£åœ¨ç”ŸæˆåµŒå…¥...")
        MAX_PARALLEL = int(get_config_value("embedding_max_concurrency", "4"))
        BATCH_SIZE = int(get_config_value("embedding_batch_size", "4"))

        # åˆ†æ‰¹æ„é€ ä»»åŠ¡ï¼šæ¯ä¸ªä»»åŠ¡åªå‘èµ·ä¸€æ¬¡ /embeddings è¯·æ±‚ï¼ˆå°† batch_size ä¼ ä¸ºè¯¥æ‰¹å¤§å°ï¼‰
        chunk_batches = [
            chunk_objects[i: i + BATCH_SIZE]
            for i in range(0, len(chunk_objects), BATCH_SIZE)
        ]

        sem = asyncio.Semaphore(MAX_PARALLEL)

        async def embed_batch_worker(batch_index: int, batch_chunks: List[Chunk]):
            async with sem:
                texts = [c.content for c in batch_chunks]
                # è®©æ¯ä¸ªä»»åŠ¡åªå‘ä¸€æ¬¡è¯·æ±‚
                embeddings = await embed_texts(
                    texts,
                    model=embedding_model,
                    batch_size=len(texts),
                    dimensions=embedding_dimensions,
                )
                return batch_index, embeddings

        tasks = [
            asyncio.create_task(embed_batch_worker(idx, batch))
            for idx, batch in enumerate(chunk_batches)
        ]

        # ç­‰å¾…æ‰€æœ‰åµŒå…¥ä»»åŠ¡å®Œæˆ
        for coro in asyncio.as_completed(tasks):
            try:
                batch_index, embeddings = await coro
                batch_chunks = chunk_batches[batch_index]
                if not embeddings or len(embeddings) != len(batch_chunks):
                    # æœ¬æ‰¹å¤±è´¥æˆ–æ•°é‡ä¸ä¸€è‡´ï¼šè·³è¿‡å¹¶è®°å½•
                    print(
                        f"Embedding batch {batch_index} failed or size mismatch: got {len(embeddings) if embeddings else 0}, expected {len(batch_chunks)}"
                    )
                    continue

                # å°†è¯¥æ‰¹ç»“æœå†™å…¥å‘é‡åº“
                await add_embeddings(source.id, batch_chunks, embeddings)
            except Exception as e:
                print(f"Embedding task failed: {e}")
                # ä¸ä¸­æ–­æ•´ä½“æµç¨‹ï¼Œç»§ç»­å…¶ä»–æ‰¹æ¬¡

        # 7. å‡†å¤‡webhookæ•°æ®
        import uuid
        from datetime import datetime
        
        # ç”Ÿæˆrequest_id: url + å½“å‰æ—¥æœŸ + uuid
        # å¯¹URLè¿›è¡Œç¼–ç ä»¥å¤„ç†ç‰¹æ®Šå­—ç¬¦
        import urllib.parse
        encoded_url = urllib.parse.quote(url, safe=':/')
        request_id = f"{encoded_url}_{datetime.now().strftime('%Y%m%d')}_{str(uuid.uuid4())}"
        
        webhook_data = {
            "document_name": document_name,
            "collection_name": collection_name,
            "url": url,
            "total_chunks": total_chunks,
            "task_name": "agenttic_ingest",
            "prompt": 
            f"ä½ æ­£åœ¨é˜…è¯»ä¸€ä¸ªç½‘é¡µçš„éƒ¨åˆ†htmlï¼Œè¿™ä¸ªç½‘é¡µçš„urlæ˜¯{url}ï¼Œå†…å®¹æ˜¯æŸä¸ªå¼€æºæ¡†æ¶æ–‡æ¡£ã€‚ç°åœ¨æˆ‘éœ€è¦ä½ è¯†åˆ«è¿™ä¸ªæ–‡æ¡£ä¸‹é¢çš„çš„å­æ–‡æ¡£ã€‚æ¯”å¦‚ï¼šhttps://lmstudio.ai/docs/python/getting-started/project-setupæ˜¯https://lmstudio.ai/docs/pythonçš„å­æ–‡æ¡£ã€‚å­æ–‡æ¡£çš„URLæœ‰å¯èƒ½åœ¨HTMLä¸­ä»¥aæ ‡ç­¾çš„hrefï¼Œbuttonçš„è·³è½¬linkç­‰ç­‰å½¢å¼å­˜åœ¨ï¼Œä½ éœ€è¦è°ƒç”¨ä½ çš„ç¼–ç¨‹çŸ¥è¯†è¿›è¡Œè¯†åˆ«ï¼Œä½¿ç”¨{url}è¿›è¡Œæ‹¼æ¥ã€‚æœ€ç»ˆå°†è¯†åˆ«å‡ºæ¥çš„å­æ–‡æ¡£URLä»¥æ•°ç»„çš„å½¢å¼æ”¾åœ¨sub_docså±æ€§è”åˆchunk_idã€indexè¿”å›ï¼Œæ³¨æ„ï¼šå¦‚æœæ²¡æœ‰å‘ç°ä»»ä½•å­æ–‡æ¡£ï¼Œé‚£ä¹ˆè¿”å›ç©ºæ•°ç»„",
            "data_list": [
                {
                    "chunk_id": chunk.chunk_id,
                    "content": chunk.content,
                    "index": idx
                }
                for idx, chunk in enumerate(raw_html_chunk_objects)
            ],
            "request_id": request_id,
            "recursive_depth": recursive_depth,  # æ·»åŠ é€’å½’æ·±åº¦å‚æ•°
        }

        # 8. å‘é€webhookï¼ˆä»…åœ¨é€’å½’æ·±åº¦å¤§äº0è°ƒç”¨æ—¶ï¼‰
        if recursive_depth > 0:
            print("æ­£åœ¨å‘é€webhookè¿›è¡Œå­æ–‡æ¡£è¯†åˆ«...")
            
            # åˆ›å»ºå·¥ä½œæµæ‰§è¡Œè®°å½•
            try:
                workflow_execution = WorkflowExecution(
                    execution_id=request_id,  # ä½¿ç”¨request_idä½œä¸ºä¸´æ—¶execution_id
                    document_name=document_name,
                    status="running",
                    session_id=FIXED_SESSION_ID
                )
                db.add(workflow_execution)
                await db.commit()
                print(f"å·¥ä½œæµæ‰§è¡Œè®°å½•å·²åˆ›å»º: {request_id}")
            except Exception as e:
                print(f"åˆ›å»ºå·¥ä½œæµæ‰§è¡Œè®°å½•å¤±è´¥: {e}")
                # ä¸é˜»å¡webhookå‘é€ï¼Œç»§ç»­æ‰§è¡Œ
            
            # ç›´æ¥å‘æŒ‡å®šçš„webhook URLå‘é€POSTè¯·æ±‚
            try:
                webhook_timeout = int(get_config_value("webhook_timeout", "30"))
                async with httpx.AsyncClient(timeout=webhook_timeout) as client:
                    response = await client.post(webhook_url, json=webhook_data)
                    response.raise_for_status()
                    print("Webhookå‘é€æˆåŠŸ")
            except Exception as e:
                print(f"Webhookå‘é€å¤±è´¥: {e}")
                # å¦‚æœwebhookå‘é€å¤±è´¥ï¼Œæ›´æ–°æ‰§è¡Œè®°å½•çŠ¶æ€
                try:
                    from sqlalchemy import update
                    stmt = update(WorkflowExecution).where(
                        WorkflowExecution.execution_id == request_id
                    ).values(
                        status="error",
                        stopped_at=datetime.utcnow()
                    )
                    await db.execute(stmt)
                    await db.commit()
                except Exception as update_error:
                    print(f"æ›´æ–°å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€å¤±è´¥: {update_error}")
        else:
            print(f"é€’å½’æ·±åº¦ä¸º0ï¼Œè·³è¿‡å­æ–‡æ¡£è¯†åˆ«webhook")

        return {
            "success": True,
            "message": f"æˆåŠŸæ‘„å–æ–‡æ¡£ï¼Œå…±å¤„ç†äº† {total_chunks} ä¸ªæ–‡æœ¬å—",
            "document_name": document_name,
            "collection_name": collection_name,
            "total_chunks": total_chunks,
            "source_id": source.id  # è¿”å›Source IDç”¨äºé€’å½’è°ƒç”¨
        }

    except Exception as e:
        error_message = f"æ‘„å–å¤±è´¥: {e.__class__.__name__}: {str(e)}"
        print(error_message)
        raise HTTPException(status_code=500, detail=error_message)


@router.get("/documents", summary="è·å–é€šè¿‡agentic ingestå¤„ç†çš„æ–‡æ¡£åˆ—è¡¨")
async def get_agentic_ingest_documents(
    db: AsyncSession = Depends(get_db)
):
    """
    è·å–é€šè¿‡agentic ingestå¤„ç†çš„æ–‡æ¡£åˆ—è¡¨
    è¿”å›æ‰€æœ‰ä½¿ç”¨å›ºå®šsession_idå­˜å‚¨çš„æ–‡æ¡£
    """
    try:
        FIXED_SESSION_ID = "fixed_session_id_for_agenttic_ingest"
        
        # æŸ¥è¯¢æ•°æ®åº“ä¸­çš„sourceè®°å½•
        stmt = select(Source).where(Source.session_id == FIXED_SESSION_ID)
        result = await db.execute(stmt)
        sources = result.scalars().all()
        
        documents = []
        for source in sources:
            documents.append({
                "id": source.id,
                "title": source.title,
                "url": source.url,
                "created_at": source.created_at.isoformat() if source.created_at else None
            })
        
        return {
            "success": True,
            "documents": documents,
            "total": len(documents)
        }
    
    except Exception as e:
        error_message = f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e.__class__.__name__}: {str(e)}"
        print(error_message)
        raise HTTPException(status_code=500, detail=error_message)


@router.post("/workflow_response", summary="å·¥ä½œæµå“åº”å¤„ç†æ¥å£ï¼ˆå…¼å®¹æ€§ç«¯ç‚¹ï¼‰")
async def workflow_response(
    background_tasks: BackgroundTasks,
    data: dict = Body(...),
    db: AsyncSession = Depends(get_db)
):
    """
    å·¥ä½œæµå“åº”å¤„ç†æ¥å£ - å…¼å®¹æ€§ç«¯ç‚¹
    æ­¤ç«¯ç‚¹å°†è¯·æ±‚é‡å®šå‘åˆ°ç»Ÿä¸€çš„ agenttic_ingest æ¥å£è¿›è¡Œå¤„ç†
    """
    print("æ”¶åˆ°workflow_responseè¯·æ±‚ï¼Œé‡å®šå‘åˆ°ç»Ÿä¸€å¤„ç†æ¥å£")
    return await agenttic_ingest(background_tasks, data, db)
