"""
æ™ºèƒ½ç¼–æ’å™¨ - å®ç°"é—®é¢˜æ‹†è§£-æ€è€ƒ-å·¥å…·è°ƒç”¨"æµç¨‹
"""
import json
from typing import List, Dict, Any, Optional, AsyncGenerator
from .models import ToolExecutionContext, RunConfig, ToolMode
from .query_decomposer import QueryDecomposer
from .reasoning_engine import ReasoningEngine
from .orchestrator import ToolOrchestrator
from .selector import StrategySelector
from ..config import LLM_SERVICE_URL
import httpx


class IntelligentOrchestrator:
    """
    æ™ºèƒ½ç¼–æ’å™¨ï¼šå®ç°"é—®é¢˜æ‹†è§£-æ€è€ƒ-å·¥å…·è°ƒç”¨"çš„å®Œæ•´æµç¨‹
    """
    
    def __init__(self, llm_service_url: str = LLM_SERVICE_URL):
        self.llm_service_url = llm_service_url
        self.decomposer = QueryDecomposer(llm_service_url)
        self.reasoning_engine = ReasoningEngine(llm_service_url)
        self.tool_orchestrator = ToolOrchestrator(llm_service_url)
        
        self.synthesis_prompt = """
åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œè¯·ä¸ºç”¨æˆ·æä¾›ä¸€ä¸ªå®Œæ•´ã€å‡†ç¡®çš„å›ç­”ã€‚

åŸå§‹é—®é¢˜: {original_query}

ç‹¬ç«‹æ€è€ƒç»“æœ:
{reasoning_summary}

å·¥å…·è°ƒç”¨ç»“æœ:
{tool_results}

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

é‡è¦æŒ‡å¯¼ï¼š
1. å¦‚æœå·¥å…·è°ƒç”¨æˆåŠŸè·å–äº†ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨è¿™äº›æœ€æ–°ã€å…·ä½“çš„æ•°æ®æ¥å›ç­”é—®é¢˜
2. ä¸è¦è¯´"æ ¹æ®æœç´¢ç»“æœ"æˆ–"æ ¹æ®è·å–çš„ä¿¡æ¯"ç­‰æç¤ºæ€§è¯è¯­
3. ä¸è¦è¯´"æ— æ³•è·å–"ã€"æ— æ³•è®¿é—®"ç­‰æ¶ˆæè¡¨è¿°ï¼Œå¦‚æœå·¥å…·å·²è·å–ä¿¡æ¯
4. ç›´æ¥åŸºäºè·å¾—çš„å…·ä½“æ•°æ®ç»™å‡ºæ˜ç¡®ã€è‚¯å®šçš„ç­”æ¡ˆ
5. å¦‚æœå·¥å…·è°ƒç”¨å¤±è´¥æˆ–æ— ç»“æœï¼Œæ‰è¯´æ˜æ— æ³•è·å–ä¿¡æ¯

è¯·ç»¼åˆæ‰€æœ‰ä¿¡æ¯ï¼Œæä¾›ä¸€ä¸ªç»“æ„åŒ–çš„å®Œæ•´å›ç­”ï¼š
- ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜
- ä½¿ç”¨è·å–åˆ°çš„å…·ä½“æ•°æ®å’Œä¿¡æ¯
- ä¿æŒå®¢è§‚å’Œå‡†ç¡®
- ç”¨è‡ªç„¶çš„è¯­è¨€ç»„ç»‡å›ç­”

è¯·ç”¨è‡ªç„¶çš„è¯­è¨€ç»„ç»‡å›ç­”ï¼Œä¸éœ€è¦è¿”å›JSONæ ¼å¼ã€‚
"""

    async def process_query_intelligently(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig
    ) -> Dict[str, Any]:
        """
        æ™ºèƒ½å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼šé—®é¢˜æ‹†è§£-æ€è€ƒ-å·¥å…·è°ƒç”¨
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
        
        Returns:
            å¤„ç†ç»“æœ
        """
        execution_context = ToolExecutionContext(
            question=query,
            contexts=contexts,
            run_config=run_config
        )
        
        try:
            # æ™ºèƒ½è·¯ç”±ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ç®€å•é—®é¢˜ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨å·¥å…·
            complexity = self.decomposer.analyze_query_complexity(query)
            if complexity == "ç®€å•" and self._should_use_fast_route(query):
                print("[IntelligentOrchestrator] æ£€æµ‹åˆ°ç®€å•é—®é¢˜ï¼Œä½¿ç”¨å¿«é€Ÿè·¯ç”±...")
                return await self._handle_simple_query_directly(query, contexts, run_config)
            
            # ç¬¬ä¸€æ­¥ï¼šé—®é¢˜æ‹†è§£
            print("[IntelligentOrchestrator] å¼€å§‹é—®é¢˜æ‹†è§£...")
            decomposition = await self.decomposer.decompose(query, execution_context)
            
            # ç¬¬äºŒæ­¥ï¼šç‹¬ç«‹æ€è€ƒ
            print("[IntelligentOrchestrator] å¼€å§‹ç‹¬ç«‹æ€è€ƒ...")
            thoughts = await self.reasoning_engine.think_about_decomposition(
                decomposition, contexts, execution_context
            )
            
            # ç¬¬ä¸‰æ­¥ï¼šå†³å®šæ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
            need_tools, knowledge_gaps = self._should_invoke_tools(thoughts)
            
            tool_results = {}
            if need_tools:
                print("[IntelligentOrchestrator] æ£€æµ‹åˆ°çŸ¥è¯†ç¼ºå£ï¼Œå¼€å§‹å·¥å…·è°ƒç”¨...")
                tool_results = await self._execute_tools_for_gaps(
                    knowledge_gaps, query, contexts, run_config
                )
            else:
                print("[IntelligentOrchestrator] æ— éœ€å·¥å…·è°ƒç”¨ï¼ŒåŸºäºæ€è€ƒç»“æœç”Ÿæˆç­”æ¡ˆ...")
            
            # ç¬¬å››æ­¥ï¼šç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            final_answer = await self._synthesize_final_answer(
                query, decomposition, thoughts, tool_results, contexts, run_config
            )
            
            return {
                "answer": final_answer,
                "decomposition": decomposition,
                "reasoning": thoughts,
                "tool_results": tool_results,
                "used_tools": need_tools,
                "success": True
            }
            
        except Exception as e:
            print(f"[IntelligentOrchestrator] æ™ºèƒ½å¤„ç†å¤±è´¥: {e}")
            return {
                "answer": f"å¤„ç†é—®é¢˜æ—¶é‡åˆ°é”™è¯¯: {str(e)}",
                "success": False
            }

    async def process_query_intelligently_stream(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼æ™ºèƒ½å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
        
        Yields:
            æµå¼å¤„ç†äº‹ä»¶
        """
        execution_context = ToolExecutionContext(
            question=query,
            contexts=contexts,
            run_config=run_config
        )
        
        try:
            # æ™ºèƒ½è·¯ç”±ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ç®€å•é—®é¢˜ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨å·¥å…·
            complexity = self.decomposer.analyze_query_complexity(query)
            if complexity == "ç®€å•" and self._should_use_fast_route(query):
                yield {
                    "type": "reasoning",
                    "content": "ğŸš€ æ£€æµ‹åˆ°ç®€å•æŸ¥è¯¢ï¼Œç›´æ¥è·å–ä¿¡æ¯..."
                }
                async for event in self._handle_simple_query_directly_stream(query, contexts, run_config):
                    yield event
                return
            
            # ç¬¬ä¸€æ­¥ï¼šé—®é¢˜æ‹†è§£
            yield {
                "type": "reasoning",
                "content": "æ­£åœ¨åˆ†æå’Œæ‹†è§£æ‚¨çš„é—®é¢˜..."
            }
            
            decomposition = await self.decomposer.decompose(query, execution_context)
            
            # æ˜¾ç¤ºå­é—®é¢˜çš„å…·ä½“å†…å®¹
            sub_queries = decomposition.get('sub_queries', [])
            sub_queries_count = len(sub_queries)
            
            yield {
                "type": "reasoning",
                "content": f"é—®é¢˜æ‹†è§£å®Œæˆï¼Œè¯†åˆ«åˆ°{sub_queries_count}ä¸ªå…³é”®å­é—®é¢˜"
            }
            
            # é€ä¸€æ˜¾ç¤ºæ¯ä¸ªå­é—®é¢˜
            for i, sub_query in enumerate(sub_queries, 1):
                if isinstance(sub_query, dict):
                    question = sub_query.get("question", "")
                    importance = sub_query.get("importance", "ä¸­")
                else:
                    question = str(sub_query)
                    importance = "ä¸­"
                
                if question:
                    yield {
                        "type": "reasoning",
                        "content": f"å­é—®é¢˜{i}ï¼ˆ{importance}é‡è¦æ€§ï¼‰ï¼š{question}"
                    }
            
            # ç¬¬äºŒæ­¥ï¼šç‹¬ç«‹æ€è€ƒ
            yield {
                "type": "reasoning", 
                "content": "åŸºäºå·²æœ‰çŸ¥è¯†è¿›è¡Œç‹¬ç«‹æ€è€ƒ..."
            }
            
            thoughts = await self.reasoning_engine.think_about_decomposition(
                decomposition, contexts, execution_context
            )
            
            overall_confidence = self.reasoning_engine.assess_overall_confidence(thoughts)
            
            yield {
                "type": "reasoning",
                "content": f"æ€è€ƒå®Œæˆï¼Œæ•´ä½“ç½®ä¿¡åº¦: {overall_confidence}"
            }
            
            # ç¬¬ä¸‰æ­¥ï¼šå†³å®šæ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
            need_tools, knowledge_gaps = self._should_invoke_tools(thoughts)
            
            if need_tools:
                yield {
                    "type": "reasoning",
                    "content": f"æ£€æµ‹åˆ°{len(knowledge_gaps)}ä¸ªçŸ¥è¯†ç¼ºå£ï¼Œå¼€å§‹æœç´¢å¤–éƒ¨ä¿¡æ¯..."
                }
                
                # æµå¼æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_results = {}
                async for tool_event in self._execute_tools_for_gaps_stream(
                    knowledge_gaps, query, contexts, run_config
                ):
                    # è½¬å‘å·¥å…·è°ƒç”¨äº‹ä»¶
                    yield tool_event
                    
                    # æ”¶é›†å·¥å…·ç»“æœ
                    if tool_event.get("type") == "final_tool_result":
                        tool_results = tool_event.get("result", {})
                
            else:
                yield {
                    "type": "reasoning",
                    "content": "åŸºäºç°æœ‰çŸ¥è¯†å¯ä»¥å›ç­”ï¼Œæ— éœ€å¤–éƒ¨æœç´¢"
                }
                tool_results = {}
            
            # ç¬¬å››æ­¥ï¼šæµå¼ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            yield {
                "type": "reasoning",
                "content": "æ­£åœ¨ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆå®Œæ•´ç­”æ¡ˆ..."
            }
            
            async for synthesis_event in self._synthesize_final_answer_stream(
                query, decomposition, thoughts, tool_results, contexts, run_config
            ):
                yield synthesis_event
                
        except Exception as e:
            yield {
                "type": "error",
                "message": f"æ™ºèƒ½å¤„ç†å¤±è´¥: {str(e)}"
            }

    def _should_invoke_tools(self, thoughts: List[Dict[str, Any]]) -> tuple[bool, List[Dict[str, Any]]]:
        """
        åŸºäºæ€è€ƒç»“æœå†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        
        Args:
            thoughts: æ€è€ƒç»“æœåˆ—è¡¨
        
        Returns:
            (æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨, çŸ¥è¯†ç¼ºå£åˆ—è¡¨)
        """
        all_gaps = self.reasoning_engine.extract_all_knowledge_gaps(thoughts)
        
        # å¦‚æœæœ‰é«˜é‡è¦æ€§çš„çŸ¥è¯†ç¼ºå£ï¼Œæˆ–è€…æ•´ä½“ç½®ä¿¡åº¦ä½ï¼Œåˆ™éœ€è¦å·¥å…·è°ƒç”¨
        high_importance_gaps = [gap for gap in all_gaps if gap.get("importance") == "é«˜"]
        overall_confidence = self.reasoning_engine.assess_overall_confidence(thoughts)
        
        # å†³ç­–é€»è¾‘
        need_tools = (
            len(high_importance_gaps) > 0 or  # æœ‰é«˜é‡è¦æ€§ç¼ºå£
            overall_confidence == "ä½" or     # æ•´ä½“ç½®ä¿¡åº¦ä½
            any(thought.get("needs_verification", False) for thought in thoughts)  # éœ€è¦éªŒè¯
        )
        
        return need_tools, all_gaps

    async def _execute_tools_for_gaps(
        self, 
        knowledge_gaps: List[Dict[str, Any]], 
        original_query: str,
        contexts: List[str],
        run_config: RunConfig
    ) -> Dict[str, Any]:
        """
        é’ˆå¯¹çŸ¥è¯†ç¼ºå£æ‰§è¡Œå·¥å…·è°ƒç”¨
        """
        if not knowledge_gaps:
            return {}
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        search_queries = []
        for gap in knowledge_gaps[:3]:  # é™åˆ¶æœ€å¤š3ä¸ªé«˜ä¼˜å…ˆçº§ç¼ºå£
            keywords = gap.get("search_keywords", [])
            if keywords:
                search_query = " ".join(keywords) if isinstance(keywords, list) else str(keywords)
                search_queries.append(search_query)
        
        if not search_queries:
            search_queries = [original_query]  # å›é€€åˆ°åŸå§‹æŸ¥è¯¢
        
        # ç¡®ä¿è‡³å°‘åŒ…å«åŸå§‹æŸ¥è¯¢
        if original_query not in search_queries:
            search_queries.insert(0, original_query)
        
        # é™åˆ¶æœç´¢æŸ¥è¯¢æ•°é‡ï¼Œé¿å…è¿‡å¤šæœç´¢
        final_queries = search_queries[:3]
        
        print(f"[IntelligentOrchestrator] ç›´æ¥æ§åˆ¶æœç´¢å…³é”®è¯: {final_queries}")
        
        # åˆ›å»ºå¸¦æœ‰é¢„å®šä¹‰æœç´¢å…³é”®è¯çš„å·¥å…·æ‰§è¡Œç¯å¢ƒ
        from .models import ToolExecutionContext
        from ..tools.web_search_tool import web_search_tool
        import uuid
        
        # ç”Ÿæˆç»Ÿä¸€çš„ä¼šè¯IDç”¨äºæœ¬æ¬¡æ™ºèƒ½é—®ç­”
        unified_session_id = str(uuid.uuid4())
        print(f"[IntelligentOrchestrator] ä½¿ç”¨ç»Ÿä¸€ä¼šè¯ID: {unified_session_id}")
        
        try:
            # ç›´æ¥è°ƒç”¨webæœç´¢å·¥å…·ï¼Œä¼ é€’é¢„å®šä¹‰çš„æœç´¢å…³é”®è¯
            result = await web_search_tool.execute(
                query=original_query,
                language="zh-CN",  # é»˜è®¤ä¸­æ–‡æœç´¢
                categories="",
                filter_list=None,
                model=run_config.model,
                predefined_queries=final_queries,
                session_id=unified_session_id
            )
            
            # è½¬æ¢ä¸ºå·¥å…·ç¼–æ’å™¨æœŸæœ›çš„æ ¼å¼
            if result.get("success"):
                return {
                    "answer": self._format_search_result_answer(result),
                    "success": True,
                    "steps": [
                        {
                            "type": "action",
                            "content": f"æ‰§è¡Œwebæœç´¢ï¼ŒæŸ¥è¯¢: {final_queries}",
                            "tool": "web_search"
                        },
                        {
                            "type": "observation", 
                            "content": result.get("message", "æœç´¢å®Œæˆ")
                        }
                    ],
                    "tool_calls": 1,
                    "session_id": unified_session_id
                }
            else:
                return {
                    "answer": "æœç´¢å¤±è´¥ï¼Œæ— æ³•è·å–å¤–éƒ¨ä¿¡æ¯",
                    "success": False,
                    "steps": []
                }
                
        except Exception as e:
            print(f"[IntelligentOrchestrator] ç›´æ¥æœç´¢è°ƒç”¨å¤±è´¥: {e}")
            # å›é€€åˆ°åŸæœ‰çš„å·¥å…·ç¼–æ’å™¨
            enhanced_query = f"{original_query} {' '.join(search_queries[:2])}"
            return await self.tool_orchestrator.execute_non_stream(
                enhanced_query, contexts, run_config
            )

    def _format_search_result_answer(self, search_result: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºç­”æ¡ˆæ–‡æœ¬
        """
        if not search_result.get("success"):
            return "æœç´¢å¤±è´¥ï¼Œæ— æ³•è·å–ä¿¡æ¯"
        
        retrieved_content = search_result.get("retrieved_content", [])
        if not retrieved_content:
            return "æœç´¢å®Œæˆä½†æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        
        # æ„å»ºåŸºäºæœç´¢ç»“æœçš„ç­”æ¡ˆ
        answer_parts = ["åŸºäºç½‘ç»œæœç´¢è·å–çš„ä¿¡æ¯ï¼š\n"]
        
        for i, content in enumerate(retrieved_content[:3], 1):  # åªå–å‰3ä¸ªæœ€ç›¸å…³çš„ç»“æœ
            content_text = content.get("content", "").strip()
            source_url = content.get("source_url", "")
            score = content.get("score", 0)
            
            if content_text:
                # é™åˆ¶æ¯ä¸ªç‰‡æ®µçš„é•¿åº¦
                if len(content_text) > 300:
                    content_text = content_text[:300] + "..."
                
                answer_parts.append(f"{i}. {content_text}")
                if source_url:
                    answer_parts.append(f"   æ¥æº: {source_url}")
                answer_parts.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(answer_parts)

    async def _execute_tools_for_gaps_stream(
        self, 
        knowledge_gaps: List[Dict[str, Any]], 
        original_query: str,
        contexts: List[str],
        run_config: RunConfig
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        é’ˆå¯¹çŸ¥è¯†ç¼ºå£æµå¼æ‰§è¡Œå·¥å…·è°ƒç”¨
        """
        if not knowledge_gaps:
            yield {"type": "final_tool_result", "result": {}}
            return
        
        # æ„å»ºæœç´¢æŸ¥è¯¢ï¼ˆä¸éæµå¼ç‰ˆæœ¬ç›¸åŒï¼‰
        search_queries = []
        for gap in knowledge_gaps[:3]:  # é™åˆ¶æœ€å¤š3ä¸ªé«˜ä¼˜å…ˆçº§ç¼ºå£
            keywords = gap.get("search_keywords", [])
            if keywords:
                search_query = " ".join(keywords) if isinstance(keywords, list) else str(keywords)
                search_queries.append(search_query)
        
        if not search_queries:
            search_queries = [original_query]  # å›é€€åˆ°åŸå§‹æŸ¥è¯¢
        
        # ç¡®ä¿è‡³å°‘åŒ…å«åŸå§‹æŸ¥è¯¢
        if original_query not in search_queries:
            search_queries.insert(0, original_query)
        
        # é™åˆ¶æœç´¢æŸ¥è¯¢æ•°é‡ï¼Œé¿å…è¿‡å¤šæœç´¢
        final_queries = search_queries[:3]
        
        print(f"[IntelligentOrchestrator] æµå¼ç›´æ¥æ§åˆ¶æœç´¢å…³é”®è¯: {final_queries}")
        
        from ..tools.web_search_tool import web_search_tool
        import uuid
        
        # ç”Ÿæˆç»Ÿä¸€çš„ä¼šè¯IDç”¨äºæœ¬æ¬¡æ™ºèƒ½é—®ç­”
        unified_session_id = str(uuid.uuid4())
        
        try:
            # å‘å‡ºæœç´¢å¼€å§‹äº‹ä»¶
            yield {
                "type": "tool_call",
                "name": "web_search",
                "args": {
                    "query": original_query,
                    "predefined_queries": final_queries
                }
            }
            
            # ç›´æ¥è°ƒç”¨webæœç´¢å·¥å…·ï¼ˆéæµå¼ï¼Œå› ä¸ºwebæœç´¢æœ¬èº«ä¸æ”¯æŒæµå¼ï¼‰
            result = await web_search_tool.execute(
                query=original_query,
                language="zh-CN",  # é»˜è®¤ä¸­æ–‡æœç´¢
                categories="",
                filter_list=None,
                model=run_config.model,
                predefined_queries=final_queries,
                session_id=unified_session_id
            )
            
            # å‘å‡ºæœç´¢ç»“æœäº‹ä»¶
            yield {
                "type": "tool_result",
                "name": "web_search",
                "result": result.get("message", "æœç´¢å®Œæˆ"),
                "success": result.get("success", False)
            }
            
            # å‘å‡ºæœ€ç»ˆç»“æœ
            if result.get("success"):
                final_result = {
                    "answer": self._format_search_result_answer(result),
                    "success": True,
                    "steps": [
                        {
                            "type": "action",
                            "content": f"æ‰§è¡Œwebæœç´¢ï¼ŒæŸ¥è¯¢: {final_queries}",
                            "tool": "web_search"
                        },
                        {
                            "type": "observation",
                            "content": result.get("message", "æœç´¢å®Œæˆ")
                        }
                    ],
                    "tool_calls": 1,
                    "session_id": unified_session_id
                }
            else:
                final_result = {
                    "answer": "æœç´¢å¤±è´¥ï¼Œæ— æ³•è·å–å¤–éƒ¨ä¿¡æ¯",
                    "success": False,
                    "steps": []
                }
            
            yield {"type": "final_tool_result", "result": final_result}
            
        except Exception as e:
            print(f"[IntelligentOrchestrator] æµå¼ç›´æ¥æœç´¢è°ƒç”¨å¤±è´¥: {e}")
            # å›é€€åˆ°åŸæœ‰çš„å·¥å…·ç¼–æ’å™¨
            enhanced_query = f"{original_query} {' '.join(search_queries[:2])}"
            
            async for event in self.tool_orchestrator.execute_stream(
                enhanced_query, contexts, run_config
            ):
                yield event

    async def _synthesize_final_answer(
        self, 
        original_query: str,
        decomposition: Dict[str, Any],
        thoughts: List[Dict[str, Any]],
        tool_results: Dict[str, Any],
        contexts: List[str],
        run_config: RunConfig
    ) -> str:
        """
        ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        """
        try:
            # å‡†å¤‡ç»¼åˆä¿¡æ¯
            reasoning_summary = self._format_reasoning_summary(thoughts)
            tool_summary = self._format_tool_results(tool_results)
            context_str = "\n".join(contexts) if contexts else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"
            
            prompt = self.synthesis_prompt.format(
                original_query=original_query,
                reasoning_summary=reasoning_summary,
                tool_results=tool_summary,
                context=context_str
            )
            
            # è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.llm_service_url}/chat/completions",
                    json={
                        "model": run_config.model,
                        "messages": [
                            {
                                "role": "system",
                                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜å›ç­”ä¸“å®¶ï¼Œèƒ½å¤Ÿç»¼åˆå¤šç§ä¿¡æ¯æºæä¾›å‡†ç¡®ã€å®Œæ•´çš„ç­”æ¡ˆã€‚"
                            },
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.1,
                        "max_tokens": 2000
                    },
                    timeout=45.0
                )
                
                if response.status_code != 200:
                    raise Exception(f"LLMè¯·æ±‚å¤±è´¥: {response.status_code}")
                
                result = response.json()
                return result["choices"][0]["message"]["content"]
                
        except Exception as e:
            print(f"ç­”æ¡ˆç»¼åˆå¤±è´¥: {e}")
            # è¿”å›åŸºäºæ€è€ƒç»“æœçš„ç®€å•ç­”æ¡ˆ
            return self.reasoning_engine.generate_preliminary_answer(thoughts)

    async def _synthesize_final_answer_stream(
        self,
        original_query: str,
        decomposition: Dict[str, Any], 
        thoughts: List[Dict[str, Any]],
        tool_results: Dict[str, Any],
        contexts: List[str],
        run_config: RunConfig
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼ç»¼åˆç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        """
        try:
            reasoning_summary = self._format_reasoning_summary(thoughts)
            tool_summary = self._format_tool_results(tool_results)
            context_str = "\n".join(contexts) if contexts else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"
            
            prompt = self.synthesis_prompt.format(
                original_query=original_query,
                reasoning_summary=reasoning_summary,
                tool_results=tool_summary,
                context=context_str
            )
            
            # æµå¼è°ƒç”¨LLM
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.llm_service_url}/chat/completions",
                    json={
                        "model": run_config.model,
                        "messages": [
                            {
                                "role": "system",
                                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜å›ç­”ä¸“å®¶ï¼Œèƒ½å¤Ÿç»¼åˆå¤šç§ä¿¡æ¯æºæä¾›å‡†ç¡®ã€å®Œæ•´çš„ç­”æ¡ˆã€‚"
                            },
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.1,
                        "max_tokens": 2000,
                        "stream": True
                    },
                    timeout=45.0
                )
                
                if response.status_code != 200:
                    yield {
                        "type": "error",
                        "message": f"LLMè¯·æ±‚å¤±è´¥: {response.status_code}"
                    }
                    return
                
                # å¤„ç†æµå¼å“åº”
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]  # ç§»é™¤"data: "å‰ç¼€
                        if data == "[DONE]":
                            break
                        
                        try:
                            chunk = json.loads(data)
                            if "choices" in chunk and len(chunk["choices"]) > 0:
                                delta = chunk["choices"][0].get("delta", {})
                                if "content" in delta:
                                    yield {
                                        "type": "content",
                                        "content": delta["content"]
                                    }
                        except json.JSONDecodeError:
                            continue
                            
        except Exception as e:
            yield {
                "type": "error", 
                "message": f"æµå¼ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}"
            }

    def _format_reasoning_summary(self, thoughts: List[Dict[str, Any]]) -> str:
        """
        æ ¼å¼åŒ–æ€è€ƒç»“æœæ‘˜è¦
        """
        if not thoughts:
            return "æ— æ€è€ƒç»“æœ"
        
        summary_parts = []
        for i, thought in enumerate(thoughts, 1):
            question = thought.get("question", f"å­é—®é¢˜{i}")
            process = thought.get("thought_process", "æ— æ€è€ƒè¿‡ç¨‹")
            confidence = thought.get("confidence_level", "æœªçŸ¥")
            
            summary_parts.append(f"é—®é¢˜{i}: {question}")
            summary_parts.append(f"æ€è€ƒ: {process}")
            summary_parts.append(f"ç½®ä¿¡åº¦: {confidence}")
            summary_parts.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(summary_parts)

    def _format_tool_results(self, tool_results: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–å·¥å…·è°ƒç”¨ç»“æœ
        """
        if not tool_results:
            return "æ— å·¥å…·è°ƒç”¨ç»“æœ"
        
        if not tool_results.get("success", False):
            return "å·¥å…·è°ƒç”¨å¤±è´¥ï¼Œæ— å¯ç”¨ä¿¡æ¯"
        
        # ä¼˜å…ˆä½¿ç”¨å®Œæ•´ç­”æ¡ˆ
        answer = tool_results.get("answer", "")
        if answer and len(answer.strip()) > 10:  # ç¡®ä¿ç­”æ¡ˆæœ‰å®é™…å†…å®¹
            return f"è·å–çš„ä¿¡æ¯: {answer.strip()}"
        
        # å…¶æ¬¡ä½¿ç”¨æ­¥éª¤ä¸­çš„å†…å®¹ä¿¡æ¯
        steps = tool_results.get("steps", [])
        content_steps = []
        
        for step in steps:
            step_type = step.get("type", "")
            content = step.get("content", "")
            
            # ä¼˜å…ˆæ”¶é›†åŒ…å«å®é™…ä¿¡æ¯çš„æ­¥éª¤
            if step_type == "content" and content and len(content.strip()) > 10:
                content_steps.append(content.strip())
            elif "å¤©æ°”" in content or "æ¸©åº¦" in content or "é™æ°´" in content:  # å¤©æ°”ç›¸å…³å†…å®¹
                content_steps.append(content.strip())
        
        if content_steps:
            return "è·å–çš„å…·ä½“ä¿¡æ¯:\n" + "\n".join(content_steps)
        
        # æœ€åå°è¯•ä»æ‰€æœ‰æ­¥éª¤ä¸­æå–ä¿¡æ¯
        if steps:
            step_summaries = []
            for step in steps:
                step_type = step.get("type", "unknown")
                content = step.get("content", "")
                if content and len(content.strip()) > 5:
                    step_summaries.append(f"{step_type}: {content.strip()}")
            
            if step_summaries:
                return "å·¥å…·æ‰§è¡Œè¿‡ç¨‹:\n" + "\n".join(step_summaries[:3])  # é™åˆ¶æ˜¾ç¤ºå‰3ä¸ª
        
        return "å·¥å…·è°ƒç”¨å®Œæˆä½†æå–ä¸åˆ°å…·ä½“ä¿¡æ¯"

    def _should_use_fast_route(self, query: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å¿«é€Ÿè·¯ç”±ï¼ˆè·³è¿‡å¤æ‚åˆ†è§£ï¼‰
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            
        Returns:
            æ˜¯å¦ä½¿ç”¨å¿«é€Ÿè·¯ç”±
        """
        import re
        
        # æ˜ç¡®çš„å¿«é€Ÿè·¯ç”±æ¨¡å¼ï¼ˆè¿™äº›é—®é¢˜é€šå¸¸åªéœ€è¦ä¸€æ¬¡å·¥å…·è°ƒç”¨ï¼‰
        fast_route_patterns = [
            r'(ä»Šå¤©|ç°åœ¨|å½“å‰|ç›®å‰).*(å¤©æ°”|æ°”æ¸©|æ¸©åº¦|ä¸‹é›¨|æ™´å¤©)',  # å½“å‰å¤©æ°”
            r'.*(å¤©æ°”|æ°”æ¸©|æ¸©åº¦).*å¦‚ä½•',                        # å¤©æ°”è¯¢é—®
            r'.*(æ—¶é—´|å‡ ç‚¹).*ç°åœ¨',                            # å½“å‰æ—¶é—´
            r'.*è‚¡ä»·.*å¤šå°‘',                                  # è‚¡ä»·æŸ¥è¯¢
            r'.*ä»·æ ¼.*å¤šå°‘',                                  # ä»·æ ¼æŸ¥è¯¢
            r'.*æ±‡ç‡.*å¤šå°‘',                                  # æ±‡ç‡æŸ¥è¯¢
            r'.*æ–°é—».*ä»Šå¤©',                                  # ä»Šæ—¥æ–°é—»
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…å¿«é€Ÿè·¯ç”±æ¨¡å¼
        return any(re.search(pattern, query) for pattern in fast_route_patterns)

    async def _handle_simple_query_directly(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig
    ) -> Dict[str, Any]:
        """
        ç›´æ¥å¤„ç†ç®€å•é—®é¢˜ï¼Œè·³è¿‡å¤æ‚çš„åˆ†è§£å’Œæ¨ç†æµç¨‹
        
        Args:
            query: ç®€å•é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            # ç›´æ¥ä½¿ç”¨å·¥å…·ç¼–æ’å™¨å¤„ç†
            result = await self.tool_orchestrator.execute_non_stream(
                query, contexts, run_config
            )
            
            return {
                "answer": result.get("answer", "æ— æ³•è·å–ä¿¡æ¯"),
                "decomposition": {
                    "original_query": query,
                    "complexity_level": "ç®€å•",
                    "sub_queries": [{"question": query}]
                },
                "reasoning": [{"question": query, "confidence_level": "é«˜"}],
                "tool_results": result,
                "used_tools": True,
                "success": result.get("success", False),
                "fast_route": True  # æ ‡è®°ä½¿ç”¨äº†å¿«é€Ÿè·¯ç”±
            }
            
        except Exception as e:
            print(f"å¿«é€Ÿè·¯ç”±å¤„ç†å¤±è´¥: {e}")
            return {
                "answer": f"å¤„ç†ç®€å•é—®é¢˜æ—¶é‡åˆ°é”™è¯¯: {str(e)}",
                "success": False,
                "fast_route": True
            }

    async def _handle_simple_query_directly_stream(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼å¤„ç†ç®€å•é—®é¢˜ï¼Œè·³è¿‡å¤æ‚çš„åˆ†è§£å’Œæ¨ç†æµç¨‹
        
        Args:
            query: ç®€å•é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
            
        Yields:
            æµå¼å¤„ç†äº‹ä»¶
        """
        try:
            # ç›´æ¥ä½¿ç”¨å·¥å…·ç¼–æ’å™¨æµå¼å¤„ç†
            async for event in self.tool_orchestrator.execute_stream(
                query, contexts, run_config
            ):
                yield event
                
        except Exception as e:
            yield {
                "type": "error",
                "message": f"å¿«é€Ÿè·¯ç”±æµå¼å¤„ç†å¤±è´¥: {str(e)}"
            }

    async def close(self):
        """æ¸…ç†èµ„æº"""
        if self.tool_orchestrator:
            await self.tool_orchestrator.close()
