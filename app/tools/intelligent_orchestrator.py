"""
æ™ºèƒ½ç¼–æ’å™¨ - å®ç°"é—®é¢˜æ‹†è§£-æ€è€ƒ-å·¥å…·è°ƒç”¨"æµç¨‹
"""
import json
from typing import List, Dict, Any, Optional, AsyncGenerator
from .models import ToolExecutionContext, RunConfig, ToolMode
from .query_decomposer import QueryDecomposer
from .reasoning_engine import ReasoningEngine
from .orchestrator import ToolOrchestrator
from .selector import StrategySelector
from .search_planner import SearchPlanner
from .formatters import OutputFormatter
from ..config_manager import get_config_value
from .prompts import SYNTHESIS_SYSTEM_PROMPT, SYNTHESIS_USER_PROMPT_TEMPLATE
import httpx


class IntelligentOrchestrator:
    """
    æ™ºèƒ½ç¼–æ’å™¨ï¼šå®ç°"é—®é¢˜æ‹†è§£-æ€è€ƒ-å·¥å…·è°ƒç”¨"çš„å®Œæ•´æµç¨‹
    """
    
    def __init__(self, llm_service_url: str = None):
        self.llm_service_url = llm_service_url or get_config_value("llm_service_url", "http://localhost:11434/v1")
        self.decomposer = QueryDecomposer(llm_service_url)
        self.reasoning_engine = ReasoningEngine(llm_service_url)
        self.tool_orchestrator = ToolOrchestrator(llm_service_url)
        self.search_planner = SearchPlanner()

    async def process_query_intelligently(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        æ™ºèƒ½å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼šé—®é¢˜æ‹†è§£-æ€è€ƒ-å·¥å…·è°ƒç”¨ (éæµå¼ç‰ˆæœ¬)
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
            conversation_history: å¯¹è¯å†å²
        
        Returns:
            å¤„ç†ç»“æœ
        """
        # è°ƒç”¨ç»Ÿä¸€çš„æ ¸å¿ƒå¤„ç†æ–¹æ³•
        result = {"answer": "", "decomposition": {}, "reasoning": [], "tool_results": {}, "used_tools": False, "success": True}
        
        async def collect_result(event_data):
            """æ”¶é›†éæµå¼ç»“æœçš„å›è°ƒå‡½æ•°"""
            if event_data.get("type") == "final_result":
                result.update(event_data.get("data", {}))
        
        # ä½¿ç”¨æ ¸å¿ƒå¤„ç†æ–¹æ³•ï¼Œä¼ å…¥ç»“æœæ”¶é›†å›è°ƒ
        await self._process_query_core(query, contexts, run_config, conversation_history, collect_result)
        return result

    async def process_query_intelligently_stream(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼æ™ºèƒ½å¤„ç†ç”¨æˆ·æŸ¥è¯¢ (æµå¼ç‰ˆæœ¬)
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
            conversation_history: å¯¹è¯å†å²
        
        Yields:
            æµå¼å¤„ç†äº‹ä»¶
        """
        # åˆ›å»ºä¸€ä¸ªçœŸæ­£çš„æµå¼å›è°ƒå‡½æ•°ï¼Œå®æ—¶yieldäº‹ä»¶
        async def stream_event(event_data):
            """å®æ—¶æµå¼è¾“å‡ºå›è°ƒå‡½æ•°"""
            # è¿‡æ»¤æ‰final_resultäº‹ä»¶ï¼Œå› ä¸ºæµå¼å¤„ç†ä¸éœ€è¦æœ€ç»ˆç»“æœäº‹ä»¶
            if event_data.get("type") != "final_result":
                yield event_data
        
        # ä½¿ç”¨æµå¼æ ¸å¿ƒå¤„ç†æ–¹æ³•ï¼Œå®æ—¶yieldäº‹ä»¶
        async for event in self._process_query_core_stream(query, contexts, run_config, conversation_history):
            yield event

    async def _process_query_core(
        self,
        query: str,
        contexts: List[str],
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        event_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼ŒåŒæ—¶æ”¯æŒæµå¼å’Œéæµå¼è°ƒç”¨
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
            conversation_history: å¯¹è¯å†å²
            event_callback: äº‹ä»¶å›è°ƒå‡½æ•°ï¼Œç”¨äºæµå¼è¾“å‡ºæˆ–æ”¶é›†ç»“æœ
        
        Yields:
            å¤„ç†äº‹ä»¶ï¼ˆä»…æµå¼è°ƒç”¨æ—¶ï¼‰
        """
        execution_context = ToolExecutionContext(
            question=query,
            contexts=contexts,
            run_config=run_config,
            conversation_history=conversation_history
        )
        
        try:
            # æ™ºèƒ½è·¯ç”±ï¼šæ£€æŸ¥é—®é¢˜çš„å¤„ç†æ–¹å¼ï¼ˆç”±LLMåˆ¤å®šï¼‰
            route_decision = await self.decomposer.should_use_fast_route_async(query, execution_context, conversation_history)
            use_fast_route = route_decision.get("use_fast_route", False)
            needs_tools = route_decision.get("needs_tools", True)
            reason = route_decision.get("reason", "")
            
            if use_fast_route:
                if needs_tools:
                    if event_callback:
                        await event_callback({
                            "type": "reasoning",
                            "content": f"åˆ†ç±»ä¸ºç®€å•æŸ¥è¯¢ï¼Œéœ€è¦å¤–éƒ¨å·¥å…·ï¼Œç›´æ¥è·å–ä¿¡æ¯... ({reason})"
                        })
                    
                    result = await self._handle_simple_query_unified(query, contexts, run_config, conversation_history, event_callback)
                    
                    if event_callback:
                        await event_callback({"type": "final_result", "data": result})
                    return result
                else:
                    if event_callback:
                        await event_callback({
                            "type": "reasoning",
                            "content": f"åˆ†ç±»ä¸ºç®€å•é—®é¢˜ï¼ŒåŸºäºå·²æœ‰çŸ¥è¯†å›ç­”... ({reason})"
                        })
                    
                    result = await self._handle_context_only_query_unified(query, contexts, run_config, conversation_history, event_callback)
                    
                    if event_callback:
                        await event_callback({"type": "final_result", "data": result})
                    return result
            
            # ç¬¬ä¸€æ­¥ï¼šé—®é¢˜æ‹†è§£
            if event_callback:
                await event_callback({
                    "type": "reasoning",
                    "content": "æ­£åœ¨åˆ†æå’Œæ‹†è§£æ‚¨çš„é—®é¢˜..."
                })
            else:
                print("[IntelligentOrchestrator] å¼€å§‹é—®é¢˜æ‹†è§£...")
            
            decomposition = await self.decomposer.decompose(query, execution_context, conversation_history)
            
            # æ˜¾ç¤ºå­é—®é¢˜çš„å…·ä½“å†…å®¹ï¼ˆä»…æµå¼æ—¶ï¼‰
            if event_callback:
                sub_queries = decomposition.get('sub_queries', [])
                sub_queries_count = len(sub_queries)
                
                await event_callback({
                    "type": "reasoning",
                    "content": f"é—®é¢˜æ‹†è§£å®Œæˆï¼Œè¯†åˆ«åˆ°{sub_queries_count}ä¸ªå…³é”®å­é—®é¢˜ã€‚"
                })
                
                # é€ä¸€æ˜¾ç¤ºæ¯ä¸ªå­é—®é¢˜
                for i, sub_query in enumerate(sub_queries, 1):
                    if isinstance(sub_query, dict):
                        question = sub_query.get("question", "")
                        importance = sub_query.get("importance", "ä¸­")
                    else:
                        question = str(sub_query)
                        importance = "ä¸­"
                    
                    if question:
                        await event_callback({
                            "type": "reasoning",
                            "content": f"å­é—®é¢˜{i}ï¼ˆ{importance}é‡è¦æ€§ï¼‰ï¼š{question}"
                        })
            
            # ç¬¬äºŒæ­¥ï¼šç‹¬ç«‹æ€è€ƒ
            if event_callback:
                await event_callback({
                    "type": "reasoning", 
                    "content": "ğŸ’¡åŸºäºå·²æœ‰çŸ¥è¯†è¿›è¡Œç‹¬ç«‹æ€è€ƒ..."
                })
            else:
                print("[IntelligentOrchestrator] å¼€å§‹ç‹¬ç«‹æ€è€ƒ...")
            
            thoughts = await self.reasoning_engine.think_about_decomposition(
                decomposition, contexts, execution_context, conversation_history
            )
            
            if event_callback:
                overall_confidence = self.reasoning_engine.assess_overall_confidence(thoughts)
                await event_callback({
                    "type": "reasoning",
                    "content": f"æ€è€ƒå®Œæˆï¼Œæ•´ä½“ç½®ä¿¡åº¦: {overall_confidence}ã€‚"
                })
            
            # ç¬¬ä¸‰æ­¥ï¼šå†³å®šæ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
            need_tools, knowledge_gaps = self._should_invoke_tools(thoughts)
            
            tool_results = {}
            if need_tools:
                if event_callback:
                    await event_callback({
                        "type": "reasoning",
                        "content": f"æ£€æµ‹åˆ°{len(knowledge_gaps)}ä¸ªçŸ¥è¯†ç¼ºå£ï¼Œå¼€å§‹æœç´¢å¤–éƒ¨ä¿¡æ¯..."
                    })
                else:
                    print("[IntelligentOrchestrator] æ£€æµ‹åˆ°çŸ¥è¯†ç¼ºå£ï¼Œå¼€å§‹å·¥å…·è°ƒç”¨...")
                
                tool_results = await self._execute_tools_for_gaps_unified(
                    knowledge_gaps, query, contexts, run_config, event_callback
                )
            else:
                if event_callback:
                    await event_callback({
                        "type": "reasoning",
                        "content": "åŸºäºç°æœ‰çŸ¥è¯†å¯ä»¥å›ç­”ï¼Œæ— éœ€å¤–éƒ¨æœç´¢"
                    })
                else:
                    print("[IntelligentOrchestrator] æ— éœ€å·¥å…·è°ƒç”¨ï¼ŒåŸºäºæ€è€ƒç»“æœç”Ÿæˆç­”æ¡ˆ...")
            
            # ç¬¬å››æ­¥ï¼šç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            if event_callback:
                await event_callback({
                    "type": "reasoning",
                    "content": "æ­£åœ¨ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆå®Œæ•´ç­”æ¡ˆ..."
                })
            
            final_answer = await self._synthesize_final_answer_unified(
                query, decomposition, thoughts, tool_results, contexts, run_config, conversation_history, event_callback
            )
            
            result = {
                "answer": final_answer,
                "decomposition": decomposition,
                "reasoning": thoughts,
                "tool_results": tool_results,
                "used_tools": need_tools,
                "success": True
            }
            
            if event_callback:
                await event_callback({"type": "final_result", "data": result})
            
            return result
            
        except Exception as e:
            error_msg = f"æ™ºèƒ½å¤„ç†å¤±è´¥: {str(e)}"
            if event_callback:
                await event_callback({
                    "type": "error",
                    "message": error_msg
                })
            else:
                print(f"[IntelligentOrchestrator] {error_msg}")
            
            error_result = {
                "answer": f"å¤„ç†é—®é¢˜æ—¶é‡åˆ°é”™è¯¯: {str(e)}",
                "success": False
            }
            
            if event_callback:
                await event_callback({"type": "final_result", "data": error_result})
            
            return error_result

    async def _process_query_core_stream(
        self,
        query: str,
        contexts: List[str],
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼Œå®æ—¶yieldäº‹ä»¶
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
            conversation_history: å¯¹è¯å†å²
        
        Yields:
            æµå¼å¤„ç†äº‹ä»¶
        """
        execution_context = ToolExecutionContext(
            question=query,
            contexts=contexts,
            run_config=run_config,
            conversation_history=conversation_history
        )
        
        try:
            # æ™ºèƒ½è·¯ç”±ï¼šæ£€æŸ¥é—®é¢˜çš„å¤„ç†æ–¹å¼ï¼ˆç”±LLMåˆ¤å®šï¼‰
            route_decision = await self.decomposer.should_use_fast_route_async(query, execution_context, conversation_history)
            use_fast_route = route_decision.get("use_fast_route", False)
            needs_tools = route_decision.get("needs_tools", True)
            reason = route_decision.get("reason", "")
            
            if use_fast_route:
                if needs_tools:
                    yield {
                        "type": "reasoning",
                        "content": f"åˆ†ç±»ä¸ºç®€å•æŸ¥è¯¢ï¼Œéœ€è¦å¤–éƒ¨å·¥å…·ï¼Œç›´æ¥è·å–ä¿¡æ¯... ({reason})"
                    }
                    
                    # ä½¿ç”¨å·¥å…·ç¼–æ’å™¨ç›´æ¥è¿›è¡Œæµå¼å¤„ç†
                    from ..tools.orchestrator import get_orchestrator
                    orchestrator = get_orchestrator()
                    if orchestrator:
                        async for event in orchestrator.execute_stream(query, contexts, run_config, conversation_history):
                            yield event
                    else:
                        # å›é€€åˆ°æ™®é€šæµå¼é—®ç­”
                        from ..llm_client import stream_answer
                        async for event in stream_answer(query, contexts, run_config.model, conversation_history):
                            yield event
                    return
                else:
                    yield {
                        "type": "reasoning",
                        "content": f"åˆ†ç±»ä¸ºç®€å•é—®é¢˜ï¼ŒåŸºäºå·²æœ‰çŸ¥è¯†å›ç­”... ({reason})"
                    }
                    
                    # ç›´æ¥è¿›è¡Œæµå¼åŸºäºä¸Šä¸‹æ–‡çš„é—®ç­”
                    context_str = "\n".join(contexts) if contexts else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"
                    
                    system_prompt = (
                        "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„åŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»å¯¹è¯å†å²ï¼Œç†è§£ç”¨æˆ·é—®é¢˜çš„å®Œæ•´è¯­å¢ƒï¼Œç„¶ååŸºäºä½ çš„å·²æœ‰çŸ¥è¯†å’Œæä¾›çš„ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
                        "é‡è¦æŒ‡å¯¼åŸåˆ™ï¼š\n"
                        "1. å……åˆ†ç†è§£å¯¹è¯å†å²ï¼šå¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯å¯¹ä¹‹å‰å¯¹è¯çš„å»¶ç»­æˆ–è¿½é—®ï¼ˆå¦‚'é‚£æ˜å¤©å‘¢ï¼Ÿ'ã€'è¿˜æœ‰å…¶ä»–çš„å—ï¼Ÿ'ï¼‰ï¼Œè¯·ç»“åˆå†å²å¯¹è¯æ¥ç†è§£å½“å‰é—®é¢˜çš„çœŸå®æ„å›¾ã€‚\n"
                        "2. ä¸è¦æåŠéœ€è¦æœç´¢æˆ–æŸ¥æ‰¾å¤–éƒ¨ä¿¡æ¯ï¼Œç›´æ¥ç»™å‡ºæ¸…æ™°ã€å‡†ç¡®çš„ç­”æ¡ˆã€‚\n"
                        "**é‡è¦è¦æ±‚ï¼šå¿…é¡»å®Œå…¨ä½¿ç”¨ä¸­æ–‡è¿›è¡Œå›ç­”ã€‚**"
                    )
                    
                    user_prompt = (
                        f"ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n{context_str}\n\n"
                        f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\n"
                        "è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
                    )
                    
                    # ä½¿ç”¨æµå¼LLMè°ƒç”¨
                    from ..llm_client import chat_complete_stream
                    async for event in chat_complete_stream(
                        system_prompt=system_prompt,
                        user_prompt=user_prompt,
                        model=run_config.model,
                        conversation_history=conversation_history
                    ):
                        yield event
                    return
            
            # ç¬¬ä¸€æ­¥ï¼šé—®é¢˜æ‹†è§£
            yield {
                "type": "reasoning",
                "content": "æ­£åœ¨åˆ†æå’Œæ‹†è§£æ‚¨çš„é—®é¢˜..."
            }
            
            decomposition = await self.decomposer.decompose(query, execution_context, conversation_history)
            
            # æ˜¾ç¤ºå­é—®é¢˜çš„å…·ä½“å†…å®¹
            sub_queries = decomposition.get('sub_queries', [])
            sub_queries_count = len(sub_queries)
            
            yield {
                "type": "reasoning",
                "content": f"é—®é¢˜æ‹†è§£å®Œæˆï¼Œè¯†åˆ«åˆ°{sub_queries_count}ä¸ªå…³é”®å­é—®é¢˜ã€‚"
            }
            
            # é€ä¸€æ˜¾ç¤ºæ¯ä¸ªå­é—®é¢˜
            for i, sub_query in enumerate(sub_queries, 1):
                if isinstance(sub_query, dict):
                    question = sub_query.get("question", "")
                    importance = sub_query.get("importance", "ä¸­")
                else:
                    question = str(sub_query)
                    importance = "ä¸­"
                
                if question:
                    yield {
                        "type": "reasoning",
                        "content": f"å­é—®é¢˜{i}ï¼ˆ{importance}é‡è¦æ€§ï¼‰ï¼š{question}"
                    }
            
            # ç¬¬äºŒæ­¥ï¼šç‹¬ç«‹æ€è€ƒ
            yield {
                "type": "reasoning", 
                "content": "ğŸ’¡åŸºäºå·²æœ‰çŸ¥è¯†è¿›è¡Œç‹¬ç«‹æ€è€ƒ..."
            }
            
            thoughts = await self.reasoning_engine.think_about_decomposition(
                decomposition, contexts, execution_context, conversation_history
            )
            
            overall_confidence = self.reasoning_engine.assess_overall_confidence(thoughts)
            yield {
                "type": "reasoning",
                "content": f"æ€è€ƒå®Œæˆï¼Œæ•´ä½“ç½®ä¿¡åº¦: {overall_confidence}ã€‚"
            }
            
            # ç¬¬ä¸‰æ­¥ï¼šå†³å®šæ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
            need_tools, knowledge_gaps = self._should_invoke_tools(thoughts)
            
            tool_results = {}
            if need_tools:
                yield {
                    "type": "reasoning",
                    "content": f"æ£€æµ‹åˆ°{len(knowledge_gaps)}ä¸ªçŸ¥è¯†ç¼ºå£ï¼Œå¼€å§‹æœç´¢å¤–éƒ¨ä¿¡æ¯..."
                }
                
                # æµå¼æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨ç»Ÿä¸€çš„å·¥å…·æ‰§è¡Œæ–¹æ³•
                tool_results = await self._execute_tools_for_gaps_unified(
                    knowledge_gaps, query, contexts, run_config, None
                )
                
                # æ‰‹åŠ¨å‘é€å·¥å…·è°ƒç”¨äº‹ä»¶ï¼ˆå› ä¸ºç»Ÿä¸€æ–¹æ³•å¯èƒ½ä¸å‘é€æµå¼äº‹ä»¶ï¼‰
                if tool_results.get("success", False):
                    yield {
                        "type": "tool_result",
                        "name": "web_search_and_recall",
                        "result": "æœç´¢å’Œå¬å›å®Œæˆ",
                        "success": True
                    }
            else:
                yield {
                    "type": "reasoning",
                    "content": "åŸºäºç°æœ‰çŸ¥è¯†å¯ä»¥å›ç­”ï¼Œæ— éœ€å¤–éƒ¨æœç´¢"
                }
            
            # ç¬¬å››æ­¥ï¼šæµå¼ç»¼åˆæœ€ç»ˆç­”æ¡ˆ
            yield {
                "type": "reasoning",
                "content": "æ­£åœ¨ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆå®Œæ•´ç­”æ¡ˆ..."
            }
            
            # ç›´æ¥è¿›è¡Œæµå¼ç­”æ¡ˆç»¼åˆï¼Œä¸ä½¿ç”¨ç»Ÿä¸€æ–¹æ³•çš„å›è°ƒæœºåˆ¶
            
            # å‡†å¤‡ç»¼åˆä¿¡æ¯
            reasoning_summary = OutputFormatter.format_reasoning_summary(thoughts)
            tool_summary = OutputFormatter.format_tool_results(tool_results)
            context_str = "\n".join(contexts) if contexts else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"
            
            user_prompt = SYNTHESIS_USER_PROMPT_TEMPLATE.format(
                original_query=query,
                reasoning_summary=reasoning_summary,
                tool_results=tool_summary,
                context=context_str
            )
            
            # ç›´æ¥ä½¿ç”¨æµå¼LLMè°ƒç”¨
            from ..llm_client import chat_complete_stream
            async for event in chat_complete_stream(
                system_prompt=SYNTHESIS_SYSTEM_PROMPT,
                user_prompt=user_prompt,
                model=run_config.model,
                conversation_history=conversation_history
            ):
                yield event
                
        except Exception as e:
            yield {
                "type": "error",
                "message": f"æ™ºèƒ½å¤„ç†å¤±è´¥: {str(e)}"
            }

    async def _run_web_search_and_recall(
        self,
        knowledge_gaps: List[Dict[str, Any]],
        original_query: str,
        run_config: RunConfig,
        is_simple_query: bool = False
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„webæœç´¢å’Œå¬å›å®ç°ï¼ˆæµå¼/éæµå¼å…±ç”¨ï¼‰
        
        Args:
            knowledge_gaps: çŸ¥è¯†ç¼ºå£åˆ—è¡¨
            original_query: åŸå§‹æŸ¥è¯¢
            run_config: è¿è¡Œé…ç½®
            is_simple_query: æ˜¯å¦ä¸ºç®€å•æŸ¥è¯¢æ¨¡å¼ï¼Œç”¨äºåº”ç”¨ä¸åŒçš„æœç´¢é…ç½®
            
        Returns:
            åŒ…å«knowledge_gaps_search_resultså’Œç»Ÿè®¡ä¿¡æ¯çš„ç»“æœå­—å…¸
        """
        if not knowledge_gaps:
            return {"knowledge_gaps_search_results": {}, "success": False, "message": "æ²¡æœ‰çŸ¥è¯†ç¼ºå£éœ€è¦æœç´¢"}
        
        try:
            # 1. ä½¿ç”¨æœç´¢è§„åˆ’å™¨ç”Ÿæˆç»Ÿä¸€çš„æŸ¥è¯¢åˆ—è¡¨ï¼Œæ ¹æ®æŸ¥è¯¢ç±»å‹ä½¿ç”¨ä¸åŒé…ç½®
            final_queries = self.search_planner.plan_search_queries(
                original_query, knowledge_gaps, is_simple_query=is_simple_query
            )
            
            from ..config import MAX_KNOWLEDGE_GAPS, GAP_RECALL_TOP_K
            from ..tools.web_search_tool import web_search_tool
            import uuid
            
            # ç”Ÿæˆç»Ÿä¸€çš„ä¼šè¯ID
            unified_session_id = str(uuid.uuid4())
            print(f"[IntelligentOrchestrator] ä½¿ç”¨ç»Ÿä¸€ä¼šè¯ID: {unified_session_id}")
            
            # 2. ä¸€æ¬¡æ€§æ‰§è¡Œwebæœç´¢ï¼ˆé¿å…é‡å¤æŠ“å–ï¼‰
            print(f"[IntelligentOrchestrator] å¼€å§‹ä¸€æ¬¡æ€§webæœç´¢ï¼ŒæŸ¥è¯¢: {final_queries}")
            search_result = await web_search_tool.execute(
                query=original_query,
                filter_list=None,
                model=run_config.model,
                predefined_queries=final_queries,
                session_id=unified_session_id,
                perform_retrieval=False,
                is_simple_query=is_simple_query
            )
            
            if not search_result.get("success") or not search_result.get("source_ids"):
                return {
                    "knowledge_gaps_search_results": {},
                    "success": False,
                    "message": "æœç´¢å¤±è´¥æˆ–æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®æº",
                    "search_result": search_result
                }
            
            source_ids = search_result.get("source_ids", [])
            print(f"[IntelligentOrchestrator] æœç´¢å®Œæˆï¼Œè·å¾—{len(source_ids)}ä¸ªæ•°æ®æº")
            
            # 3. ä¸ºæ¯ä¸ªçŸ¥è¯†ç¼ºå£è¿›è¡Œç‹¬ç«‹å¬å›
            selected_gaps = knowledge_gaps[:MAX_KNOWLEDGE_GAPS]
            knowledge_gaps_search_results = {}
            
            import asyncio
            recall_tasks = []
            
            for gap_idx, gap in enumerate(selected_gaps):
                gap_id = f"gap_{gap_idx}"
                gap_description = gap.get("gap_description", f"çŸ¥è¯†ç¼ºå£{gap_idx + 1}")
                
                # ä½¿ç”¨çŸ¥è¯†ç¼ºå£æè¿°è¿›è¡Œå¬å›
                recall_task = self._recall_for_knowledge_gap(
                    gap_description, source_ids, unified_session_id, GAP_RECALL_TOP_K
                )
                recall_tasks.append((gap_id, gap, recall_task))
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å¬å›ä»»åŠ¡
            for gap_id, gap, task in recall_tasks:
                try:
                    recalled_content = await task
                    knowledge_gaps_search_results[gap_id] = {
                        "gap": gap,
                        "recalled_content": recalled_content
                    }
                    print(f"[IntelligentOrchestrator] çŸ¥è¯†ç¼ºå£'{gap_id}'å¬å›å®Œæˆï¼Œè·å¾—{len(recalled_content)}ä¸ªå†…å®¹ç‰‡æ®µ")
                except Exception as e:
                    print(f"[IntelligentOrchestrator] çŸ¥è¯†ç¼ºå£'{gap_id}'å¬å›å¤±è´¥: {e}")
                    knowledge_gaps_search_results[gap_id] = {
                        "gap": gap,
                        "recalled_content": []
                    }
            
            # 4. ç»Ÿè®¡ä¿¡æ¯
            total_recalled = sum(len(result["recalled_content"]) for result in knowledge_gaps_search_results.values())
            
            return {
                "knowledge_gaps_search_results": knowledge_gaps_search_results,
                "success": True,
                "message": f"æœç´¢å’Œå¬å›å®Œæˆï¼šå¤„ç†äº†{len(final_queries)}ä¸ªæŸ¥è¯¢ï¼Œä¸º{len(selected_gaps)}ä¸ªçŸ¥è¯†ç¼ºå£å¬å›äº†{total_recalled}ä¸ªå†…å®¹ç‰‡æ®µ",
                "session_id": unified_session_id,
                "search_queries": final_queries,
                "source_ids": source_ids,
                "selected_gaps": selected_gaps,
                "statistics": {
                    "query_count": len(final_queries),
                    "gap_count": len(selected_gaps),
                    "source_count": len(source_ids),
                    "total_recalled": total_recalled
                }
            }
            
        except Exception as e:
            print(f"[IntelligentOrchestrator] ç»Ÿä¸€æœç´¢å’Œå¬å›å¤±è´¥: {e}")
            return {
                "knowledge_gaps_search_results": {},
                "success": False,
                "message": f"æœç´¢å’Œå¬å›è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            }

    def _should_invoke_tools(self, thoughts: List[Dict[str, Any]]) -> tuple[bool, List[Dict[str, Any]]]:
        """
        åŸºäºæ€è€ƒç»“æœå†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        
        Args:
            thoughts: æ€è€ƒç»“æœåˆ—è¡¨
        
        Returns:
            (æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨, çŸ¥è¯†ç¼ºå£åˆ—è¡¨)
        """
        all_gaps = self.reasoning_engine.extract_all_knowledge_gaps(thoughts)
        
        # å¦‚æœæœ‰é«˜é‡è¦æ€§çš„çŸ¥è¯†ç¼ºå£ï¼Œæˆ–è€…æ•´ä½“ç½®ä¿¡åº¦ä½ï¼Œåˆ™éœ€è¦å·¥å…·è°ƒç”¨
        high_importance_gaps = [gap for gap in all_gaps if gap.get("importance") == "é«˜"]
        overall_confidence = self.reasoning_engine.assess_overall_confidence(thoughts)
        
        # å†³ç­–é€»è¾‘
        need_tools = (
            len(high_importance_gaps) > 0 or  # æœ‰é«˜é‡è¦æ€§ç¼ºå£
            overall_confidence == "ä½" or     # æ•´ä½“ç½®ä¿¡åº¦ä½
            any(thought.get("needs_verification", False) for thought in thoughts)  # éœ€è¦éªŒè¯
        )
        
        return need_tools, all_gaps

    async def _execute_tools_for_gaps_unified(
        self, 
        knowledge_gaps: List[Dict[str, Any]], 
        original_query: str,
        contexts: List[str],
        run_config: RunConfig,
        event_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„å·¥å…·æ‰§è¡Œæ–¹æ³•ï¼ŒåŒæ—¶æ”¯æŒæµå¼å’Œéæµå¼è°ƒç”¨
        
        Args:
            knowledge_gaps: çŸ¥è¯†ç¼ºå£åˆ—è¡¨
            original_query: åŸå§‹æŸ¥è¯¢
            contexts: ä¸Šä¸‹æ–‡åˆ—è¡¨
            run_config: è¿è¡Œé…ç½®
            event_callback: äº‹ä»¶å›è°ƒå‡½æ•°ï¼Œç”¨äºæµå¼è¾“å‡º
        
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        if not knowledge_gaps:
            empty_result = {}
            if event_callback:
                await event_callback({"type": "final_tool_result", "result": empty_result})
            return empty_result
        
        try:
            # å‘å‡ºæœç´¢å¼€å§‹äº‹ä»¶ï¼ˆä»…æµå¼æ—¶ï¼‰
            if event_callback:
                await event_callback({
                    "type": "tool_call",
                    "name": "web_search_and_recall",
                    "args": {
                        "query": original_query,
                        "gap_count": len(knowledge_gaps)
                    }
                })
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æœç´¢å’Œå¬å›å®ç°
            result = await self._run_web_search_and_recall(knowledge_gaps, original_query, run_config)
            
            # å‘å‡ºæœç´¢ç»“æœäº‹ä»¶ï¼ˆä»…æµå¼æ—¶ï¼‰
            if event_callback:
                await event_callback({
                    "type": "tool_result",
                    "name": "web_search_and_recall", 
                    "result": result.get("message", "æœç´¢å’Œå¬å›å®Œæˆ"),
                    "success": result.get("success", False)
                })
            
            if not result.get("success"):
                error_result = {
                    "answer": result.get("message", "æœç´¢å’Œå¬å›å¤±è´¥"),
                    "success": False,
                    "steps": [],
                    "error": result.get("error")
                }
                
                if event_callback:
                    await event_callback({"type": "final_tool_result", "result": error_result})
                
                return error_result
            
            # æ ¼å¼åŒ–ä¸ºæœ€ç»ˆç­”æ¡ˆ
            knowledge_gaps_search_results = result.get("knowledge_gaps_search_results", {})
            selected_gaps = result.get("selected_gaps", [])
            statistics = result.get("statistics", {})
            
            final_result = {
                "answer": OutputFormatter.format_gap_based_answer(knowledge_gaps_search_results, selected_gaps),
                "success": True,
                "steps": [
                    {
                        "type": "action",
                        "content": f"ç»Ÿä¸€æ‰§è¡Œ{statistics.get('query_count', 0)}ä¸ªæœç´¢æŸ¥è¯¢",
                        "tool": "web_search_unified"
                    },
                    {
                        "type": "observation", 
                        "content": result.get("message", "æœç´¢å’Œå¬å›å®Œæˆ")
                    }
                ],
                "tool_calls": statistics.get("query_count", 0) + statistics.get("gap_count", 0),
                "session_id": result.get("session_id"),
                "knowledge_gaps_search_results": knowledge_gaps_search_results,
                "search_queries": result.get("search_queries", [])
            }
            
            if event_callback:
                await event_callback({"type": "final_tool_result", "result": final_result})
            
            return final_result
            
        except Exception as e:
            print(f"[IntelligentOrchestrator] ç»Ÿä¸€å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
            # å›é€€åˆ°åŸæœ‰çš„å·¥å…·ç¼–æ’å™¨
            if event_callback:
                # ä» run_config ä¸­è·å– is_simple_query ä¿¡æ¯
                is_simple_query = getattr(run_config, 'is_simple_query', False)
                final_queries = self.search_planner.plan_search_queries(original_query, knowledge_gaps, is_simple_query=is_simple_query)
                enhanced_query = f"{original_query} {' '.join(final_queries[:2])}"
                
                async for event in self.tool_orchestrator.execute_stream(
                    enhanced_query, contexts, run_config
                ):
                    await event_callback(event)
            
            error_result = {
                "answer": f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}",
                "success": False,
                "error": str(e)
            }
            return error_result

    async def _recall_for_knowledge_gap(
        self, 
        gap_description: str, 
        source_ids: List[int], 
        session_id: str, 
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ä¸ºç‰¹å®šçŸ¥è¯†ç¼ºå£å¬å›ç›¸å…³å†…å®¹
        
        Args:
            gap_description: çŸ¥è¯†ç¼ºå£æè¿°
            source_ids: å¯ç”¨çš„æ•°æ®æºIDåˆ—è¡¨
            session_id: ä¼šè¯ID
            top_k: å¬å›çš„å†…å®¹æ•°é‡
        
        Returns:
            å¬å›çš„å†…å®¹åˆ—è¡¨
        """
        try:
            from ..tools.web_search_tool import web_search_tool
            
            # ä½¿ç”¨web_search_toolçš„search_and_retrieveæ–¹æ³•
            hits = await web_search_tool.search_and_retrieve(
                query=gap_description,
                session_id=session_id,
                source_ids=source_ids
            )
            
            # é™åˆ¶è¿”å›ç»“æœæ•°é‡å¹¶æ ¼å¼åŒ–
            limited_hits = hits[:top_k]
            recalled_content = []
            
            for chunk, score in limited_hits:
                recalled_content.append({
                    "content": chunk.content,
                    "score": float(score),
                    "source_url": chunk.source.url if chunk.source else "",
                    "source_title": chunk.source.title if chunk.source else "",
                    "chunk_id": chunk.chunk_id
                })
            
            return recalled_content
            
        except Exception as e:
            print(f"[IntelligentOrchestrator] çŸ¥è¯†ç¼ºå£å¬å›å¤±è´¥: {e}")
            return []

    async def _synthesize_final_answer_unified(
        self, 
        original_query: str,
        decomposition: Dict[str, Any],
        thoughts: List[Dict[str, Any]],
        tool_results: Dict[str, Any],
        contexts: List[str],
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        event_callback: Optional[callable] = None
    ) -> str:
        """
        ç»Ÿä¸€çš„ç­”æ¡ˆç»¼åˆæ–¹æ³•ï¼ŒåŒæ—¶æ”¯æŒæµå¼å’Œéæµå¼è°ƒç”¨
        
        Args:
            original_query: åŸå§‹æŸ¥è¯¢
            decomposition: é—®é¢˜æ‹†è§£ç»“æœ
            thoughts: æ€è€ƒç»“æœ
            tool_results: å·¥å…·è°ƒç”¨ç»“æœ
            contexts: ä¸Šä¸‹æ–‡åˆ—è¡¨
            run_config: è¿è¡Œé…ç½®
            conversation_history: å¯¹è¯å†å²
            event_callback: äº‹ä»¶å›è°ƒå‡½æ•°ï¼Œç”¨äºæµå¼è¾“å‡º
            
        Returns:
            æœ€ç»ˆç­”æ¡ˆå­—ç¬¦ä¸²
        """
        try:
            # å‡†å¤‡ç»¼åˆä¿¡æ¯
            reasoning_summary = OutputFormatter.format_reasoning_summary(thoughts)
            tool_summary = OutputFormatter.format_tool_results(tool_results)
            context_str = "\n".join(contexts) if contexts else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"
            
            user_prompt = SYNTHESIS_USER_PROMPT_TEMPLATE.format(
                original_query=original_query,
                reasoning_summary=reasoning_summary,
                tool_results=tool_summary,
                context=context_str
            )
            
            # æ ¹æ®æ˜¯å¦æœ‰å›è°ƒæ¥å†³å®šä½¿ç”¨æµå¼è¿˜æ˜¯éæµå¼è°ƒç”¨
            if event_callback:
                # æµå¼è°ƒç”¨
                from ..llm_client import chat_complete_stream
                full_answer = ""
                async for event in chat_complete_stream(
                    system_prompt=SYNTHESIS_SYSTEM_PROMPT,
                    user_prompt=user_prompt,
                    model=run_config.model,
                    conversation_history=conversation_history
                ):
                    await event_callback(event)
                    # æ”¶é›†å®Œæ•´ç­”æ¡ˆç”¨äºè¿”å›
                    if event.get("type") == "content":
                        full_answer += event.get("content", "")
                return full_answer
            else:
                # éæµå¼è°ƒç”¨
                from ..llm_client import chat_complete
                return await chat_complete(
                    system_prompt=SYNTHESIS_SYSTEM_PROMPT,
                    user_prompt=user_prompt,
                    model=run_config.model,
                    conversation_history=conversation_history
                )
                
        except Exception as e:
            error_msg = f"ç­”æ¡ˆç»¼åˆå¤±è´¥: {str(e)}"
            fallback_answer = self.reasoning_engine.generate_preliminary_answer(thoughts)
            
            if event_callback:
                await event_callback({
                    "type": "error", 
                    "message": error_msg
                })
                # å°è¯•æµå¼è¾“å‡ºå›é€€ç­”æ¡ˆ
                await event_callback({
                    "type": "content",
                    "content": fallback_answer
                })
            else:
                print(error_msg)
            
            return fallback_answer

    async def _handle_simple_query_unified(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        event_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„ç®€å•é—®é¢˜å¤„ç†æ–¹æ³•ï¼ŒåŒæ—¶æ”¯æŒæµå¼å’Œéæµå¼è°ƒç”¨
        
        Args:
            query: ç®€å•é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
            conversation_history: å¯¹è¯å†å²
            event_callback: äº‹ä»¶å›è°ƒå‡½æ•°ï¼Œç”¨äºæµå¼è¾“å‡º
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            if event_callback:
                # æµå¼å¤„ç†
                events_queue = []
                
                async def collect_tool_events(event_data):
                    """æ”¶é›†å·¥å…·äº‹ä»¶"""
                    events_queue.append(event_data)
                    await event_callback(event_data)
                
                # ä½¿ç”¨å·¥å…·ç¼–æ’å™¨æµå¼å¤„ç†
                result = {}
                async for event in self.tool_orchestrator.execute_stream(
                    query, contexts, run_config, conversation_history
                ):
                    await collect_tool_events(event)
                    # æ”¶é›†æœ€ç»ˆç»“æœ
                    if event.get("type") == "final_result":
                        result = event.get("data", {})
                        

                
                return {
                    "answer": result.get("answer", "æ— æ³•è·å–ä¿¡æ¯"),
                    "decomposition": {
                        "original_query": query,
                        "complexity_level": "ç®€å•",
                        "sub_queries": [{"question": query}]
                    },
                    "reasoning": [{"question": query, "confidence_level": "é«˜"}],
                    "tool_results": result,
                    "used_tools": True,
                    "success": result.get("success", False),
                    "fast_route": True  # æ ‡è®°ä½¿ç”¨äº†å¿«é€Ÿè·¯ç”±
                }
            else:
                # éæµå¼å¤„ç†
                result = await self.tool_orchestrator.execute_non_stream(
                    query, contexts, run_config, conversation_history
                )
                

                
                return {
                    "answer": result.get("answer", "æ— æ³•è·å–ä¿¡æ¯"),
                    "decomposition": {
                        "original_query": query,
                        "complexity_level": "ç®€å•",
                        "sub_queries": [{"question": query}]
                    },
                    "reasoning": [{"question": query, "confidence_level": "é«˜"}],
                    "tool_results": result,
                    "used_tools": True,
                    "success": result.get("success", False),
                    "fast_route": True  # æ ‡è®°ä½¿ç”¨äº†å¿«é€Ÿè·¯ç”±
                }
            
        except Exception as e:
            error_msg = f"å¿«é€Ÿè·¯ç”±å¤„ç†å¤±è´¥: {str(e)}"
            error_result = {
                "answer": f"å¤„ç†ç®€å•é—®é¢˜æ—¶é‡åˆ°é”™è¯¯: {str(e)}",
                "success": False,
                "fast_route": True
            }
            
            if event_callback:
                await event_callback({
                    "type": "error",
                    "message": error_msg
                })
            else:
                print(error_msg)
                
            return error_result

    async def _handle_context_only_query_unified(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        event_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„ä¸Šä¸‹æ–‡æŸ¥è¯¢å¤„ç†æ–¹æ³•ï¼ŒåŒæ—¶æ”¯æŒæµå¼å’Œéæµå¼è°ƒç”¨
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
            conversation_history: å¯¹è¯å†å²
            event_callback: äº‹ä»¶å›è°ƒå‡½æ•°ï¼Œç”¨äºæµå¼è¾“å‡º
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            # å‡†å¤‡åŸºäºä¸Šä¸‹æ–‡çš„æç¤º
            context_str = "\n".join(contexts) if contexts else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"
            
            system_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„åŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»å¯¹è¯å†å²ï¼Œç†è§£ç”¨æˆ·é—®é¢˜çš„å®Œæ•´è¯­å¢ƒï¼Œç„¶ååŸºäºä½ çš„å·²æœ‰çŸ¥è¯†å’Œæä¾›çš„ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
                "é‡è¦æŒ‡å¯¼åŸåˆ™ï¼š\n"
                "1. å……åˆ†ç†è§£å¯¹è¯å†å²ï¼šå¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯å¯¹ä¹‹å‰å¯¹è¯çš„å»¶ç»­æˆ–è¿½é—®ï¼ˆå¦‚'é‚£æ˜å¤©å‘¢ï¼Ÿ'ã€'è¿˜æœ‰å…¶ä»–çš„å—ï¼Ÿ'ï¼‰ï¼Œè¯·ç»“åˆå†å²å¯¹è¯æ¥ç†è§£å½“å‰é—®é¢˜çš„çœŸå®æ„å›¾ã€‚\n"
                "2. ä¸è¦æåŠéœ€è¦æœç´¢æˆ–æŸ¥æ‰¾å¤–éƒ¨ä¿¡æ¯ï¼Œç›´æ¥ç»™å‡ºæ¸…æ™°ã€å‡†ç¡®çš„ç­”æ¡ˆã€‚\n"
                "**é‡è¦è¦æ±‚ï¼šå¿…é¡»å®Œå…¨ä½¿ç”¨ä¸­æ–‡è¿›è¡Œå›ç­”ã€‚**"
            )
            
            user_prompt = (
                f"ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n{context_str}\n\n"
                f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\n"
                "è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
            )
            
            # æ ¹æ®æ˜¯å¦æœ‰å›è°ƒæ¥å†³å®šä½¿ç”¨æµå¼è¿˜æ˜¯éæµå¼è°ƒç”¨
            if event_callback:
                # æµå¼è°ƒç”¨
                from ..llm_client import chat_complete_stream
                full_answer = ""
                async for event in chat_complete_stream(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    model=run_config.model,
                    conversation_history=conversation_history
                ):
                    await event_callback(event)
                    # æ”¶é›†å®Œæ•´ç­”æ¡ˆç”¨äºè¿”å›
                    if event.get("type") == "content":
                        full_answer += event.get("content", "")
                        
                answer = full_answer
            else:
                # éæµå¼è°ƒç”¨
                from ..llm_client import chat_complete
                answer = await chat_complete(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    model=run_config.model,
                    conversation_history=conversation_history
                )
            
            return {
                "answer": answer,
                "decomposition": {
                    "original_query": query,
                    "complexity_level": "ç®€å•",
                    "sub_queries": [{"question": query}]
                },
                "reasoning": [{"question": query, "confidence_level": "é«˜", "needs_tools": False}],
                "tool_results": {},
                "used_tools": False,
                "success": True,
                "context_only": True  # æ ‡è®°åŸºäºä¸Šä¸‹æ–‡å›ç­”
            }
            
        except Exception as e:
            error_msg = f"åŸºäºä¸Šä¸‹æ–‡çš„é—®é¢˜å¤„ç†å¤±è´¥: {str(e)}"
            error_result = {
                "answer": f"å¤„ç†é—®é¢˜æ—¶é‡åˆ°é”™è¯¯: {str(e)}",
                "success": False,
                "context_only": True
            }
            
            if event_callback:
                await event_callback({
                    "type": "error", 
                    "message": error_msg
                })
            else:
                print(error_msg)
                
            return error_result

    async def close(self):
        """æ¸…ç†èµ„æº"""
        if self.tool_orchestrator:
            await self.tool_orchestrator.close()