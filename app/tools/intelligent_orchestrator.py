"""
æ™ºèƒ½ç¼–æ’å™¨ - å®ç°"é—®é¢˜æ‹†è§£-æ€è€ƒ-å·¥å…·è°ƒç”¨"æµç¨‹
"""
import json
from typing import List, Dict, Any, Optional, AsyncGenerator
from .models import ToolExecutionContext, RunConfig, ToolMode
from .query_decomposer import QueryDecomposer
from .reasoning_engine import ReasoningEngine
from .orchestrator import ToolOrchestrator
from .selector import StrategySelector
from .search_planner import SearchPlanner
from .formatters import OutputFormatter
from ..config import LLM_SERVICE_URL
from .prompts import SYNTHESIS_SYSTEM_PROMPT, SYNTHESIS_USER_PROMPT_TEMPLATE
import httpx


class IntelligentOrchestrator:
    """
    æ™ºèƒ½ç¼–æ’å™¨ï¼šå®ç°"é—®é¢˜æ‹†è§£-æ€è€ƒ-å·¥å…·è°ƒç”¨"çš„å®Œæ•´æµç¨‹
    """
    
    def __init__(self, llm_service_url: str = LLM_SERVICE_URL):
        self.llm_service_url = llm_service_url
        self.decomposer = QueryDecomposer(llm_service_url)
        self.reasoning_engine = ReasoningEngine(llm_service_url)
        self.tool_orchestrator = ToolOrchestrator(llm_service_url)
        self.search_planner = SearchPlanner()



    async def process_query_intelligently(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        æ™ºèƒ½å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼šé—®é¢˜æ‹†è§£-æ€è€ƒ-å·¥å…·è°ƒç”¨
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
        
        Returns:
            å¤„ç†ç»“æœ
        """
        execution_context = ToolExecutionContext(
            question=query,
            contexts=contexts,
            run_config=run_config
        )
        
        try:
            # æ™ºèƒ½è·¯ç”±ï¼šæ£€æŸ¥é—®é¢˜çš„å¤„ç†æ–¹å¼ï¼ˆç”±LLMåˆ¤å®šï¼‰
            route_decision = await self.decomposer.should_use_fast_route_async(query, execution_context, conversation_history)
            use_fast_route = route_decision.get("use_fast_route", False)
            needs_tools = route_decision.get("needs_tools", True)
            reason = route_decision.get("reason", "")
            
            if use_fast_route:
                if needs_tools:
                    print(f"[IntelligentOrchestrator] æ£€æµ‹åˆ°ç®€å•é—®é¢˜ï¼Œéœ€è¦å·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨å¿«é€Ÿè·¯ç”±: {reason}")
                    return await self._handle_simple_query_directly(query, contexts, run_config, conversation_history)
                else:
                    print(f"[IntelligentOrchestrator] æ£€æµ‹åˆ°ç®€å•é—®é¢˜ï¼Œæ— éœ€å·¥å…·ï¼Œç›´æ¥åŸºäºçŸ¥è¯†å›ç­”: {reason}")
                    return await self._handle_context_only_query(query, contexts, run_config, conversation_history)
            
            # ç¬¬ä¸€æ­¥ï¼šé—®é¢˜æ‹†è§£
            print("[IntelligentOrchestrator] å¼€å§‹é—®é¢˜æ‹†è§£...")
            decomposition = await self.decomposer.decompose(query, execution_context, conversation_history)
            
            # ç¬¬äºŒæ­¥ï¼šç‹¬ç«‹æ€è€ƒ
            print("[IntelligentOrchestrator] å¼€å§‹ç‹¬ç«‹æ€è€ƒ...")
            thoughts = await self.reasoning_engine.think_about_decomposition(
                decomposition, contexts, execution_context
            )
            
            # ç¬¬ä¸‰æ­¥ï¼šå†³å®šæ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
            need_tools, knowledge_gaps = self._should_invoke_tools(thoughts)
            
            tool_results = {}
            if need_tools:
                print("[IntelligentOrchestrator] æ£€æµ‹åˆ°çŸ¥è¯†ç¼ºå£ï¼Œå¼€å§‹å·¥å…·è°ƒç”¨...")
                tool_results = await self._execute_tools_for_gaps(
                    knowledge_gaps, query, contexts, run_config
                )
            else:
                print("[IntelligentOrchestrator] æ— éœ€å·¥å…·è°ƒç”¨ï¼ŒåŸºäºæ€è€ƒç»“æœç”Ÿæˆç­”æ¡ˆ...")
            
            # ç¬¬å››æ­¥ï¼šç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            final_answer = await self._synthesize_final_answer(
                query, decomposition, thoughts, tool_results, contexts, run_config, conversation_history
            )
            
            return {
                "answer": final_answer,
                "decomposition": decomposition,
                "reasoning": thoughts,
                "tool_results": tool_results,
                "used_tools": need_tools,
                "success": True
            }
            
        except Exception as e:
            print(f"[IntelligentOrchestrator] æ™ºèƒ½å¤„ç†å¤±è´¥: {e}")
            return {
                "answer": f"å¤„ç†é—®é¢˜æ—¶é‡åˆ°é”™è¯¯: {str(e)}",
                "success": False
            }

    async def process_query_intelligently_stream(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼æ™ºèƒ½å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
        
        Yields:
            æµå¼å¤„ç†äº‹ä»¶
        """
        execution_context = ToolExecutionContext(
            question=query,
            contexts=contexts,
            run_config=run_config
        )
        
        try:
            # æ™ºèƒ½è·¯ç”±ï¼šæ£€æŸ¥é—®é¢˜çš„å¤„ç†æ–¹å¼ï¼ˆç”±LLMåˆ¤å®šï¼‰
            route_decision = await self.decomposer.should_use_fast_route_async(query, execution_context, conversation_history)
            use_fast_route = route_decision.get("use_fast_route", False)
            needs_tools = route_decision.get("needs_tools", True)
            reason = route_decision.get("reason", "")
            
            if use_fast_route:
                if needs_tools:
                    yield {
                        "type": "reasoning",
                        "content": f"åˆ†ç±»ä¸ºç®€å•æŸ¥è¯¢ï¼Œéœ€è¦å¤–éƒ¨å·¥å…·ï¼Œç›´æ¥è·å–ä¿¡æ¯... ({reason})"
                    }
                    async for event in self._handle_simple_query_directly_stream(query, contexts, run_config, conversation_history):
                        yield event
                    return
                else:
                    yield {
                        "type": "reasoning",
                        "content": f"åˆ†ç±»ä¸ºç®€å•é—®é¢˜ï¼ŒåŸºäºå·²æœ‰çŸ¥è¯†å›ç­”... ({reason})"
                    }
                    async for event in self._handle_context_only_query_stream(query, contexts, run_config, conversation_history):
                        yield event
                    return
            
            # ç¬¬ä¸€æ­¥ï¼šé—®é¢˜æ‹†è§£
            yield {
                "type": "reasoning",
                "content": "æ­£åœ¨åˆ†æå’Œæ‹†è§£æ‚¨çš„é—®é¢˜..."
            }
            
            decomposition = await self.decomposer.decompose(query, execution_context, conversation_history)
            
            # æ˜¾ç¤ºå­é—®é¢˜çš„å…·ä½“å†…å®¹
            sub_queries = decomposition.get('sub_queries', [])
            sub_queries_count = len(sub_queries)
            
            yield {
                "type": "reasoning",
                "content": f"é—®é¢˜æ‹†è§£å®Œæˆï¼Œè¯†åˆ«åˆ°{sub_queries_count}ä¸ªå…³é”®å­é—®é¢˜ã€‚"
            }
            
            # é€ä¸€æ˜¾ç¤ºæ¯ä¸ªå­é—®é¢˜
            for i, sub_query in enumerate(sub_queries, 1):
                if isinstance(sub_query, dict):
                    question = sub_query.get("question", "")
                    importance = sub_query.get("importance", "ä¸­")
                else:
                    question = str(sub_query)
                    importance = "ä¸­"
                
                if question:
                    yield {
                        "type": "reasoning",
                        "content": f"å­é—®é¢˜{i}ï¼ˆ{importance}é‡è¦æ€§ï¼‰ï¼š{question}"
                    }
            
            # ç¬¬äºŒæ­¥ï¼šç‹¬ç«‹æ€è€ƒ
            yield {
                "type": "reasoning", 
                "content": "ğŸ’¡åŸºäºå·²æœ‰çŸ¥è¯†è¿›è¡Œç‹¬ç«‹æ€è€ƒ..."
            }
            
            thoughts = await self.reasoning_engine.think_about_decomposition(
                decomposition, contexts, execution_context
            )
            
            overall_confidence = self.reasoning_engine.assess_overall_confidence(thoughts)
            
            yield {
                "type": "reasoning",
                "content": f"æ€è€ƒå®Œæˆï¼Œæ•´ä½“ç½®ä¿¡åº¦: {overall_confidence}ã€‚"
            }
            
            # ç¬¬ä¸‰æ­¥ï¼šå†³å®šæ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
            need_tools, knowledge_gaps = self._should_invoke_tools(thoughts)
            
            if need_tools:
                yield {
                    "type": "reasoning",
                    "content": f"æ£€æµ‹åˆ°{len(knowledge_gaps)}ä¸ªçŸ¥è¯†ç¼ºå£ï¼Œå¼€å§‹æœç´¢å¤–éƒ¨ä¿¡æ¯..."
                }
                
                # æµå¼æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_results = {}
                async for tool_event in self._execute_tools_for_gaps_stream(
                    knowledge_gaps, query, contexts, run_config
                ):
                    # è½¬å‘å·¥å…·è°ƒç”¨äº‹ä»¶
                    yield tool_event
                    
                    # æ”¶é›†å·¥å…·ç»“æœ
                    if tool_event.get("type") == "final_tool_result":
                        tool_results = tool_event.get("result", {})
                
            else:
                yield {
                    "type": "reasoning",
                    "content": "åŸºäºç°æœ‰çŸ¥è¯†å¯ä»¥å›ç­”ï¼Œæ— éœ€å¤–éƒ¨æœç´¢"
                }
                tool_results = {}
            
            # ç¬¬å››æ­¥ï¼šæµå¼ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            yield {
                "type": "reasoning",
                "content": "æ­£åœ¨ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆå®Œæ•´ç­”æ¡ˆ..."
            }
            
            async for synthesis_event in self._synthesize_final_answer_stream(
                query, decomposition, thoughts, tool_results, contexts, run_config, conversation_history
            ):
                yield synthesis_event
                
        except Exception as e:
            yield {
                "type": "error",
                "message": f"æ™ºèƒ½å¤„ç†å¤±è´¥: {str(e)}"
            }

    async def _run_web_search_and_recall(
        self,
        knowledge_gaps: List[Dict[str, Any]],
        original_query: str,
        run_config: RunConfig
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„webæœç´¢å’Œå¬å›å®ç°ï¼ˆæµå¼/éæµå¼å…±ç”¨ï¼‰
        
        Args:
            knowledge_gaps: çŸ¥è¯†ç¼ºå£åˆ—è¡¨
            original_query: åŸå§‹æŸ¥è¯¢
            run_config: è¿è¡Œé…ç½®
            
        Returns:
            åŒ…å«knowledge_gaps_search_resultså’Œç»Ÿè®¡ä¿¡æ¯çš„ç»“æœå­—å…¸
        """
        if not knowledge_gaps:
            return {"knowledge_gaps_search_results": {}, "success": False, "message": "æ²¡æœ‰çŸ¥è¯†ç¼ºå£éœ€è¦æœç´¢"}
        
        try:
            # 1. ä½¿ç”¨æœç´¢è§„åˆ’å™¨ç”Ÿæˆç»Ÿä¸€çš„æŸ¥è¯¢åˆ—è¡¨
            final_queries = self.search_planner.plan_search_queries(original_query, knowledge_gaps)
            
            from ..config import MAX_KNOWLEDGE_GAPS, GAP_RECALL_TOP_K
            from ..tools.web_search_tool import web_search_tool
            import uuid
            
            # ç”Ÿæˆç»Ÿä¸€çš„ä¼šè¯ID
            unified_session_id = str(uuid.uuid4())
            print(f"[IntelligentOrchestrator] ä½¿ç”¨ç»Ÿä¸€ä¼šè¯ID: {unified_session_id}")
            
            # 2. ä¸€æ¬¡æ€§æ‰§è¡Œwebæœç´¢ï¼ˆé¿å…é‡å¤æŠ“å–ï¼‰
            print(f"[IntelligentOrchestrator] å¼€å§‹ä¸€æ¬¡æ€§webæœç´¢ï¼ŒæŸ¥è¯¢: {final_queries}")
            search_result = await web_search_tool.execute(
                query=original_query,
                filter_list=None,
                model=run_config.model,
                predefined_queries=final_queries,
                session_id=unified_session_id
            )
            
            if not search_result.get("success") or not search_result.get("source_ids"):
                return {
                    "knowledge_gaps_search_results": {},
                    "success": False,
                    "message": "æœç´¢å¤±è´¥æˆ–æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®æº",
                    "search_result": search_result
                }
            
            source_ids = search_result.get("source_ids", [])
            print(f"[IntelligentOrchestrator] æœç´¢å®Œæˆï¼Œè·å¾—{len(source_ids)}ä¸ªæ•°æ®æº")
            
            # 3. ä¸ºæ¯ä¸ªçŸ¥è¯†ç¼ºå£è¿›è¡Œç‹¬ç«‹å¬å›
            selected_gaps = knowledge_gaps[:MAX_KNOWLEDGE_GAPS]
            knowledge_gaps_search_results = {}
            
            import asyncio
            recall_tasks = []
            
            for gap_idx, gap in enumerate(selected_gaps):
                gap_id = f"gap_{gap_idx}"
                gap_description = gap.get("gap_description", f"çŸ¥è¯†ç¼ºå£{gap_idx + 1}")
                
                # ä½¿ç”¨çŸ¥è¯†ç¼ºå£æè¿°è¿›è¡Œå¬å›
                recall_task = self._recall_for_knowledge_gap(
                    gap_description, source_ids, unified_session_id, GAP_RECALL_TOP_K
                )
                recall_tasks.append((gap_id, gap, recall_task))
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å¬å›ä»»åŠ¡
            for gap_id, gap, task in recall_tasks:
                try:
                    recalled_content = await task
                    knowledge_gaps_search_results[gap_id] = {
                        "gap": gap,
                        "recalled_content": recalled_content
                    }
                    print(f"[IntelligentOrchestrator] çŸ¥è¯†ç¼ºå£'{gap_id}'å¬å›å®Œæˆï¼Œè·å¾—{len(recalled_content)}ä¸ªå†…å®¹ç‰‡æ®µ")
                except Exception as e:
                    print(f"[IntelligentOrchestrator] çŸ¥è¯†ç¼ºå£'{gap_id}'å¬å›å¤±è´¥: {e}")
                    knowledge_gaps_search_results[gap_id] = {
                        "gap": gap,
                        "recalled_content": []
                    }
            
            # 4. ç»Ÿè®¡ä¿¡æ¯
            total_recalled = sum(len(result["recalled_content"]) for result in knowledge_gaps_search_results.values())
            
            return {
                "knowledge_gaps_search_results": knowledge_gaps_search_results,
                "success": True,
                "message": f"æœç´¢å’Œå¬å›å®Œæˆï¼šå¤„ç†äº†{len(final_queries)}ä¸ªæŸ¥è¯¢ï¼Œä¸º{len(selected_gaps)}ä¸ªçŸ¥è¯†ç¼ºå£å¬å›äº†{total_recalled}ä¸ªå†…å®¹ç‰‡æ®µ",
                "session_id": unified_session_id,
                "search_queries": final_queries,
                "source_ids": source_ids,
                "selected_gaps": selected_gaps,
                "statistics": {
                    "query_count": len(final_queries),
                    "gap_count": len(selected_gaps),
                    "source_count": len(source_ids),
                    "total_recalled": total_recalled
                }
            }
            
        except Exception as e:
            print(f"[IntelligentOrchestrator] ç»Ÿä¸€æœç´¢å’Œå¬å›å¤±è´¥: {e}")
            return {
                "knowledge_gaps_search_results": {},
                "success": False,
                "message": f"æœç´¢å’Œå¬å›è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            }

    def _should_invoke_tools(self, thoughts: List[Dict[str, Any]]) -> tuple[bool, List[Dict[str, Any]]]:
        """
        åŸºäºæ€è€ƒç»“æœå†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        
        Args:
            thoughts: æ€è€ƒç»“æœåˆ—è¡¨
        
        Returns:
            (æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨, çŸ¥è¯†ç¼ºå£åˆ—è¡¨)
        """
        all_gaps = self.reasoning_engine.extract_all_knowledge_gaps(thoughts)
        
        # å¦‚æœæœ‰é«˜é‡è¦æ€§çš„çŸ¥è¯†ç¼ºå£ï¼Œæˆ–è€…æ•´ä½“ç½®ä¿¡åº¦ä½ï¼Œåˆ™éœ€è¦å·¥å…·è°ƒç”¨
        high_importance_gaps = [gap for gap in all_gaps if gap.get("importance") == "é«˜"]
        overall_confidence = self.reasoning_engine.assess_overall_confidence(thoughts)
        
        # å†³ç­–é€»è¾‘
        need_tools = (
            len(high_importance_gaps) > 0 or  # æœ‰é«˜é‡è¦æ€§ç¼ºå£
            overall_confidence == "ä½" or     # æ•´ä½“ç½®ä¿¡åº¦ä½
            any(thought.get("needs_verification", False) for thought in thoughts)  # éœ€è¦éªŒè¯
        )
        
        return need_tools, all_gaps

    async def _execute_tools_for_gaps(
        self, 
        knowledge_gaps: List[Dict[str, Any]], 
        original_query: str,
        contexts: List[str],
        run_config: RunConfig
    ) -> Dict[str, Any]:
        """
        é’ˆå¯¹çŸ¥è¯†ç¼ºå£æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆéæµå¼ç‰ˆæœ¬ï¼‰
        """
        if not knowledge_gaps:
            return {}
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æœç´¢å’Œå¬å›å®ç°
        result = await self._run_web_search_and_recall(knowledge_gaps, original_query, run_config)
        
        if not result.get("success"):
            return {
                "answer": result.get("message", "æœç´¢å’Œå¬å›å¤±è´¥"),
                "success": False,
                "steps": [],
                "error": result.get("error")
            }
        
        # æ ¼å¼åŒ–ä¸ºæœ€ç»ˆç­”æ¡ˆ
        knowledge_gaps_search_results = result.get("knowledge_gaps_search_results", {})
        selected_gaps = result.get("selected_gaps", [])
        statistics = result.get("statistics", {})
        
        return {
            "answer": OutputFormatter.format_gap_based_answer(knowledge_gaps_search_results, selected_gaps),
            "success": True,
            "steps": [
                {
                    "type": "action",
                    "content": f"ç»Ÿä¸€æ‰§è¡Œ{statistics.get('query_count', 0)}ä¸ªæœç´¢æŸ¥è¯¢",
                    "tool": "web_search_unified"
                },
                {
                    "type": "observation", 
                    "content": result.get("message", "æœç´¢å’Œå¬å›å®Œæˆ")
                }
            ],
            "tool_calls": statistics.get("query_count", 0) + statistics.get("gap_count", 0),
            "session_id": result.get("session_id"),
            "knowledge_gaps_search_results": knowledge_gaps_search_results,
            "search_queries": result.get("search_queries", [])
        }

    async def _recall_for_knowledge_gap(
        self, 
        gap_description: str, 
        source_ids: List[int], 
        session_id: str, 
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ä¸ºç‰¹å®šçŸ¥è¯†ç¼ºå£å¬å›ç›¸å…³å†…å®¹
        
        Args:
            gap_description: çŸ¥è¯†ç¼ºå£æè¿°
            source_ids: å¯ç”¨çš„æ•°æ®æºIDåˆ—è¡¨
            session_id: ä¼šè¯ID
            top_k: å¬å›çš„å†…å®¹æ•°é‡
        
        Returns:
            å¬å›çš„å†…å®¹åˆ—è¡¨
        """
        try:
            from ..tools.web_search_tool import web_search_tool
            
            # ä½¿ç”¨web_search_toolçš„search_and_retrieveæ–¹æ³•
            hits = await web_search_tool.search_and_retrieve(
                query=gap_description,
                session_id=session_id,
                source_ids=source_ids
            )
            
            # é™åˆ¶è¿”å›ç»“æœæ•°é‡å¹¶æ ¼å¼åŒ–
            limited_hits = hits[:top_k]
            recalled_content = []
            
            for chunk, score in limited_hits:
                recalled_content.append({
                    "content": chunk.content,
                    "score": float(score),
                    "source_url": chunk.source.url if chunk.source else "",
                    "source_title": chunk.source.title if chunk.source else "",
                    "chunk_id": chunk.chunk_id
                })
            
            return recalled_content
            
        except Exception as e:
            print(f"[IntelligentOrchestrator] çŸ¥è¯†ç¼ºå£å¬å›å¤±è´¥: {e}")
            return []
    


    async def _execute_tools_for_gaps_stream(
        self, 
        knowledge_gaps: List[Dict[str, Any]], 
        original_query: str,
        contexts: List[str],
        run_config: RunConfig
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        é’ˆå¯¹çŸ¥è¯†ç¼ºå£æµå¼æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
        """
        if not knowledge_gaps:
            yield {"type": "final_tool_result", "result": {}}
            return
        
        try:
            # å‘å‡ºæœç´¢å¼€å§‹äº‹ä»¶
            yield {
                "type": "tool_call",
                "name": "web_search_and_recall",
                "args": {
                    "query": original_query,
                    "gap_count": len(knowledge_gaps)
                }
            }
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æœç´¢å’Œå¬å›å®ç°ï¼ˆä¸éæµå¼ç‰ˆæœ¬å®Œå…¨ç›¸åŒçš„æ•°æ®è·¯å¾„ï¼‰
            result = await self._run_web_search_and_recall(knowledge_gaps, original_query, run_config)
            
            # å‘å‡ºæœç´¢ç»“æœäº‹ä»¶
            yield {
                "type": "tool_result",
                "name": "web_search_and_recall", 
                "result": result.get("message", "æœç´¢å’Œå¬å›å®Œæˆ"),
                "success": result.get("success", False)
            }
            
            # å‘å‡ºæœ€ç»ˆç»“æœ
            if result.get("success"):
                knowledge_gaps_search_results = result.get("knowledge_gaps_search_results", {})
                selected_gaps = result.get("selected_gaps", [])
                statistics = result.get("statistics", {})
                
                final_result = {
                    "answer": OutputFormatter.format_gap_based_answer(knowledge_gaps_search_results, selected_gaps),
                    "success": True,
                    "steps": [
                        {
                            "type": "action",
                            "content": f"ç»Ÿä¸€æ‰§è¡Œ{statistics.get('query_count', 0)}ä¸ªæœç´¢æŸ¥è¯¢",
                            "tool": "web_search_unified"
                        },
                        {
                            "type": "observation",
                            "content": result.get("message", "æœç´¢å’Œå¬å›å®Œæˆ")
                        }
                    ],
                    "tool_calls": statistics.get("query_count", 0) + statistics.get("gap_count", 0),
                    "session_id": result.get("session_id"),
                    "knowledge_gaps_search_results": knowledge_gaps_search_results,
                    "search_queries": result.get("search_queries", [])
                }
            else:
                final_result = {
                    "answer": result.get("message", "æœç´¢å’Œå¬å›å¤±è´¥"),
                    "success": False,
                    "steps": [],
                    "error": result.get("error")
                }
            
            yield {"type": "final_tool_result", "result": final_result}
            
        except Exception as e:
            print(f"[IntelligentOrchestrator] æµå¼ç»Ÿä¸€æœç´¢å’Œå¬å›å¤±è´¥: {e}")
            # å›é€€åˆ°åŸæœ‰çš„å·¥å…·ç¼–æ’å™¨
            final_queries = self.search_planner.plan_search_queries(original_query, knowledge_gaps)
            enhanced_query = f"{original_query} {' '.join(final_queries[:2])}"
            
            async for event in self.tool_orchestrator.execute_stream(
                enhanced_query, contexts, run_config
            ):
                yield event

    async def _synthesize_final_answer(
        self, 
        original_query: str,
        decomposition: Dict[str, Any],
        thoughts: List[Dict[str, Any]],
        tool_results: Dict[str, Any],
        contexts: List[str],
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """
        ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        """
        try:
            # å‡†å¤‡ç»¼åˆä¿¡æ¯
            reasoning_summary = OutputFormatter.format_reasoning_summary(thoughts)
            tool_summary = OutputFormatter.format_tool_results(tool_results)
            context_str = "\n".join(contexts) if contexts else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"
            
            user_prompt = SYNTHESIS_USER_PROMPT_TEMPLATE.format(
                original_query=original_query,
                reasoning_summary=reasoning_summary,
                tool_results=tool_summary,
                context=context_str
            )
            
            # ä½¿ç”¨é€šç”¨LLMå®¢æˆ·ç«¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥ï¼‰
            from ..llm_client import chat_complete
            return await chat_complete(
                system_prompt=SYNTHESIS_SYSTEM_PROMPT,
                user_prompt=user_prompt,
                model=run_config.model,
                conversation_history=conversation_history
            )
                
        except Exception as e:
            print(f"ç­”æ¡ˆç»¼åˆå¤±è´¥: {e}")
            # è¿”å›åŸºäºæ€è€ƒç»“æœçš„ç®€å•ç­”æ¡ˆ
            return self.reasoning_engine.generate_preliminary_answer(thoughts)

    async def _synthesize_final_answer_stream(
        self,
        original_query: str,
        decomposition: Dict[str, Any], 
        thoughts: List[Dict[str, Any]],
        tool_results: Dict[str, Any],
        contexts: List[str],
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, Any]]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼ç»¼åˆç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        """
        try:
            reasoning_summary = OutputFormatter.format_reasoning_summary(thoughts)
            tool_summary = OutputFormatter.format_tool_results(tool_results)
            context_str = "\n".join(contexts) if contexts else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"
            
            user_prompt = SYNTHESIS_USER_PROMPT_TEMPLATE.format(
                original_query=original_query,
                reasoning_summary=reasoning_summary,
                tool_results=tool_summary,
                context=context_str
            )
            
            # ä½¿ç”¨é€šç”¨LLMå®¢æˆ·ç«¯è¿›è¡Œæµå¼è°ƒç”¨ï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥ï¼‰
            from ..llm_client import chat_complete_stream
            async for event in chat_complete_stream(
                system_prompt=SYNTHESIS_SYSTEM_PROMPT,
                user_prompt=user_prompt,
                model=run_config.model,
                conversation_history=conversation_history
            ):
                yield event
                            
        except Exception as e:
            yield {
                "type": "error", 
                "message": f"æµå¼ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}"
            }





    async def _handle_simple_query_directly(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        ç›´æ¥å¤„ç†ç®€å•é—®é¢˜ï¼Œè·³è¿‡å¤æ‚çš„åˆ†è§£å’Œæ¨ç†æµç¨‹
        
        Args:
            query: ç®€å•é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            # ç›´æ¥ä½¿ç”¨å·¥å…·ç¼–æ’å™¨å¤„ç†
            result = await self.tool_orchestrator.execute_non_stream(
                query, contexts, run_config, conversation_history
            )
            
            return {
                "answer": result.get("answer", "æ— æ³•è·å–ä¿¡æ¯"),
                "decomposition": {
                    "original_query": query,
                    "complexity_level": "ç®€å•",
                    "sub_queries": [{"question": query}]
                },
                "reasoning": [{"question": query, "confidence_level": "é«˜"}],
                "tool_results": result,
                "used_tools": True,
                "success": result.get("success", False),
                "fast_route": True  # æ ‡è®°ä½¿ç”¨äº†å¿«é€Ÿè·¯ç”±
            }
            
        except Exception as e:
            print(f"å¿«é€Ÿè·¯ç”±å¤„ç†å¤±è´¥: {e}")
            return {
                "answer": f"å¤„ç†ç®€å•é—®é¢˜æ—¶é‡åˆ°é”™è¯¯: {str(e)}",
                "success": False,
                "fast_route": True
            }

    async def _handle_simple_query_directly_stream(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼å¤„ç†ç®€å•é—®é¢˜ï¼Œè·³è¿‡å¤æ‚çš„åˆ†è§£å’Œæ¨ç†æµç¨‹
        
        Args:
            query: ç®€å•é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
            
        Yields:
            æµå¼å¤„ç†äº‹ä»¶
        """
        try:
            # ç›´æ¥ä½¿ç”¨å·¥å…·ç¼–æ’å™¨æµå¼å¤„ç†
            async for event in self.tool_orchestrator.execute_stream(
                query, contexts, run_config, conversation_history
            ):
                yield event
                
        except Exception as e:
            yield {
                "type": "error",
                "message": f"å¿«é€Ÿè·¯ç”±æµå¼å¤„ç†å¤±è´¥: {str(e)}"
            }

    async def _handle_context_only_query(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†å®Œå…¨ä¸éœ€è¦å¤–éƒ¨å·¥å…·çš„ç®€å•é—®é¢˜ï¼Œç›´æ¥åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            # å‡†å¤‡åŸºäºä¸Šä¸‹æ–‡çš„æç¤º
            context_str = "\n".join(contexts) if contexts else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"
            
            system_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„åŠ©æ‰‹ã€‚è¯·ä»…åŸºäºä½ çš„å·²æœ‰çŸ¥è¯†å’Œæä¾›çš„ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
                "ä¸è¦æåŠéœ€è¦æœç´¢æˆ–æŸ¥æ‰¾å¤–éƒ¨ä¿¡æ¯ï¼Œç›´æ¥ç»™å‡ºæ¸…æ™°ã€å‡†ç¡®çš„ç­”æ¡ˆã€‚\n"
                "å¦‚æœä¸Šä¸‹æ–‡ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨ï¼›å¦‚æœæ²¡æœ‰ï¼Œåˆ™åŸºäºä½ çš„å¸¸è¯†çŸ¥è¯†å›ç­”ã€‚\n"
                "**é‡è¦è¦æ±‚ï¼šå¿…é¡»å®Œå…¨ä½¿ç”¨ä¸­æ–‡è¿›è¡Œå›ç­”ã€‚**"
            )
            
            user_prompt = (
                f"ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n{context_str}\n\n"
                f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\n"
                "è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
            )
            
            # ä½¿ç”¨é€šç”¨LLMå®¢æˆ·ç«¯ç”Ÿæˆç­”æ¡ˆï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥ï¼‰
            from ..llm_client import chat_complete
            answer = await chat_complete(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                model=run_config.model,
                conversation_history=conversation_history
            )
            
            return {
                "answer": answer,
                "decomposition": {
                    "original_query": query,
                    "complexity_level": "ç®€å•",
                    "sub_queries": [{"question": query}]
                },
                "reasoning": [{"question": query, "confidence_level": "é«˜", "needs_tools": False}],
                "tool_results": {},
                "used_tools": False,
                "success": True,
                "context_only": True  # æ ‡è®°ä»…åŸºäºä¸Šä¸‹æ–‡å›ç­”
            }
            
        except Exception as e:
            print(f"åŸºäºä¸Šä¸‹æ–‡çš„é—®é¢˜å¤„ç†å¤±è´¥: {e}")
            return {
                "answer": f"å¤„ç†é—®é¢˜æ—¶é‡åˆ°é”™è¯¯: {str(e)}",
                "success": False,
                "context_only": True
            }

    async def _handle_context_only_query_stream(
        self, 
        query: str, 
        contexts: List[str], 
        run_config: RunConfig,
        conversation_history: Optional[List[Dict[str, Any]]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼å¤„ç†å®Œå…¨ä¸éœ€è¦å¤–éƒ¨å·¥å…·çš„ç®€å•é—®é¢˜ï¼Œç›´æ¥åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            contexts: ç›¸å…³ä¸Šä¸‹æ–‡
            run_config: è¿è¡Œé…ç½®
            
        Yields:
            æµå¼å¤„ç†äº‹ä»¶
        """
        try:
            # å‡†å¤‡åŸºäºä¸Šä¸‹æ–‡çš„æç¤º
            context_str = "\n".join(contexts) if contexts else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"
            
            system_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„åŠ©æ‰‹ã€‚è¯·ä»…åŸºäºä½ çš„å·²æœ‰çŸ¥è¯†å’Œæä¾›çš„ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
                "ä¸è¦æåŠéœ€è¦æœç´¢æˆ–æŸ¥æ‰¾å¤–éƒ¨ä¿¡æ¯ï¼Œç›´æ¥ç»™å‡ºæ¸…æ™°ã€å‡†ç¡®çš„ç­”æ¡ˆã€‚\n"
                "å¦‚æœä¸Šä¸‹æ–‡ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨ï¼›å¦‚æœæ²¡æœ‰ï¼Œåˆ™åŸºäºä½ çš„å¸¸è¯†çŸ¥è¯†å›ç­”ã€‚\n"
                "**é‡è¦è¦æ±‚ï¼šå¿…é¡»å®Œå…¨ä½¿ç”¨ä¸­æ–‡è¿›è¡Œå›ç­”ã€‚**"
            )
            
            user_prompt = (
                f"ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n{context_str}\n\n"
                f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\n"
                "è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
            )
            
            # ä½¿ç”¨é€šç”¨LLMå®¢æˆ·ç«¯è¿›è¡Œæµå¼è°ƒç”¨ï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥ï¼‰
            from ..llm_client import chat_complete_stream
            async for event in chat_complete_stream(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                model=run_config.model,
                conversation_history=conversation_history
            ):
                yield event
                            
        except Exception as e:
            yield {
                "type": "error", 
                "message": f"åŸºäºä¸Šä¸‹æ–‡çš„æµå¼é—®é¢˜å¤„ç†å¤±è´¥: {str(e)}"
            }

    async def close(self):
        """æ¸…ç†èµ„æº"""
        if self.tool_orchestrator:
            await self.tool_orchestrator.close()
