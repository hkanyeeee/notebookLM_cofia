#!/usr/bin/env python3
"""
é€šç”¨å‘é‡æ•°æ®åº“æ•°æ®ä¿®å¤è„šæœ¬
æ”¯æŒä¿®å¤æŒ‡å®šé›†åˆæˆ–å…¨éƒ¨é›†åˆçš„å‘é‡æ•°æ®
"""
import asyncio
import argparse
from typing import List, Optional, Dict, Any
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.database import get_db
from app.models import Source, Chunk
from app.embedding_client import embed_texts
from app.vector_db_client import add_embeddings, qdrant_client, COLLECTION_NAME, delete_vector_db_data
from app.config import EMBEDDING_BATCH_SIZE, EMBEDDING_DIMENSIONS, DEFAULT_EMBEDDING_MODEL


class VectorDataFixer:
    """å‘é‡æ•°æ®ä¿®å¤å™¨"""

    def __init__(self, session_id: str, force_regenerate: bool = False, dry_run: bool = False):
        self.session_id = session_id
        self.force_regenerate = force_regenerate
        self.dry_run = dry_run
        self.stats = {
            'processed_collections': 0,
            'total_chunks': 0,
            'generated_embeddings': 0,
            'errors': 0
        }

    async def list_collections(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å¯ä¿®å¤çš„é›†åˆ"""
        print("=== æ‰«ææ‰€æœ‰é›†åˆ ===")

        async for db in get_db():
            try:
                # è·å–æ‰€æœ‰Source
                sources_stmt = select(Source).where(Source.session_id == self.session_id)
                sources_result = await db.execute(sources_stmt)
                sources = sources_result.scalars().all()

                collections = []
                for source in sources:
                    # è·å–æ¯ä¸ªsourceçš„chunksæ•°é‡
                    chunks_stmt = select(Chunk).where(
                        Chunk.source_id == source.id,
                        Chunk.session_id == self.session_id
                    )
                    chunks_result = await db.execute(chunks_stmt)
                    chunks_count = len(chunks_result.scalars().all())

                    # æ£€æŸ¥Qdrantä¸­æ˜¯å¦å·²æœ‰å‘é‡æ•°æ®
                    try:
                        search_result = qdrant_client.scroll(
                            collection_name=COLLECTION_NAME,
                            scroll_filter={
                                "must": [
                                    {"key": "source_id", "match": {"value": source.id}},
                                    {"key": "session_id", "match": {"value": self.session_id}}
                                ]
                            },
                            limit=1
                        )
                        qdrant_count = len(search_result[0])
                    except Exception:
                        qdrant_count = 0

                    collections.append({
                        'id': source.id,
                        'title': source.title,
                        'chunks_count': chunks_count,
                        'qdrant_count': qdrant_count,
                        'needs_fix': chunks_count > 0 and (qdrant_count == 0 or self.force_regenerate)
                    })

                print(f"âœ… æ‰¾åˆ° {len(collections)} ä¸ªé›†åˆ")
                return collections

            except Exception as e:
                print(f"âŒ æ‰«æé›†åˆå¤±è´¥: {e}")
                return []

    async def fix_collection(self, collection_id: int) -> bool:
        """ä¿®å¤æŒ‡å®šé›†åˆçš„å‘é‡æ•°æ®"""
        print(f"\n=== ä¿®å¤Collection {collection_id} ===")

        async for db in get_db():
            try:
                # 1. è·å–Collectionä¿¡æ¯
                print(f"1. è·å–Collection {collection_id}çš„ä¿¡æ¯...")
                source_stmt = select(Source).where(
                    Source.id == collection_id,
                    Source.session_id == self.session_id
                )
                source_result = await db.execute(source_stmt)
                source = source_result.scalar_one_or_none()

                if not source:
                    print(f"âŒ Collection {collection_id} ä¸å­˜åœ¨")
                    return False

                print(f"âœ… æ‰¾åˆ°Collection: {source.title}")

                # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®å¤
                if not self.force_regenerate:
                    try:
                        search_result = qdrant_client.scroll(
                            collection_name=COLLECTION_NAME,
                            scroll_filter={
                                "must": [
                                    {"key": "source_id", "match": {"value": source.id}},
                                    {"key": "session_id", "match": {"value": self.session_id}}
                                ]
                            },
                            limit=1
                        )
                        if len(search_result[0]) > 0:
                            print(f"â„¹ï¸ Collection {collection_id} å·²æœ‰å‘é‡æ•°æ®ï¼Œè·³è¿‡")
                            return True
                    except Exception:
                        pass  # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­å¤„ç†

                # åœ¨é‡å»ºå‰æ¸…ç†è¯¥é›†åˆåœ¨ Qdrant çš„å†å²å‘é‡ï¼Œé¿å…æ—§æ•°æ®æ®‹ç•™
                try:
                    await delete_vector_db_data([source.id])
                except Exception as e:
                    print(f"æ¸…ç†æ—§å‘é‡å¤±è´¥ï¼ˆè·³è¿‡ç»§ç»­ï¼‰: {e}")

                # 3. è·å–æ‰€æœ‰chunks
                print("2. è·å–chunks...")
                chunks_stmt = select(Chunk).where(
                    Chunk.source_id == source.id,
                    Chunk.session_id == self.session_id
                )
                chunks_result = await db.execute(chunks_stmt)
                chunks = chunks_result.scalars().all()

                print(f"âœ… æ‰¾åˆ° {len(chunks)} ä¸ªchunks")

                if not chunks:
                    print("âŒ æ²¡æœ‰chunkséœ€è¦å¤„ç†")
                    return False

                # 4. åˆ†æ‰¹å¤„ç†embeddings
                if self.dry_run:
                    print(f"ğŸ“‹ [DRY RUN] é¢„ä¼°å¤„ç† {len(chunks)} ä¸ªchunks")
                    self.stats['generated_embeddings'] += len(chunks)
                    self.stats['processed_collections'] += 1
                    return True

                print(f"3. å¼€å§‹ç”Ÿæˆembeddings (æ‰¹æ¬¡å¤§å°: {EMBEDDING_BATCH_SIZE})...")
                batch_size = EMBEDDING_BATCH_SIZE
                total_batches = (len(chunks) + batch_size - 1) // batch_size

                for batch_index in range(total_batches):
                    start_idx = batch_index * batch_size
                    end_idx = min((batch_index + 1) * batch_size, len(chunks))
                    batch_chunks = chunks[start_idx:end_idx]

                    print(f"å¤„ç†æ‰¹æ¬¡ {batch_index + 1}/{total_batches}: {len(batch_chunks)} chunks")

                    # æå–æ–‡æœ¬å†…å®¹
                    batch_texts = [chunk.content for chunk in batch_chunks]

                    try:
                        # ç”Ÿæˆembeddings
                        embeddings = await embed_texts(
                            texts=batch_texts,
                            model=DEFAULT_EMBEDDING_MODEL,
                            batch_size=EMBEDDING_BATCH_SIZE,
                            dimensions=EMBEDDING_DIMENSIONS
                        )

                        if not embeddings or len(embeddings) != len(batch_chunks):
                            print(f"âŒ æ‰¹æ¬¡ {batch_index + 1} embeddingç”Ÿæˆå¤±è´¥æˆ–æ•°é‡ä¸åŒ¹é…")
                            self.stats['errors'] += 1
                            continue

                        # å­˜å‚¨åˆ°Qdrant
                        await add_embeddings(source.id, batch_chunks, embeddings)
                        print(f"âœ… æ‰¹æ¬¡ {batch_index + 1} å­˜å‚¨å®Œæˆ")

                        self.stats['generated_embeddings'] += len(batch_chunks)

                    except Exception as e:
                        print(f"âŒ æ‰¹æ¬¡ {batch_index + 1} å¤„ç†å¤±è´¥: {e}")
                        self.stats['errors'] += 1
                        continue

                print(f"âœ… Collection {collection_id} å‘é‡æ•°æ®ä¿®å¤å®Œæˆï¼")
                self.stats['processed_collections'] += 1
                self.stats['total_chunks'] += len(chunks)
                return True

            except Exception as e:
                print(f"âŒ ä¿®å¤Collection {collection_id} å¤±è´¥: {e}")
                self.stats['errors'] += 1
                import traceback
                traceback.print_exc()
                return False

    async def fix_all_collections(self) -> None:
        """ä¿®å¤æ‰€æœ‰éœ€è¦ä¿®å¤çš„é›†åˆ"""
        collections = await self.list_collections()

        if not collections:
            return

        # æ˜¾ç¤ºéœ€è¦ä¿®å¤çš„é›†åˆ
        need_fix = [c for c in collections if c['needs_fix']]
        if not need_fix:
            print("â„¹ï¸ æ‰€æœ‰é›†åˆéƒ½å·²æœ‰å‘é‡æ•°æ®ï¼Œæ— éœ€ä¿®å¤")
            return

        print(f"\néœ€è¦ä¿®å¤çš„é›†åˆ: {len(need_fix)} ä¸ª")
        for collection in need_fix:
            print(f"  - ID: {collection['id']}, æ ‡é¢˜: {collection['title']}, Chunks: {collection['chunks_count']}")

        # é€ä¸ªä¿®å¤
        for collection in need_fix:
            await self.fix_collection(collection['id'])

    async def verify_collection(self, collection_id: int) -> Dict[str, Any]:
        """éªŒè¯é›†åˆçš„å‘é‡æ•°æ®"""
        print(f"\n=== éªŒè¯Collection {collection_id} ===")

        result = {
            'collection_id': collection_id,
            'db_chunks': 0,
            'qdrant_points': 0,
            'status': 'unknown'
        }

        try:
            async for db in get_db():
                # è·å–æ•°æ®åº“ä¸­çš„chunksæ•°é‡
                chunks_stmt = select(Chunk).where(
                    Chunk.source_id == collection_id,
                    Chunk.session_id == self.session_id
                )
                chunks_result = await db.execute(chunks_stmt)
                chunks = chunks_result.scalars().all()
                result['db_chunks'] = len(chunks)

            # è·å–Qdrantä¸­çš„æ•°æ®
            search_result = qdrant_client.scroll(
                collection_name=COLLECTION_NAME,
                scroll_filter={
                    "must": [
                        {"key": "source_id", "match": {"value": collection_id}},
                        {"key": "session_id", "match": {"value": self.session_id}}
                    ]
                },
                limit=1000,  # è·å–æ›´å¤šè®°å½•ç”¨äºç»Ÿè®¡
                with_payload=True
            )

            result['qdrant_points'] = len(search_result[0])

            if result['db_chunks'] == result['qdrant_points']:
                result['status'] = 'complete'
            elif result['qdrant_points'] == 0:
                result['status'] = 'missing'
            else:
                result['status'] = 'partial'

            print(f"æ•°æ®åº“chunks: {result['db_chunks']}")
            print(f"Qdrantå‘é‡: {result['qdrant_points']}")
            print(f"çŠ¶æ€: {result['status']}")

            if search_result[0]:
                print("å‰3æ¡è®°å½•é¢„è§ˆ:")
                for i, point in enumerate(search_result[0][:3]):
                    content_preview = point.payload.get('content', '')[:80] + "..."
                    print(f"  {i+1}. Point {point.id}: {content_preview}")

        except Exception as e:
            print(f"âŒ éªŒè¯å¤±è´¥: {e}")
            result['status'] = 'error'

        return result

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n=== ä¿®å¤ç»Ÿè®¡ ===")
        print(f"å¤„ç†é›†åˆæ•°: {self.stats['processed_collections']}")
        print(f"å¤„ç†chunksæ€»æ•°: {self.stats['total_chunks']}")
        print(f"ç”Ÿæˆembeddingsæ•°: {self.stats['generated_embeddings']}")
        print(f"é”™è¯¯æ¬¡æ•°: {self.stats['errors']}")


async def main():
    parser = argparse.ArgumentParser(description="å‘é‡æ•°æ®åº“æ•°æ®ä¿®å¤å·¥å…·")
    # é»˜è®¤ä½¿ç”¨ä¸ agenttic_ingest ä¸€è‡´çš„å›ºå®š Session IDï¼Œç”¨æˆ·æ— éœ€ç†è§£/ä¼ é€’
    parser.add_argument(
        "--session-id",
        required=False,
        default="fixed_session_id_for_agenttic_ingest",
        help="Session IDï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ agenttic_ingest ä¸€è‡´ï¼‰",
    )
    parser.add_argument("--collection-id", type=int, help="æŒ‡å®šä¿®å¤çš„é›†åˆID")
    parser.add_argument("--all", action="store_true", help="ä¿®å¤æ‰€æœ‰éœ€è¦ä¿®å¤çš„é›†åˆ")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæ‰€æœ‰é›†åˆçŠ¶æ€")
    parser.add_argument("--verify", type=int, help="éªŒè¯æŒ‡å®šé›†åˆçš„çŠ¶æ€")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰å‘é‡æ•°æ®")
    parser.add_argument("--dry-run", action="store_true", help="ä»…é¢„è§ˆï¼Œä¸å®é™…æ‰§è¡Œ")

    args = parser.parse_args()

    if not any([args.collection_id, args.all, args.list, args.verify]):
        parser.error("å¿…é¡»æŒ‡å®š --collection-idã€--allã€--list æˆ– --verify ä¸­çš„ä¸€ä¸ª")

    fixer = VectorDataFixer(
        session_id=args.session_id,
        force_regenerate=args.force,
        dry_run=args.dry_run
    )

    try:
        if args.list:
            collections = await fixer.list_collections()
            print("\n=== é›†åˆçŠ¶æ€è¯¦æƒ… ===")
            for collection in collections:
                status_icon = "âœ…" if collection['qdrant_count'] > 0 else "âŒ"
                needs_fix_icon = "ğŸ”§" if collection['needs_fix'] else "âœ“"
                print(f"{status_icon} {needs_fix_icon} ID: {collection['id']}, "
                      f"æ ‡é¢˜: {collection['title']}, "
                      f"Chunks: {collection['chunks_count']}, "
                      f"å‘é‡: {collection['qdrant_count']}")

        elif args.verify:
            result = await fixer.verify_collection(args.verify)

        elif args.collection_id:
            success = await fixer.fix_collection(args.collection_id)
            if success:
                # éªŒè¯ä¿®å¤ç»“æœ
                await fixer.verify_collection(args.collection_id)

        elif args.all:
            await fixer.fix_all_collections()

        fixer.print_stats()

    except KeyboardInterrupt:
        print("\nâš ï¸ æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
