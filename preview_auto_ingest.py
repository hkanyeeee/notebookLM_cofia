#!/usr/bin/env python3
"""
Auto Ingest é¢„è§ˆè„šæœ¬

è¿™ä¸ªè„šæœ¬ä½¿ç”¨ä¸ auto ingest ç›¸åŒçš„æ–¹æ³•æ‹‰å–é¡µé¢å†…å®¹ï¼Œ
å¹¶æ˜¾ç¤ºæ‹‰å–ç»“æœä¾›äººå·¥ç¡®è®¤æ˜¯å¦åŒ…å«å­ URLã€‚

ä½¿ç”¨æ–¹æ³•:
    python preview_auto_ingest.py <URL>
    
æˆ–è€…äº¤äº’å¼è¿è¡Œ:
    python preview_auto_ingest.py
"""

import asyncio
import sys
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from typing import List, Tuple
import json

# å¯¼å…¥é¡¹ç›®çš„è·å–å’Œè§£ææ¨¡å—
from app.fetch_parse import fetch_html, fetch_then_extract, extract_text


def extract_links_from_html(html: str, base_url: str) -> List[Tuple[str, str]]:
    """
    ä»HTMLä¸­æå–æ‰€æœ‰é“¾æ¥
    
    Args:
        html: HTMLå†…å®¹
        base_url: åŸºç¡€URLï¼Œç”¨äºæ‹¼æ¥ç›¸å¯¹é“¾æ¥
        
    Returns:
        List[Tuple[str, str]]: (é“¾æ¥æ–‡æœ¬, å®Œæ•´URL) çš„åˆ—è¡¨
    """
    soup = BeautifulSoup(html, "html.parser")
    links = []
    
    # è·å–æ‰€æœ‰aæ ‡ç­¾
    for a_tag in soup.find_all("a", href=True):
        href = a_tag.get("href", "").strip()
        text = a_tag.get_text(strip=True)
        
        if not href or href.startswith(("#", "javascript:", "mailto:", "tel:")):
            continue
            
        # è½¬æ¢ä¸ºç»å¯¹URL
        full_url = urljoin(base_url, href)
        
        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å­æ–‡æ¡£çš„é“¾æ¥
        if is_potential_sub_doc(full_url, base_url):
            links.append((text or "[æ— æ–‡æœ¬]", full_url))
    
    # è·å–å¯èƒ½çš„æŒ‰é’®é“¾æ¥
    for button in soup.find_all("button"):
        onclick = button.get("onclick", "")
        if "location" in onclick or "href" in onclick:
            # ç®€å•çš„onclickè§£æ
            url_match = re.search(r"['\"]([^'\"]+)['\"]", onclick)
            if url_match:
                href = url_match.group(1)
                full_url = urljoin(base_url, href)
                if is_potential_sub_doc(full_url, base_url):
                    text = button.get_text(strip=True)
                    links.append((text or "[æŒ‰é’®]", full_url))
    
    return links


def is_potential_sub_doc(url: str, base_url: str) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯å­æ–‡æ¡£é“¾æ¥
    
    Args:
        url: è¦æ£€æŸ¥çš„URL
        base_url: åŸºç¡€URL
        
    Returns:
        bool: æ˜¯å¦å¯èƒ½æ˜¯å­æ–‡æ¡£
    """
    try:
        parsed_url = urlparse(url)
        parsed_base = urlparse(base_url)
        
        # å¿…é¡»æ˜¯åŒåŸŸå
        if parsed_url.netloc != parsed_base.netloc:
            return False
            
        # URLè·¯å¾„åº”è¯¥ä»¥åŸºç¡€è·¯å¾„å¼€å¤´ï¼ˆè¡¨ç¤ºæ˜¯å­è·¯å¾„ï¼‰
        base_path = parsed_base.path.rstrip('/')
        url_path = parsed_url.path.rstrip('/')
        
        # å¦‚æœæ˜¯æ›´æ·±å±‚çš„è·¯å¾„ï¼Œå¯èƒ½æ˜¯å­æ–‡æ¡£
        if url_path.startswith(base_path) and len(url_path) > len(base_path):
            return True
            
        # å¦‚æœåœ¨åŒä¸€å±‚çº§ä½†ä¸åŒæ–‡ä»¶ï¼Œä¹Ÿå¯èƒ½æ˜¯å­æ–‡æ¡£
        if base_path and url_path.startswith(base_path.rsplit('/', 1)[0]):
            return True
            
        return False
        
    except Exception:
        return False


def display_content_preview(content: str, max_length: int = 1000) -> None:
    """æ˜¾ç¤ºå†…å®¹é¢„è§ˆ"""
    print("\n" + "="*80)
    print("ğŸ“„ é¡µé¢å†…å®¹é¢„è§ˆ")
    print("="*80)
    
    if len(content) <= max_length:
        print(content)
    else:
        print(content[:max_length])
        print(f"\n... (å†…å®¹è¢«æˆªæ–­ï¼Œæ€»é•¿åº¦: {len(content)} å­—ç¬¦)")
    
    print("="*80)


def display_links(links: List[Tuple[str, str]]) -> None:
    """æ˜¾ç¤ºæ‰¾åˆ°çš„é“¾æ¥"""
    print(f"\nğŸ”— æ‰¾åˆ° {len(links)} ä¸ªæ½œåœ¨çš„å­æ–‡æ¡£é“¾æ¥:")
    print("-"*80)
    
    if not links:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ½œåœ¨çš„å­æ–‡æ¡£é“¾æ¥")
        return
    
    for i, (text, url) in enumerate(links, 1):
        print(f"{i:2d}. {text[:50]:<50} -> {url}")
        if len(text) > 50:
            print(f"     å®Œæ•´æ–‡æœ¬: {text}")
        print()


def display_html_structure(html: str) -> None:
    """æ˜¾ç¤ºHTMLç»“æ„æ¦‚è§ˆ"""
    soup = BeautifulSoup(html, "html.parser")
    
    print("\nğŸ—ï¸  HTMLç»“æ„æ¦‚è§ˆ:")
    print("-"*50)
    
    # ç»Ÿè®¡ä¸»è¦æ ‡ç­¾
    tag_counts = {}
    for tag in soup.find_all():
        tag_name = tag.name
        tag_counts[tag_name] = tag_counts.get(tag_name, 0) + 1
    
    # æ˜¾ç¤ºæœ€å¸¸è§çš„æ ‡ç­¾
    common_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    for tag, count in common_tags:
        print(f"  {tag}: {count}")
    
    # æ˜¾ç¤ºé‡è¦çš„ç»“æ„å…ƒç´ 
    important_tags = ['article', 'main', 'nav', 'aside', 'section', 'header', 'footer']
    found_structure = []
    for tag in important_tags:
        elements = soup.find_all(tag)
        if elements:
            found_structure.append(f"{tag}({len(elements)})")
    
    if found_structure:
        print(f"\n  ç»“æ„å…ƒç´ : {', '.join(found_structure)}")


async def preview_url(url: str) -> None:
    """
    é¢„è§ˆæŒ‡å®šURLçš„å†…å®¹å’Œé“¾æ¥
    
    Args:
        url: è¦é¢„è§ˆçš„URL
    """
    print(f"ğŸŒ æ­£åœ¨åˆ†æURL: {url}")
    print("="*80)
    
    try:
        # 1. è·å–åŸå§‹HTML (ä¸auto ingestç›¸åŒçš„æ–¹æ³•)
        print("ğŸ“¥ æ­£åœ¨è·å–HTMLå†…å®¹...")
        html = await fetch_html(url, timeout=10.0)
        
        if not html:
            print("âŒ æ— æ³•è·å–HTMLå†…å®¹")
            return
            
        print(f"âœ… HTMLè·å–æˆåŠŸ (é•¿åº¦: {len(html)} å­—ç¬¦)")
        
        # 2. æå–æ–‡æœ¬å†…å®¹ (ä¸auto ingestç›¸åŒçš„æ–¹æ³•)
        print("ğŸ“ æ­£åœ¨æå–æ–‡æœ¬å†…å®¹...")
        try:
            text_content = extract_text(html, selector="article")
            print(f"âœ… æ–‡æœ¬æå–æˆåŠŸ (é•¿åº¦: {len(text_content)} å­—ç¬¦)")
        except Exception as e:
            print(f"âš ï¸  æ–‡æœ¬æå–å¤±è´¥: {e}")
            # å°è¯•ä½¿ç”¨æ¸²æŸ“æ–¹æ³•
            print("ğŸ”„ å°è¯•ä½¿ç”¨æ¸²æŸ“æ–¹æ³•...")
            text_content = await fetch_then_extract(url, selector="article", timeout=10.0)
            print(f"âœ… æ¸²æŸ“æ–¹æ³•æˆåŠŸ (é•¿åº¦: {len(text_content)} å­—ç¬¦)")
        
        # 3. åˆ†æHTMLç»“æ„
        display_html_structure(html)
        
        # 4. æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
        display_content_preview(text_content)
        
        # 5. æå–å¹¶æ˜¾ç¤ºé“¾æ¥
        links = extract_links_from_html(html, url)
        display_links(links)
        
        # 6. ç”¨æˆ·ç¡®è®¤
        print("\n" + "="*80)
        print("âœ¨ åˆ†æå®Œæˆ!")
        
        if links:
            print(f"ğŸ“‹ æ€»ç»“: æ‰¾åˆ° {len(links)} ä¸ªæ½œåœ¨å­æ–‡æ¡£é“¾æ¥")
            
            # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦ä¿å­˜é“¾æ¥åˆ°æ–‡ä»¶
            save_choice = input("\nğŸ’¾ æ˜¯å¦å°†é“¾æ¥ä¿å­˜åˆ°æ–‡ä»¶? (y/N): ").strip().lower()
            if save_choice in ['y', 'yes']:
                filename = f"extracted_links_{urlparse(url).netloc.replace('.', '_')}.json"
                link_data = {
                    "source_url": url,
                    "extracted_at": asyncio.get_event_loop().time(),
                    "links": [{"text": text, "url": link_url} for text, link_url in links]
                }
                
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(link_data, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… é“¾æ¥å·²ä¿å­˜åˆ°: {filename}")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°æ½œåœ¨çš„å­æ–‡æ¡£é“¾æ¥")
            
        # è¯¢é—®ç”¨æˆ·å¯¹ç»“æœçš„è¯„ä»·
        print("\nğŸ“Š è¯·è¯„ä»·æ‹‰å–ç»“æœ:")
        print("1. âœ… å†…å®¹å®Œæ•´ï¼ŒåŒ…å«æ‰€éœ€çš„å­URL")
        print("2. âš ï¸  å†…å®¹éƒ¨åˆ†ç¼ºå¤±ï¼Œä½†åŒ…å«ä¸€äº›å­URL") 
        print("3. âŒ å†…å®¹ä¸¥é‡ç¼ºå¤±ï¼Œå­URLä¸è¶³")
        print("4. ğŸ” éœ€è¦æ‰‹åŠ¨æ£€æŸ¥HTML")
        
        choice = input("\nè¯·é€‰æ‹© (1-4): ").strip()
        
        feedback_map = {
            "1": "âœ… æ‹‰å–æ•ˆæœè‰¯å¥½",
            "2": "âš ï¸  æ‹‰å–æ•ˆæœä¸€èˆ¬", 
            "3": "âŒ æ‹‰å–æ•ˆæœä¸ä½³",
            "4": "ğŸ” éœ€è¦è¿›ä¸€æ­¥åˆ†æ"
        }
        
        if choice in feedback_map:
            print(f"\n{feedback_map[choice]}")
            
            if choice == "4":
                # æ˜¾ç¤ºHTMLç‰‡æ®µä¾›æ£€æŸ¥
                print("\nğŸ” HTMLå†…å®¹ç‰‡æ®µ (å‰2000å­—ç¬¦):")
                print("-"*50)
                print(html[:2000])
                if len(html) > 2000:
                    print("... (æ›´å¤šå†…å®¹)")
        
    except Exception as e:
        print(f"âŒ å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()


async def interactive_mode():
    """äº¤äº’å¼æ¨¡å¼"""
    print("ğŸš€ Auto Ingest é¢„è§ˆå·¥å…· - äº¤äº’æ¨¡å¼")
    print("="*60)
    print("æ­¤å·¥å…·ä½¿ç”¨ä¸ auto ingest ç›¸åŒçš„æ–¹æ³•æ‹‰å–é¡µé¢å†…å®¹")
    print("å¸®åŠ©æ‚¨ç¡®è®¤æ‹‰å–çš„å†…å®¹æ˜¯å¦åŒ…å«æ‰€éœ€çš„å­URL")
    print("="*60)
    
    while True:
        print("\n" + "-"*40)
        url = input("ğŸŒ è¯·è¾“å…¥è¦é¢„è§ˆçš„URL (è¾“å…¥ 'quit' é€€å‡º): ").strip()
        
        if url.lower() in ['quit', 'exit', 'q']:
            print("ğŸ‘‹ å†è§!")
            break
            
        if not url:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„URL")
            continue
            
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
            
        await preview_url(url)


def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("""
ğŸš€ Auto Ingest é¢„è§ˆå·¥å…·
========================

è¿™ä¸ªè„šæœ¬ä½¿ç”¨ä¸ auto ingest ç›¸åŒçš„æ–¹æ³•æ‹‰å–é¡µé¢å†…å®¹ï¼Œ
å¹¶æ˜¾ç¤ºæ‹‰å–ç»“æœä¾›äººå·¥ç¡®è®¤æ˜¯å¦åŒ…å«å­ URLã€‚

ä½¿ç”¨æ–¹æ³•:
  python preview_auto_ingest.py <URL>        # ç›´æ¥åˆ†ææŒ‡å®šURL
  python preview_auto_ingest.py              # äº¤äº’å¼æ¨¡å¼
  python preview_auto_ingest.py --help       # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹:
  python preview_auto_ingest.py https://lmstudio.ai/docs/python
  python preview_auto_ingest.py docs.python.org

åŠŸèƒ½ç‰¹æ€§:
  âœ… ä½¿ç”¨ä¸ auto ingest ç›¸åŒçš„é¡µé¢æ‹‰å–æ–¹æ³•
  âœ… æ˜¾ç¤ºé¡µé¢å†…å®¹é¢„è§ˆå’ŒHTMLç»“æ„åˆ†æ  
  âœ… è‡ªåŠ¨æ£€æµ‹å’Œæå–æ½œåœ¨çš„å­æ–‡æ¡£é“¾æ¥
  âœ… äº¤äº’å¼ç¡®è®¤å’Œè¯„ä»·ç•Œé¢
  âœ… æ”¯æŒå°†æå–çš„é“¾æ¥ä¿å­˜ä¸ºJSONæ–‡ä»¶
""")


async def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) > 1:
        arg = sys.argv[1]
        
        # å¤„ç†å¸®åŠ©å‚æ•°
        if arg in ['--help', '-h', 'help']:
            show_help()
            return
            
        # å‘½ä»¤è¡Œæ¨¡å¼
        url = arg
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        await preview_url(url)
    else:
        # äº¤äº’å¼æ¨¡å¼
        await interactive_mode()


if __name__ == "__main__":
    asyncio.run(main())
